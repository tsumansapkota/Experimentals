{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms as T\n",
    "\n",
    "import random, os, pathlib, time\n",
    "from tqdm import tqdm\n",
    "# from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os, time, sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtnnlib as dtnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=[0.5,],\n",
    "        std=[0.5,],\n",
    "    ),\n",
    "])\n",
    "\n",
    "# train_dataset = datasets.FashionMNIST(root=\"../../../_Datasets/\", train=True, download=True, transform=mnist_transform)\n",
    "# test_dataset = datasets.FashionMNIST(root=\"../../../_Datasets/\", train=False, download=True, transform=mnist_transform)\n",
    "train_dataset = datasets.MNIST(root=\"../../../_Datasets/\", train=True, download=True, transform=mnist_transform)\n",
    "test_dataset = datasets.MNIST(root=\"../../../_Datasets/\", train=False, download=True, transform=mnist_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_loader = data.DataLoader(dataset=train_dataset, num_workers=4, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, num_workers=4, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 28, 28]) torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "for xx, yy in train_loader:\n",
    "    xx, yy = xx.to(device), yy.to(device)\n",
    "    print(xx.shape, yy.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Layer epsilon Softmax MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceTransform_Epsilon(dtnn.DistanceTransformBase):\n",
    "    \n",
    "    def __init__(self, input_dim, num_centers, p=2, bias=False, epsilon=0.1):\n",
    "        super().__init__(input_dim, num_centers, p=2)\n",
    "        \n",
    "        nc = num_centers\n",
    "        if epsilon is not None:\n",
    "            nc += 1\n",
    "        self.scaler = nn.Parameter(torch.log(torch.ones(1, 1)*1))\n",
    "        self.bias = nn.Parameter(torch.ones(1, nc)*0) if bias else None\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def forward(self, x):\n",
    "        dists = super().forward(x)\n",
    "        \n",
    "        if self.epsilon is not None:\n",
    "            dists = torch.cat([dists, torch.ones(len(x), 1, dtype=x.dtype)*self.epsilon], dim=1)\n",
    "        \n",
    "        ### normalize similar to UMAP\n",
    "        dists = dists/torch.sqrt(dists.var(dim=1, keepdim=True)+1e-9)\n",
    "#         dists = dists/(torch.norm(dists, dim=1, keepdim=True)+1e-9)\n",
    "        \n",
    "        ## scale the dists\n",
    "#         dists = torch.exp(-dists + self.scaler)\n",
    "        dists = 1-dists*torch.exp(self.scaler)\n",
    "    \n",
    "        if self.bias is not None: dists = dists+self.bias\n",
    "        return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTeSM(DistanceTransform_Epsilon):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, epsilon=1.0, itemp=10):\n",
    "        super().__init__(input_dim, output_dim, bias=False, epsilon=epsilon)\n",
    "        \n",
    "        self.scale_shift = dtnn.ScaleShift(-1, scaler_init=itemp, shifter_init=0, scaler_const=True, shifter_const=True)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.temp_activ = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        xo = super().forward(x)\n",
    "        xo = self.scale_shift(xo)\n",
    "        xo = self.softmax(xo)\n",
    "        self.temp_activ = xo.data\n",
    "#         return xo[:, :-1]\n",
    "        return xo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTeSM(2, 5, 0.1)(torch.randn(1, 2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalMLP_epsilonsoftmax(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim0, hidden_dim1, output_dim, epsilon0=1.0, epsilon1=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer0 = DTeSM(input_dim, hidden_dim0, epsilon=epsilon0, itemp=1.0)\n",
    "        \n",
    "        if epsilon0 is not None:\n",
    "            hidden_dim0 += 1\n",
    "        \n",
    "        self.layer1 = DTeSM(hidden_dim0, hidden_dim1, epsilon=epsilon1, itemp=3.0)\n",
    "        \n",
    "        if epsilon1 is not None:\n",
    "            hidden_dim1 += 1\n",
    "        \n",
    "#         self.activ = dtnn.OneActiv(hdim, mode='relu', beta_init=np.log(1.2))\n",
    "#         self.activ = nn.ReLU()\n",
    "\n",
    "        self.layer2 = nn.Linear(hidden_dim1, output_dim)\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        xo = self.layer0(x)\n",
    "        xo = self.layer1(xo)\n",
    "        xo = self.layer2(xo)\n",
    "        return xo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = 200\n",
    "# h1 = 60\n",
    "h1 = 100\n",
    "model = LocalMLP_epsilonsoftmax(784, h0, h1, 10, epsilon0=20.0, epsilon1=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocalMLP_epsilonsoftmax(\n",
       "  (layer0): DTeSM(\n",
       "    (scale_shift): ScaleShift()\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (layer1): DTeSM(\n",
       "    (scale_shift): ScaleShift()\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (layer2): Linear(in_features=101, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.7535, grad_fn=<MeanBackward0>),\n",
       " tensor(0.2176, grad_fn=<StdBackward0>),\n",
       " tensor(4.2989, grad_fn=<MinBackward1>),\n",
       " tensor(5.2952, grad_fn=<MaxBackward1>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = super(DistanceTransform_Epsilon, model.layer1).forward(model.layer0(xx.reshape(-1, 28*28)))\n",
    "d1.mean(), d1.std(), d1.min(), d1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_training_samples(N):\n",
    "    new_center = []\n",
    "    new_labels = []\n",
    "    count = 0\n",
    "    for i, (xx, yy) in enumerate(train_loader):\n",
    "        xx = xx.reshape(xx.shape[0], -1)\n",
    "        if count+xx.shape[0] < N:\n",
    "            new_center.append(xx)\n",
    "            new_labels.append(yy)\n",
    "            count += xx.shape[0]\n",
    "        elif count >= N:\n",
    "            break\n",
    "        else:\n",
    "            new_center.append(xx[:N-count])\n",
    "            new_labels.append(yy[:N-count])\n",
    "            count = N\n",
    "            break\n",
    "\n",
    "    new_center = torch.cat(new_center, dim=0)\n",
    "    new_labels = torch.cat(new_labels, dim=0)\n",
    "    \n",
    "    weights = torch.zeros(len(new_labels), 10)\n",
    "    for i in range(len(new_labels)):\n",
    "        weights[i, new_labels[i]] = 1.\n",
    "    \n",
    "    return new_center.to(device), weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_random_training_samples(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0, _ = get_random_training_samples(h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first layer\n",
    "model.layer0.centers.data = c0.to(model.layer0.centers.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, v1 = get_random_training_samples(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## second layer\n",
    "model.layer1.centers.data = model.layer0(c1.to(device))\n",
    "if model.layer1.epsilon is not None:\n",
    "    e = torch.ones(1, v1.shape[1])*0\n",
    "    v1 = torch.cat([v1, e], dim=0)\n",
    "    \n",
    "model.layer2.weight.data = v1.t().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([199,  46,  85,  67, 133,  54, 185, 119,  94,  29, 123, 157, 200, 172,\n",
       "          2,  40, 141, 172,  66,  58, 101,  23, 150,  73,  94, 141,   0,  13,\n",
       "         62,  66,  44, 197, 150,   8,  76,  74, 160, 114,  59, 180,  26, 120,\n",
       "        144,  38,  53,  42,  66,  26,  99,  27,  40, 165,  24, 177, 104, 181,\n",
       "          3,   0,  64,  30,  12, 173,  76,  79, 115,  50, 157,  47,  67,  82,\n",
       "          5, 119,  82,  15, 115, 104, 188,  50,   8,   8,  26, 175,  10, 117,\n",
       "        134, 101,   0,  90,  10,  15,   2, 193,  25, 181,   0,  60, 185, 162,\n",
       "         64,  44])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer1.centers.data.max(dim=1)[1] #### This is the problem for multilayer rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 10])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xx.reshape(-1, 28*28)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdasd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-efb445db9a2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0masdasd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'asdasd' is not defined"
     ]
    }
   ],
   "source": [
    "asdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "        inputs, targets = inputs.to(device).view(-1, 28*28), targets.to(device)\n",
    "        \n",
    "        ### Train with random image and \"10\" as class\n",
    "#         inputs = torch.cat([inputs, torch.rand(batch_size//10, 28*28, dtype=inputs.dtype).to(device)*2-1], dim=0)\n",
    "#         targets = torch.cat([targets, torch.ones(batch_size//10, dtype=targets.dtype).to(device)*10], dim=0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    print(f\"[Train] {epoch} Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = -1\n",
    "def test(epoch, model):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "#         for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader)):\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device).view(-1, 28*28), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "    print(f\"[Test] {epoch} Loss: {test_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "    \n",
    "    acc = 100.*correct/total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocalMLP_epsilonsoftmax(\n",
       "  (layer0): DTeSM(\n",
       "    (scale_shift): ScaleShift()\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (layer1): DTeSM(\n",
       "    (scale_shift): ScaleShift()\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (layer2): Linear(in_features=101, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 0 Loss: 2.305 | Acc: 9.820 982/10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.82"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc = test(0, model)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training - evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "# EPOCHS = 10\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1, p2 = [], []\n",
    "for p in model.named_parameters():\n",
    "    if p[0].endswith(\".centers\"):\n",
    "        p1.append(p[1])\n",
    "    else:\n",
    "        p2.append(p[1])\n",
    "\n",
    "params = [\n",
    "    {\"params\": p1, \"lr\": learning_rate*0.03}, ## default - to change little from data point\n",
    "#     {\"params\": p1},\n",
    "    {\"params\": p2},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:11<00:00, 108.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 0 Loss: 0.897 | Acc: 70.983 42590/60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 0 Loss: 0.312 | Acc: 90.960 9096/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:12<00:00, 95.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 1 Loss: 0.261 | Acc: 92.713 55628/60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 1 Loss: 0.233 | Acc: 93.270 9327/10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "best_acc = -1\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch, model, optimizer)\n",
    "    test(epoch, model)\n",
    "    scheduler.step()\n",
    "    \n",
    "\"\"\"\n",
    "Note: It trains to about 95% on MNIST\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for Adverserial Rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import foolbox as fb\n",
    "import foolbox.attacks as fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel = fb.PyTorchModel(model.eval(), bounds=(-1, 1), device=device)\n",
    "\n",
    "# attack = fa.LinfPGD()\n",
    "attack = fa.FGSM()\n",
    "# epsilons = [0.0, 0.001, 0.01, 0.03, 0.1, 0.3, 0.5, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 77.53it/s]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "failed = 0\n",
    "rejected = 0\n",
    "x_rejected = 0\n",
    "for i, (xx, yy) in enumerate(tqdm(test_loader)):\n",
    "    xx = xx.reshape(-1, 28*28)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        yout = model(xx)\n",
    "    reject0 = model.layer0.temp_activ.max(dim=1)[1] == h0\n",
    "    reject1 = model.layer1.temp_activ.max(dim=1)[1] == h1\n",
    "#     reject = torch.bitwise_or(reject0, reject1)\n",
    "#     reject = torch.bitwise_and(reject0, reject1)\n",
    "    reject = reject0\n",
    "    \n",
    "    x_rejected += int(reject.type(torch.float32).sum())\n",
    "    \n",
    "    _, advs, success = attack(fmodel, xx, yy, epsilons=0.5)\n",
    "    with torch.no_grad(): ### just to access neuron activation for adverserial examples\n",
    "        yout = model(advs)\n",
    "    reject0 = model.layer0.temp_activ.max(dim=1)[1] == h0\n",
    "    reject1 = model.layer1.temp_activ.max(dim=1)[1] == h1\n",
    "#     reject = torch.bitwise_or(reject0, reject1)\n",
    "#     reject = torch.bitwise_and(reject0, reject1)\n",
    "    reject = reject0\n",
    "    \n",
    "    \n",
    "    rejected += int(reject.type(torch.float32).sum())\n",
    "    fail = torch.bitwise_and(success, ~reject).type(torch.float32).sum()\n",
    "#     fail = success.type(torch.float32).sum()\n",
    "    failed += int(fail)    \n",
    "    count += len(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 43, 9896, 8441)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count, failed, rejected, x_rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8484"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(failed+x_rejected)/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reject1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(advs[0].reshape(28,28))\n",
    "plt.show()\n",
    "plt.imshow(xx[0].reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layer0.epsilon = 10.0\n",
    "model.layer1.epsilon = 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel = fb.PyTorchModel(model.eval(), bounds=(-1, 1), device=device)\n",
    "attack = fa.FGSM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, (xx, yy) in enumerate(tqdm(test_loader)):\n",
    "    xx = xx.reshape(-1, 28*28)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        yout = model(xx)\n",
    "    reject0 = model.layer0.temp_activ\n",
    "    reject1 = model.layer1.temp_activ\n",
    "    \n",
    "    _, advs, success = attack(fmodel, xx, yy, epsilons=0.1)\n",
    "    with torch.no_grad(): ### just to access neuron activation for adverserial examples\n",
    "        yout = model(advs)\n",
    "    _reject0 = model.layer0.temp_activ\n",
    "    _reject1 = model.layer1.temp_activ\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([200, 200, 147, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
       "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
       "        200, 136, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
       "         30, 200, 200, 200, 200, 200,  60, 200])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reject0.max(dim=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([200, 200, 147, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
       "        200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
       "        200, 136, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
       "         30, 200, 200, 200, 200, 200, 139, 200])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_reject0.max(dim=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.count_nonzero(reject0.max(dim=1)[1] != _reject0.max(dim=1)[1]) ## num changed elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([81, 81,  9, 86, 86, 86, 86, 86, 86, 72, 86, 86, 72, 72, 86, 86, 48, 86,\n",
       "        86, 48, 33, 48, 48, 86, 86, 67, 81, 83, 72, 59, 86, 81, 48, 86, 86, 48,\n",
       "        48, 86, 72, 86, 81, 86, 22, 48, 83, 86, 86, 86, 22, 48])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reject1.max(dim=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([81, 81,  9, 86, 86, 86, 86, 48, 86, 72, 86, 86, 72, 72, 86, 86, 48, 86,\n",
       "        86, 48, 86, 48, 48, 86, 86, 67, 81, 86, 82, 81, 86, 81, 48, 86, 86, 48,\n",
       "        48, 86, 72, 86, 81, 86, 22, 48, 86, 41, 86, 86, 22, 48])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_reject1.max(dim=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.4523e-08, 4.7168e-08, 2.0874e-04, 6.9645e-06, 6.0155e-02, 2.2666e-05,\n",
       "        5.6615e-10, 7.3360e-05, 1.5609e-10, 2.1486e-06, 2.0511e-08, 2.1947e-08,\n",
       "        1.8529e-05, 1.6527e-04, 5.6255e-07, 4.8054e-10, 1.9433e-04, 2.7907e-03,\n",
       "        7.3857e-04, 3.4430e-09, 2.5579e-04, 2.5400e-02, 4.8888e-08, 9.5534e-05,\n",
       "        1.8658e-07, 2.1808e-04, 1.3578e-03, 7.1720e-04, 5.7741e-02, 2.4944e-03,\n",
       "        4.0905e-04, 6.3832e-06, 1.6758e-10, 5.9255e-04, 1.6968e-05, 1.7880e-05,\n",
       "        5.4120e-04, 2.0647e-10, 3.1418e-09, 1.1336e-01, 5.2511e-10, 4.2141e-02,\n",
       "        3.9557e-04, 1.6381e-06, 1.2358e-04, 5.8208e-04, 2.2773e-01, 9.5568e-10,\n",
       "        5.3888e-03, 2.6257e-06, 7.0738e-06, 8.5563e-05, 9.5788e-04, 2.7225e-09,\n",
       "        2.8969e-03, 9.0384e-05, 3.0670e-04, 3.1689e-03, 1.4847e-10, 7.6193e-10,\n",
       "        8.0246e-03, 3.2848e-04, 5.6505e-05, 1.4291e-07, 2.1582e-05, 1.3250e-03,\n",
       "        3.2928e-09, 1.4068e-03, 1.8088e-06, 3.8148e-04, 1.3477e-10, 1.5241e-03,\n",
       "        7.9394e-03, 4.6428e-10, 9.9034e-06, 4.9698e-03, 9.5069e-05, 1.6429e-08,\n",
       "        6.6371e-06, 5.5506e-06, 2.6626e-10, 2.8388e-01, 8.7482e-02, 7.6180e-03,\n",
       "        3.3778e-07, 3.0001e-04, 2.8866e-02, 3.3124e-08, 1.9712e-05, 1.2156e-02,\n",
       "        9.9680e-07, 8.4332e-06, 4.8516e-04, 1.9612e-04, 1.2790e-04, 6.2041e-11,\n",
       "        1.9500e-07, 5.0014e-04, 2.4807e-09, 7.8261e-04, 0.0000e+00])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reject1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.3740e-08, 2.0263e-07, 5.6070e-04, 1.7922e-05, 6.2139e-02, 4.1768e-05,\n",
       "        3.1933e-09, 1.8092e-04, 9.6321e-10, 6.9309e-06, 7.8051e-08, 8.5070e-08,\n",
       "        3.6469e-05, 3.7655e-04, 1.6466e-06, 2.7399e-09, 5.0810e-04, 4.8638e-03,\n",
       "        1.5527e-03, 1.5064e-08, 6.9359e-04, 2.5628e-02, 1.7391e-07, 2.8423e-04,\n",
       "        7.3123e-07, 5.6159e-04, 2.6548e-03, 1.8791e-03, 5.4017e-02, 5.0892e-03,\n",
       "        9.5041e-04, 1.2646e-05, 1.0288e-09, 1.2426e-03, 2.9739e-05, 3.2908e-05,\n",
       "        1.4608e-03, 1.2483e-09, 1.5908e-08, 9.6714e-02, 2.9763e-09, 7.4721e-02,\n",
       "        8.8210e-04, 4.5707e-06, 3.4968e-04, 8.0729e-04, 1.7118e-01, 5.2077e-09,\n",
       "        1.2876e-02, 5.5073e-06, 1.3312e-05, 1.4665e-04, 1.9534e-03, 1.0524e-08,\n",
       "        4.6764e-03, 2.7709e-04, 4.9760e-04, 5.3275e-03, 9.1956e-10, 3.5950e-09,\n",
       "        8.5316e-03, 9.1500e-04, 8.9728e-05, 4.4756e-07, 3.9209e-05, 2.4703e-03,\n",
       "        1.4160e-08, 2.7876e-03, 3.9559e-06, 8.9228e-04, 8.4095e-10, 2.8229e-03,\n",
       "        1.5903e-02, 2.0277e-09, 1.9799e-05, 8.0171e-03, 1.5054e-04, 5.7709e-08,\n",
       "        1.9252e-05, 1.1199e-05, 1.5814e-09, 2.3700e-01, 1.0038e-01, 1.2695e-02,\n",
       "        1.2472e-06, 8.0390e-04, 5.3401e-02, 1.3581e-07, 3.9132e-05, 1.2597e-02,\n",
       "        2.8334e-06, 2.4700e-05, 1.2186e-03, 5.7252e-04, 2.7625e-04, 4.1060e-10,\n",
       "        7.6193e-07, 1.3677e-03, 1.2734e-08, 1.6910e-03, 0.0000e+00])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_reject1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Problem:\n",
    "Even some inputs will produce very high value for epsilon neuron (more than adverserial example)\n",
    "Hence, layer1 regards it as non-adverserial (no epsilon is maximized for adverserial example)\n",
    "\n",
    "Solution -> do not forward propagate the epsilon neuron (just use it for rejection) -> needs large code change\n",
    "        -> Or use residual layer.. (dtSM might have very skewed representation- difficult to disintangle..)\n",
    "\n",
    "\n",
    "Problem:\n",
    "Epsilon in second layer changes the adverserial examples.\n",
    "\n",
    "\n",
    "It requires more understanding... maybe different approach... (maybe non-normalized DT-eSM)\n",
    "        --- Try Solution in 02.12 (replace /std with /norm)\n",
    "        --- Try to use threshold value in epsilon rather than argmax !!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 4e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reject1[:, -1], _reject1[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.count_nonzero(reject1[:, -1] > a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.count_nonzero(_reject1[:, -1] > a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
