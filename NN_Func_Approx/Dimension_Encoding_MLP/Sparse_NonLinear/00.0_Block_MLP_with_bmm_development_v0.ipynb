{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9adde6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16e6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import datasets\n",
    "import random, time, os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62adff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockLinear(nn.Module):\n",
    "    def __init__(self, num_blocks, input_block_dim, output_block_dim, bias=True):\n",
    "        super().__init__()\n",
    "        self.weight = torch.randn(num_blocks, input_block_dim, output_block_dim)\n",
    "        \n",
    "        self.weight = nn.Parameter(self.weight)\n",
    "        \n",
    "        self.bias = None\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(self.weight.shape[0], 1, output_block_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         nblocks, bs, dim = x.shape[0], x.shape[1], x.shape[2]\n",
    "#         print(x.shape)\n",
    "        x = torch.bmm(x, self.weight)\n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "        return x\n",
    "    \n",
    "    def __repr__(self):\n",
    "        S = f'BlockLinear: [{self.weight.shape}]'\n",
    "        return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e71c189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bl = BlockLinear(256//4, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15aecb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 4, 5]), torch.Size([64, 1, 5]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl.weight.shape, bl.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb7eb2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl(torch.randn(64, 2, 4)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b820da62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "920de99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockMLP(nn.Module):\n",
    "    def __init__(self, input_dim, layer_dims, actf=nn.ELU):\n",
    "        super().__init__()\n",
    "        self.block_dim = layer_dims[0]\n",
    "        \n",
    "        assert input_dim%self.block_dim == 0, \"Input dim must be even number\"\n",
    "        ### Create a block MLP\n",
    "        self.mlp = []\n",
    "        n_blocks = input_dim//layer_dims[0]\n",
    "        for i in range(len(layer_dims)-1):\n",
    "            l = BlockLinear(n_blocks, layer_dims[i], layer_dims[i+1])\n",
    "#             print(l.weight.shape)\n",
    "            a = actf()\n",
    "            self.mlp.append(l)\n",
    "            self.mlp.append(a)\n",
    "        self.mlp = self.mlp[:-1]\n",
    "        self.mlp = nn.Sequential(*self.mlp)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, dim = x.shape[0], x.shape[1]\n",
    "        x = x.view(bs, -1, self.block_dim).transpose(0,1)\n",
    "        x = self.mlp(x) + x\n",
    "        x = x.transpose(1,0).reshape(bs, -1)\n",
    "        return x\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         S = f'BlockLinear: [{self.weight.shape}]'\n",
    "#         return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58300000",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlps = BlockMLP(256, [4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a13a0503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlps.mlp[0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd671902",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (4) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmlps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mBlockMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m bs, dim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(bs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(bs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (4) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "mlps(torch.randn(1, 256)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff019343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64874ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockMLP_MixerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, block_dim, hidden_layers_ratio=[2], actf=nn.ELU):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert input_dim%block_dim == 0, \"Input dim must be even number\"\n",
    "        self.input_dim = input_dim\n",
    "        self.block_dim = block_dim\n",
    "        \n",
    "        def log_base(a, base):\n",
    "            return np.log(a) / np.log(base)\n",
    "        \n",
    "        num_layers = int(np.ceil(log_base(input_dim, base=block_dim)))\n",
    "        hidden_layers_ratio = [1] + hidden_layers_ratio + [1]\n",
    "        \n",
    "        block_layer_dims = [int(a*block_dim) for a in hidden_layers_ratio]\n",
    "        self.facto_nets = []\n",
    "        for i in range(num_layers):\n",
    "            net = BlockMLP(self.input_dim, block_layer_dims, actf)\n",
    "            self.facto_nets.append(net)\n",
    "            \n",
    "        self.facto_nets = nn.ModuleList(self.facto_nets)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        y = x\n",
    "        for i, fn in enumerate(self.facto_nets):\n",
    "            y = y.view(-1, self.block_dim, self.block_dim**i).permute(0, 2, 1).contiguous().view(bs, -1)\n",
    "            y = fn(y)\n",
    "            y = y.view(-1, self.block_dim**i, self.block_dim).permute(0, 2, 1).contiguous()\n",
    "\n",
    "        y = y.view(bs, -1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c98fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 32\n",
    "M = 8\n",
    "bmlp = BlockMLP_MixerBlock(N, M, [2]) ## Input dim must be power of 2 and so does the block dim\n",
    "### Input dim / block dim = (in) 2^I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bea548d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlockMLP_MixerBlock(\n",
       "  (facto_nets): ModuleList(\n",
       "    (0): BlockMLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): BlockLinear: [torch.Size([4, 8, 16])]\n",
       "        (1): ELU(alpha=1.0)\n",
       "        (2): BlockLinear: [torch.Size([4, 16, 8])]\n",
       "      )\n",
       "    )\n",
       "    (1): BlockMLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): BlockLinear: [torch.Size([4, 8, 16])]\n",
       "        (1): ELU(alpha=1.0)\n",
       "        (2): BlockLinear: [torch.Size([4, 16, 8])]\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa323e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmlp(torch.randn(2, N)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69822fc1",
   "metadata": {},
   "source": [
    "### Finding Valid sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61b4b439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid for M:2 N:2\n",
      "Valid for M:2 N:4\n",
      "Valid for M:4 N:4\n",
      "Valid for M:2 N:8\n",
      "Valid for M:8 N:8\n",
      "Valid for M:2 N:16\n",
      "Valid for M:4 N:16\n",
      "Valid for M:16 N:16\n",
      "Valid for M:2 N:32\n",
      "Valid for M:32 N:32\n",
      "Valid for M:2 N:64\n",
      "Valid for M:4 N:64\n",
      "Valid for M:8 N:64\n",
      "Valid for M:64 N:64\n",
      "Valid for M:2 N:128\n",
      "Valid for M:128 N:128\n",
      "Valid for M:2 N:256\n",
      "Valid for M:4 N:256\n",
      "Valid for M:16 N:256\n",
      "Valid for M:256 N:256\n",
      "Valid for M:2 N:512\n",
      "Valid for M:8 N:512\n",
      "Valid for M:512 N:512\n",
      "Valid for M:2 N:1024\n",
      "Valid for M:4 N:1024\n",
      "Valid for M:32 N:1024\n",
      "Valid for M:1024 N:1024\n",
      "Valid for M:2 N:2048\n",
      "Valid for M:2048 N:2048\n",
      "Valid for M:2 N:4096\n",
      "Valid for M:4 N:4096\n",
      "Valid for M:8 N:4096\n",
      "Valid for M:16 N:4096\n",
      "Valid for M:64 N:4096\n",
      "Valid for M:4096 N:4096\n",
      "Valid for M:2 N:8192\n",
      "Valid for M:8192 N:8192\n",
      "Valid for M:2 N:16384\n",
      "Valid for M:4 N:16384\n",
      "Valid for M:128 N:16384\n",
      "Valid for M:16384 N:16384\n"
     ]
    }
   ],
   "source": [
    "valids = {}\n",
    "for p in range(1, 15):\n",
    "    N = int(2**p)\n",
    "    valids[N] = []\n",
    "    for q in range(1, p+1):\n",
    "        M = int(2**q)\n",
    "        net = BlockMLP_MixerBlock(N, M, [2])\n",
    "        try:\n",
    "            net(torch.randn(1, N))\n",
    "            print(f\"Valid for M:{M} N:{N}\")\n",
    "            valids[N].append(M)\n",
    "        except RuntimeError as e:\n",
    "#             print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36cd874b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: [2],\n",
       " 4: [2, 4],\n",
       " 8: [2, 8],\n",
       " 16: [2, 4, 16],\n",
       " 32: [2, 32],\n",
       " 64: [2, 4, 8, 64],\n",
       " 128: [2, 128],\n",
       " 256: [2, 4, 16, 256],\n",
       " 512: [2, 8, 512],\n",
       " 1024: [2, 4, 32, 1024],\n",
       " 2048: [2, 2048],\n",
       " 4096: [2, 4, 8, 16, 64, 4096],\n",
       " 8192: [2, 8192],\n",
       " 16384: [2, 4, 128, 16384]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a76a8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1.0),\n",
       " (2, 1.4142135623730951),\n",
       " (4, 2.0),\n",
       " (8, 2.8284271247461903),\n",
       " (16, 4.0),\n",
       " (32, 5.656854249492381),\n",
       " (64, 8.0),\n",
       " (128, 11.313708498984761),\n",
       " (256, 16.0),\n",
       " (512, 22.627416997969522),\n",
       " (1024, 32.0),\n",
       " (2048, 45.254833995939045),\n",
       " (4096, 64.0),\n",
       " (8192, 90.50966799187809),\n",
       " (16384, 128.0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(2**i, np.sqrt(2**i)) for i in range(15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7670c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: [2],\n",
       " 4: [2, 4],\n",
       " 8: [2, 4, 8],\n",
       " 16: [2, 4, 16],\n",
       " 32: [2, 4, 8, 32],\n",
       " 64: [2, 4, 8, 64],\n",
       " 128: [2, 4, 16, 128],\n",
       " 256: [2, 4, 8, 16, 256],\n",
       " 512: [2, 4, 8, 32, 512],\n",
       " 1024: [2, 4, 32, 1024],\n",
       " 2048: [2, 4, 8, 16, 64, 2048],\n",
       " 4096: [2, 4, 8, 16, 64, 4096]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### this is wrong\n",
    "{2: [2],\n",
    " 4: [2, 4],\n",
    " 8: [2, 4, 8],\n",
    " 16: [2, 4, 16],\n",
    " 32: [2, 4, 8, 32],\n",
    " 64: [2, 4, 8, 64],\n",
    " 128: [2, 4, 16, 128],\n",
    " 256: [2, 4, 8, 16, 256],\n",
    " 512: [2, 4, 8, 32, 512],\n",
    " 1024: [2, 4, 32, 1024],\n",
    " 2048: [2, 4, 8, 16, 64, 2048],\n",
    " 4096: [2, 4, 8, 16, 64, 4096]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "9bcaaddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 1.4142135623730951\n",
      "2 4 2.0\n",
      "3 8 2.8284271247461903\n",
      "4 16 4.0\n",
      "5 32 5.656854249492381\n",
      "6 64 8.0\n",
      "7 128 11.313708498984761\n",
      "8 256 16.0\n",
      "9 512 22.627416997969522\n"
     ]
    }
   ],
   "source": [
    "a = [print(a, 2**a, np.sqrt(2**a)) for a in range(1, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc9826b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
