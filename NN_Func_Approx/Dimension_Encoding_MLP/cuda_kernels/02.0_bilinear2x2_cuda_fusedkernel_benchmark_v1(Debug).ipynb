{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import datasets\n",
    "import random, sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cuda:1\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "test_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"../../../../_Datasets/FMNIST/\", train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.FashionMNIST(root=\"../../../../_Datasets/FMNIST/\", train=False, download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "BS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BS, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BS, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## demo of train loader\n",
    "xx, yy = iter(train_loader).next()\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda -bmm2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bmm2x2_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMM2x2Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "#     @torch.jit.ignore\n",
    "    def forward(ctx, inputs, weights):\n",
    "        outputs = bmm2x2_cuda.forward(inputs, weights)\n",
    "        ctx.save_for_backward(inputs, weights)\n",
    "        return outputs[0]\n",
    "    \n",
    "    @staticmethod\n",
    "#     @torch.jit.ignore\n",
    "    def backward(ctx, grad_output):\n",
    "        inputs, weights = ctx.saved_tensors\n",
    "        del_input, del_weights = bmm2x2_cuda.backward(\n",
    "            inputs, \n",
    "            weights, \n",
    "            grad_output)\n",
    "    \n",
    "        return del_input, del_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairWeight2(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        assert input_dim%2 == 0, \"Input dim must be even number\"\n",
    "        self.weight = torch.eye(2).unsqueeze(0).repeat_interleave(input_dim//2, dim=0)\n",
    "        self.weight = nn.Parameter(self.weight)\n",
    "        self.bmmfunc = BMM2x2Function()\n",
    "        \n",
    "    @torch.jit.ignore\n",
    "    def bmm(self, x, w):\n",
    "        return BMM2x2Function.apply(x, w)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, dim = x.shape[0], x.shape[1]\n",
    "        x = x.view(bs, -1, 2)\n",
    "        x = self.bmm(x, self.weight)\n",
    "        x = x.view(bs, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda - Bilinear2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bilinear2x2_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_input_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLinear2x2Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, weights):\n",
    "        outputs = bilinear2x2_cuda.forward(inputs, weights)\n",
    "        ctx.save_for_backward(inputs, weights)\n",
    "        return outputs[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        global del_input_list\n",
    "        inputs, weights = ctx.saved_tensors\n",
    "#         del_input, del_weights = bmm2x2_cuda.backward(\n",
    "#             grad_output.contiguous(), \n",
    "#             grad_cell.contiguous(), \n",
    "#             grad_output.contiguous())\n",
    "        del_input, del_weights = bilinear2x2_cuda.backward(\n",
    "            inputs, \n",
    "            weights, \n",
    "            grad_output)\n",
    "        del_input_list.append(del_input)\n",
    "        return del_input, del_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairBilinear2(nn.Module):\n",
    "    def __init__(self, dim, grid_width):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.grid_width = grid_width\n",
    "        \n",
    "        self.num_pairs = self.dim // 2\n",
    "        along_row = torch.linspace(0, 1, self.grid_width).reshape(1, -1).t()\n",
    "        along_col = torch.linspace(0, 1, self.grid_width).reshape(-1, 1).t()\n",
    "        \n",
    "        self.Y = torch.stack([along_row+along_col*0, along_row*0+along_col])\n",
    "        self.Y = torch.repeat_interleave(self.Y.unsqueeze(0), self.num_pairs, dim=0)\n",
    "        self.Y = nn.Parameter(self.Y)\n",
    "        \n",
    "        self.pairW = torch.eye(2).unsqueeze(0).repeat_interleave(self.num_pairs, dim=0)\n",
    "        self.pairW = nn.Parameter(self.pairW)\n",
    "    \n",
    "#     @torch.jit.ignore\n",
    "#     def pairbl2x2(self, x, w):\n",
    "#         return BiLinear2x2Function.apply(x, w)\n",
    "    \n",
    "#     @torch.jit.ignore\n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        \n",
    "############# This block ########################\n",
    "        ### this block is significantly faster\n",
    "    \n",
    "#         x = x.view(bs, -1, 2).transpose(0,1)\n",
    "#         x = torch.bmm(x, self.pairW)\n",
    "#         x = x.transpose(1,0)#.reshape(-1, 2)\n",
    "        \n",
    "############# OR This block ########################\n",
    "        x = x.contiguous().view(bs, -1, 2)\n",
    "#         x = BMM2x2Function.apply(x, self.pairW)\n",
    "####################################################\n",
    "#         x = x.view(bs, -1, 2)\n",
    "        x = BiLinear2x2Function.apply(x, self.Y)\n",
    "        x = x.view(bs, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizedPairBilinearSpline_2(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, grid_width, num_layers=None):\n",
    "        super().__init__()\n",
    "        assert input_dim%2 == 0, \"Input dim must be even number\"\n",
    "        self.input_dim = input_dim\n",
    "        self.num_layers = int(np.ceil(np.log2(self.input_dim)))\n",
    "        if num_layers is not None:\n",
    "            self.num_layers = num_layers\n",
    "            \n",
    "        self.facto_nets = []\n",
    "        for i in range(self.num_layers):\n",
    "            net = PairBilinear2(self.input_dim, grid_width)\n",
    "            self.facto_nets.append(net)\n",
    "        self.facto_nets = nn.ModuleList(self.facto_nets)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        global del_input_list\n",
    "        del_input_list = []\n",
    "        \n",
    "        ## swap first and then forward and reverse-swap\n",
    "        bs = x.shape[0]\n",
    "        y = x\n",
    "#         for i in range(len(self.facto_nets)):\n",
    "        for i, fn in enumerate(self.facto_nets):\n",
    "            y = y.view(-1,2,2**(i)).permute(0, 2,1).contiguous().view(bs, -1)\n",
    "            y = fn(y) \n",
    "            y = y.view(-1,2**(i),2).permute(0, 2,1).contiguous()\n",
    "#         y = x + y ## this is residual addition... remove if only want feed forward\n",
    "        return y.view(bs, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PairBilinearBlock_2(FactorizedPairBilinearSpline_2):\n",
    "    \n",
    "#     def __init__(self, input_dim, grid_width):\n",
    "#         num_layers = int(np.ceil(np.log2(input_dim)))\n",
    "#         extra =  2**num_layers - input_dim\n",
    "#         torch.manual_seed(123)\n",
    "#         self.selector = torch.randperm(input_dim)[:extra]\n",
    "        \n",
    "#         super().__init__(2**num_layers, grid_width)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         '''\n",
    "#         x should have dimension -> bs, M\n",
    "#         '''\n",
    "#         x = torch.cat((x, x[:, self.selector]), dim=1)\n",
    "#         return super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fused 2x2 Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fused2x2ops_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_del_input = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedBiLinear2x2Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, weights, grids):\n",
    "        outputs, input_buffer, dbg = fused2x2ops_cuda.bilinear2x2_forward(inputs, weights, grids)\n",
    "#         print(dbg)\n",
    "\n",
    "#         outputs, input_buffer = fused2x2ops_cuda.bilinear2x2_forward(inputs, weights, grids)\n",
    "        ctx.save_for_backward(input_buffer, weights, grids)\n",
    "        return outputs, dbg\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output, a):\n",
    "        global recent_del_input\n",
    "        input_buffer, weights, grids = ctx.saved_tensors\n",
    "#         del_input, del_weights = bmm2x2_cuda.backward(\n",
    "#             grad_output.contiguous(), \n",
    "#             grad_cell.contiguous(), \n",
    "#             grad_output.contiguous())\n",
    "        del_input, del_weights, del_grids, del_inputs = fused2x2ops_cuda.bilinear2x2_backward(\n",
    "            input_buffer, \n",
    "            weights, \n",
    "            grids,\n",
    "            grad_output)\n",
    "    \n",
    "        recent_del_input = del_inputs\n",
    "        return del_input, del_weights, del_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fused2x2BiLinear(nn.Module):\n",
    "    def __init__(self, dim, grid_width, num_layers = None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.grid_width = grid_width\n",
    "        \n",
    "        self.num_layers = int(np.ceil(np.log2(self.dim)))\n",
    "        if num_layers is not None:\n",
    "            self.num_layers = num_layers\n",
    "        \n",
    "        \n",
    "        self.num_pairs = self.dim // 2\n",
    "        \n",
    "#         along_row = torch.linspace(0, 1, self.grid_width).reshape(1, -1)\n",
    "#         along_col = torch.linspace(0, 1, self.grid_width).reshape(-1, 1)\n",
    "        \n",
    "        along_row = torch.linspace(0, 1, self.grid_width).reshape(1, -1).t()\n",
    "        along_col = torch.linspace(0, 1, self.grid_width).reshape(-1, 1).t()\n",
    "        \n",
    "        self.Y = torch.stack([along_row+along_col*0, along_row*0+along_col])\n",
    "        \n",
    "        ### repeat same for num_pairs\n",
    "        self.Y = torch.repeat_interleave(self.Y.unsqueeze(0), self.num_pairs, dim=0)\n",
    "        ### repeat same for num_Layers\n",
    "        self.Y = torch.repeat_interleave(self.Y.unsqueeze(0), self.num_layers, dim=0)\n",
    "        \n",
    "        print(self.Y.shape)\n",
    "        self.Y = nn.Parameter(self.Y)\n",
    "        \n",
    "        ### repeat same for num_pairs\n",
    "        self.pairW = torch.eye(2).unsqueeze(0).repeat_interleave(self.num_pairs, dim=0)\n",
    "        ### repeat same for num_Layers\n",
    "        self.pairW = self.pairW.unsqueeze(0).repeat_interleave(self.num_layers, dim=0)\n",
    "        \n",
    "        print(self.pairW.shape)\n",
    "        self.pairW = nn.Parameter(self.pairW)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.Y.data.clamp_(-10, 10)\n",
    "        x, self.dbg = FusedBiLinear2x2Function.apply(x.clone(), self.pairW, self.Y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 2, 3, 3])\n",
      "torch.Size([1, 8, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "N = 16\n",
    "fbl = Fused2x2BiLinear(N, 3, num_layers=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pbl = FactorizedPairBilinearSpline_2(N, 3, num_layers=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, N).to(device)\n",
    "y = fbl(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0.,  1.],\n",
       "          [ 1.,  2.,  3.],\n",
       "          [ 2.,  4.,  5.],\n",
       "          [ 3.,  6.,  7.],\n",
       "          [ 4.,  8.,  9.],\n",
       "          [ 5., 10., 11.],\n",
       "          [ 6., 12., 13.],\n",
       "          [ 7., 14., 15.]]]], device='cuda:0',\n",
       "       grad_fn=<FusedBiLinear2x2FunctionBackward>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.dbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6363, -0.2437, -2.7530,  1.5256,  0.8344, -0.3557,  0.6936, -0.1405,\n",
       "          0.3859,  0.4446, -1.0249,  0.5198,  0.0668, -1.0162, -1.2000,  1.0106]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6363, -0.2437, -2.7530,  1.5256,  0.8344, -0.3557,  0.6936, -0.1405,\n",
       "          0.3859,  0.4446, -1.0249,  0.5198,  0.0668, -1.0162, -1.2000,  1.0106]],\n",
       "       device='cuda:0', grad_fn=<FusedBiLinear2x2FunctionBackward>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbl = FactorizedPairBilinearSpline_2(N, 3, num_layers=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbl.facto_nets[0].Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = pbl(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6363, -0.2437, -2.7530,  1.5256,  0.8344, -0.3557,  0.6936, -0.1405,\n",
       "          0.3859,  0.4446, -1.0249,  0.5198,  0.0668, -1.0162, -1.2000,  1.0106]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6363, -0.2437, -2.7530,  1.5256,  0.8344, -0.3557,  0.6936, -0.1405,\n",
       "          0.3859,  0.4446, -1.0249,  0.5198,  0.0668, -1.0162, -1.2000,  1.0106]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -0.4274,  0.8341],\n",
       "         [ 0.0000,  0.3617, -0.7059],\n",
       "         [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000, -0.4274,  0.8341],\n",
       "         [ 0.0000,  0.3617, -0.7059],\n",
       "         [ 0.0000,  0.0000,  0.0000]]], device='cuda:0')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.Y.grad[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -0.4274,  0.8341],\n",
       "         [ 0.0000,  0.3617, -0.7059],\n",
       "         [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000, -0.4274,  0.8341],\n",
       "         [ 0.0000,  0.3617, -0.7059],\n",
       "         [ 0.0000,  0.0000,  0.0000]]], device='cuda:0')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbl.facto_nets[0].Y.grad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbl.Y.grad[0] - pbl.Y.grad.transpose(-1, -2)\n",
    "# fbl.Y.grad[0] - pbl.Y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-f8a08afa1313>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfacto_nets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "pbl.facto_nets[0].pairW.grad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.pairW.grad[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbl.facto_nets[0].pairW[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.pairW[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 16\n",
    "L = 2\n",
    "a = torch.randn(5, N).to(device)\n",
    "t = torch.randn_like(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(y, t):\n",
    "    return ((y-t)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 2, 2, 2])\n",
      "torch.Size([2, 8, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "fbl = Fused2x2BiLinear(N, 2, num_layers=L).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbl_opt = torch.optim.Adam(fbl.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbl = FactorizedPairBilinearSpline_2(N, 2, num_layers=L).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbl_opt = torch.optim.Adam(pbl.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbl.pairW.data = torch.randn_likee(fbl.pairW.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pbl.pairW.data = fbl.pairW.data[0].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]]],\n",
       "\n",
       "\n",
       "        [[[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.pairW.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]]], device='cuda:0')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbl.facto_nets[0].pairW.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbl.Y.data = torch.randn_like(fbl.Y.data)\n",
    "\n",
    "for i in range(L):\n",
    "    pbl.facto_nets[i].Y.data = fbl.Y.data[i].clone()#.transpose(-1,-2).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2788,  0.6000],\n",
       "         [ 0.2409, -2.0209]],\n",
       "\n",
       "        [[-0.8392,  0.3698],\n",
       "         [ 0.3418,  0.2273]]], device='cuda:0')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.Y.data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2788,  0.6000],\n",
       "         [ 0.2409, -2.0209]],\n",
       "\n",
       "        [[-0.8392,  0.3698],\n",
       "         [ 0.3418,  0.2273]]], device='cuda:0')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbl.facto_nets[0].Y.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fbl(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbl_opt.zero_grad()\n",
    "criterion(y,t).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = pbl(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbl_opt.zero_grad()\n",
    "criterion(y1,t).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.4147e+00,  1.9673e-01,  3.0928e+00,  8.6186e-02,  8.4433e+00,\n",
       "           2.4715e+00, -8.9128e+00,  7.5077e-01, -9.5820e-01,  1.7758e+00,\n",
       "           1.6081e+00, -2.7869e+00,  5.7434e+00, -2.2289e+01,  1.4339e+00,\n",
       "           7.0539e-01],\n",
       "         [ 1.1000e+00, -8.2764e-01,  1.5004e-01, -5.3716e-01,  1.5792e+01,\n",
       "           2.3115e+00, -1.5834e+01,  1.6151e+00, -2.5385e+00,  1.3427e+00,\n",
       "           3.6838e+00, -1.8933e+00, -6.1707e+00,  8.2751e+00, -4.6740e+00,\n",
       "          -2.1153e+00],\n",
       "         [-5.9749e+00, -2.1276e+00,  1.2178e+01,  1.4900e+00, -3.8751e+01,\n",
       "          -1.1987e+02,  4.3930e+01, -3.5777e+01,  2.8179e+00,  8.4018e-01,\n",
       "          -5.7506e+00,  4.2089e+00,  5.2919e+00, -2.0481e+01,  1.3606e+00,\n",
       "           1.7360e+00],\n",
       "         [ 1.0949e+01,  4.8650e-01, -9.5503e+00,  6.9283e-01,  1.4128e+00,\n",
       "          -1.4894e+01,  1.2217e+00, -3.3773e+00,  4.0301e-01,  7.5601e-01,\n",
       "          -1.4826e+00,  2.8505e+00,  1.1386e+01, -4.1574e+01,  5.4404e+00,\n",
       "           4.0386e+00],\n",
       "         [-2.6294e+01, -1.1045e+01,  4.8176e+01, -5.2124e-01, -2.4322e+00,\n",
       "           4.4645e+00,  2.5037e+00,  9.9313e-03, -2.8131e-01,  8.8139e-01,\n",
       "           2.1309e-01,  2.5705e+00,  4.8755e+00,  3.3209e+00,  1.1814e+00,\n",
       "           1.7929e-01]], device='cuda:0', grad_fn=<ViewBackward>),\n",
       " tensor([[-1.4147e+00,  1.9673e-01,  3.0928e+00,  8.6186e-02,  8.4433e+00,\n",
       "           2.4715e+00, -8.9128e+00,  7.5077e-01, -9.5820e-01,  1.7758e+00,\n",
       "           1.6081e+00, -2.7869e+00,  5.7434e+00, -2.2289e+01,  1.4339e+00,\n",
       "           7.0539e-01],\n",
       "         [ 1.1000e+00, -8.2764e-01,  1.5004e-01, -5.3716e-01,  1.5792e+01,\n",
       "           2.3115e+00, -1.5834e+01,  1.6151e+00, -2.5385e+00,  1.3427e+00,\n",
       "           3.6838e+00, -1.8933e+00, -6.1707e+00,  8.2751e+00, -4.6740e+00,\n",
       "          -2.1153e+00],\n",
       "         [-5.9749e+00, -2.1276e+00,  1.2178e+01,  1.4900e+00, -3.8751e+01,\n",
       "          -1.1987e+02,  4.3930e+01, -3.5777e+01,  2.8179e+00,  8.4018e-01,\n",
       "          -5.7506e+00,  4.2089e+00,  5.2919e+00, -2.0481e+01,  1.3606e+00,\n",
       "           1.7360e+00],\n",
       "         [ 1.0949e+01,  4.8650e-01, -9.5503e+00,  6.9283e-01,  1.4128e+00,\n",
       "          -1.4894e+01,  1.2217e+00, -3.3773e+00,  4.0301e-01,  7.5601e-01,\n",
       "          -1.4826e+00,  2.8505e+00,  1.1386e+01, -4.1574e+01,  5.4404e+00,\n",
       "           4.0386e+00],\n",
       "         [-2.6294e+01, -1.1045e+01,  4.8176e+01, -5.2124e-01, -2.4322e+00,\n",
       "           4.4645e+00,  2.5037e+00,  9.9313e-03, -2.8131e-01,  8.8139e-01,\n",
       "           2.1309e-01,  2.5705e+00,  4.8755e+00,  3.3209e+00,  1.1814e+00,\n",
       "           1.7929e-01]], device='cuda:0',\n",
       "        grad_fn=<FusedBiLinear2x2FunctionBackward>))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOBSERVATIONS:\\n1. The kernel for copy operation works.; Now try matrix multiplication.. also works\\n2. \\n'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "OBSERVATIONS:\n",
    "1. The kernel for copy operation works.; Now try matrix multiplication.. also works\n",
    "2. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.data-y.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbl.Y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pbl.Y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbl_opt.step()\n",
    "pbl_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbl.Y.grad[0] - pbl.Y.grad.transpose(-1, -2)\n",
    "# torch.abs(fbl.Y.grad[0] - pbl.Y.grad).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(fbl.Y.data[0] - pbl.facto_nets[0].Y.data).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0007, device='cuda:0')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(fbl.Y.grad[0] - pbl.facto_nets[0].Y.grad).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(fbl.pairW.data[0] - pbl.facto_nets[0].pairW.data).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'Tensor' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-2df52d2ca265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfacto_nets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'Tensor' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "torch.abs(fbl.pairW.grad[0] - pbl.facto_nets[0].pairW.grad).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pbl.pairW.grad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbl.pairW.grad[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbl.facto_nets[0].pairW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]]], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.pairW[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(fbl.pairW.data[0] - pbl.facto_nets[0].pairW.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dissecting the difference in first gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -2.4414e-04],\n",
       "          [ 0.0000e+00,  1.2207e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00, -3.0518e-05],\n",
       "          [ 0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00, -2.4414e-04],\n",
       "          [ 0.0000e+00,  3.0518e-05]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00,  3.8147e-06],\n",
       "          [ 7.6294e-06, -7.6294e-06]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00]]]], device='cuda:0')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.Y.grad[0] - pbl.facto_nets[0].Y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2831.5654,  1168.5298],\n",
       "          [ 1595.1970,  -766.5468]],\n",
       " \n",
       "         [[ -229.0080,    95.9249],\n",
       "          [  100.6610,   -41.4758]]], device='cuda:0'),\n",
       " tensor([[[-2831.5654,  1168.5298],\n",
       "          [ 1595.1970,  -766.5468]],\n",
       " \n",
       "         [[ -229.0080,    95.9249],\n",
       "          [  100.6610,   -41.4758]]], device='cuda:0'))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.Y.grad[0][0], pbl.facto_nets[0].Y.grad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.Y.grad[1] - pbl.facto_nets[1].Y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3237e+01,  4.6712e-01, -1.1120e+01,  6.1231e-01, -8.3015e+01,\n",
       "          1.7139e+01, -1.4816e+02, -1.4131e+01,  2.8407e-01, -4.5071e+00,\n",
       "         -3.1886e+00,  7.8135e+00, -2.2336e+01, -3.0173e+02, -2.2165e+01,\n",
       "         -1.8859e+02],\n",
       "        [ 6.4398e+00,  1.4490e+00,  2.4738e+00, -1.3046e+00, -1.0983e+02,\n",
       "          6.4152e+00, -9.0764e+02, -8.4267e+01,  2.6650e+01, -7.3618e-01,\n",
       "         -4.2920e+01,  2.8359e-01,  3.1488e+01,  3.1981e+01,  3.2400e+01,\n",
       "         -6.8669e+01],\n",
       "        [-1.9749e+02, -1.5759e+01, -8.7394e+01,  6.7024e+00,  3.5928e+02,\n",
       "         -2.4616e+03, -4.6154e+03, -7.1210e+03, -3.9389e+01,  1.0613e+01,\n",
       "         -4.8011e+01, -1.6281e+01, -2.1880e+01, -1.5191e+02, -2.0934e+01,\n",
       "         -3.0504e+02],\n",
       "        [ 8.7606e+01, -2.0669e+00, -1.8628e+02, -1.3838e+00, -1.3782e+00,\n",
       "          1.2751e+02, -2.2942e+01,  2.6093e+02,  6.1948e-01,  5.2663e+00,\n",
       "          2.8668e-01, -8.2319e+00, -4.4537e+01, -3.3690e+02, -4.5247e+01,\n",
       "         -1.0804e+03],\n",
       "        [-7.1770e+02, -5.7989e+01, -1.7380e+03,  6.4581e+01,  1.1398e+01,\n",
       "         -1.7213e+01, -2.8985e+01,  4.6807e+01, -1.7356e+00,  4.5675e+00,\n",
       "         -6.7960e+00, -1.1378e+01, -2.4416e+01,  3.9939e-01, -2.4287e+01,\n",
       "          2.9665e+01]], device='cuda:0')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recent_del_input[1]#.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3237e+01, -1.1120e+01],\n",
       "         [ 4.6712e-01,  6.1231e-01],\n",
       "         [-8.3015e+01, -1.4816e+02],\n",
       "         [ 1.7139e+01, -1.4131e+01],\n",
       "         [ 2.8407e-01, -3.1886e+00],\n",
       "         [-4.5071e+00,  7.8135e+00],\n",
       "         [-2.2336e+01, -2.2165e+01],\n",
       "         [-3.0173e+02, -1.8859e+02]],\n",
       "\n",
       "        [[ 6.4398e+00,  2.4738e+00],\n",
       "         [ 1.4490e+00, -1.3046e+00],\n",
       "         [-1.0983e+02, -9.0764e+02],\n",
       "         [ 6.4152e+00, -8.4267e+01],\n",
       "         [ 2.6650e+01, -4.2920e+01],\n",
       "         [-7.3618e-01,  2.8359e-01],\n",
       "         [ 3.1488e+01,  3.2400e+01],\n",
       "         [ 3.1981e+01, -6.8669e+01]],\n",
       "\n",
       "        [[-1.9749e+02, -8.7394e+01],\n",
       "         [-1.5759e+01,  6.7024e+00],\n",
       "         [ 3.5928e+02, -4.6154e+03],\n",
       "         [-2.4616e+03, -7.1210e+03],\n",
       "         [-3.9389e+01, -4.8011e+01],\n",
       "         [ 1.0613e+01, -1.6281e+01],\n",
       "         [-2.1880e+01, -2.0934e+01],\n",
       "         [-1.5191e+02, -3.0504e+02]],\n",
       "\n",
       "        [[ 8.7606e+01, -1.8628e+02],\n",
       "         [-2.0669e+00, -1.3838e+00],\n",
       "         [-1.3782e+00, -2.2942e+01],\n",
       "         [ 1.2751e+02,  2.6093e+02],\n",
       "         [ 6.1948e-01,  2.8668e-01],\n",
       "         [ 5.2663e+00, -8.2319e+00],\n",
       "         [-4.4537e+01, -4.5247e+01],\n",
       "         [-3.3690e+02, -1.0804e+03]],\n",
       "\n",
       "        [[-7.1770e+02, -1.7380e+03],\n",
       "         [-5.7989e+01,  6.4581e+01],\n",
       "         [ 1.1398e+01, -2.8985e+01],\n",
       "         [-1.7213e+01,  4.6807e+01],\n",
       "         [-1.7356e+00, -6.7960e+00],\n",
       "         [ 4.5675e+00, -1.1378e+01],\n",
       "         [-2.4416e+01, -2.4287e+01],\n",
       "         [ 3.9939e-01,  2.9665e+01]]], device='cuda:0')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_input_list[0]#.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.5367e-07,  2.9802e-08,  0.0000e+00,  5.9605e-08,  0.0000e+00,\n",
       "          0.0000e+00,  1.5259e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  6.1035e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          3.8147e-06,  0.0000e+00, -1.9073e-06,  1.9073e-06, -3.8147e-06,\n",
       "          0.0000e+00],\n",
       "        [ 1.5259e-05,  0.0000e+00, -7.6294e-06,  0.0000e+00, -3.0518e-05,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  1.9073e-06,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  1.5259e-05,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  5.9605e-08,  0.0000e+00,\n",
       "         -1.1921e-07,  0.0000e+00,  0.0000e+00,  3.0518e-05,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.5367e-07,\n",
       "         -1.9073e-06,  0.0000e+00,  0.0000e+00, -1.1921e-07,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00, -1.9073e-06,  2.9802e-08,  0.0000e+00,\n",
       "          0.0000e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_input_list[0].reshape(5, -1, 2, 2).transpose(-1, -2).reshape(5, -1) - recent_del_input[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedPairBilinearBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, grid_width):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_layers = int(np.ceil(np.log2(input_dim)))\n",
    "        \n",
    "        extra =  2**self.num_layers - input_dim\n",
    "        torch.manual_seed(123)\n",
    "        self.selector = torch.randperm(self.input_dim)[:extra]\n",
    "        \n",
    "        self.fused_pair_bilinear = Fused2x2BiLinear(2**self.num_layers, grid_width=grid_width, )#num_layers=5)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x should have dimension -> bs, M\n",
    "        '''\n",
    "        x = torch.cat((x, x[:, self.selector]), dim=1)\n",
    "        return self.fused_pair_bilinear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasLayer(nn.Module):\n",
    "    def __init__(self, dim, init_val=0):\n",
    "        super().__init__()\n",
    "        self.bias = nn.Parameter(torch.ones(dim)*init_val)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x+self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        H = 512\n",
    "        self.bias = nn.Linear(784, H)\n",
    "        self.bn1 = nn.BatchNorm1d(H)\n",
    "        self.fc = nn.Linear(H, 10)\n",
    "        self.la1 = FusedPairBilinearBlock(H, grid_width=3)\n",
    "#         self.la1 = PairBilinearBlock_2(H, grid_width=3)\n",
    "\n",
    "#         self.bias = BiasLayer(784)\n",
    "#         self.la1 = FusedPairBilinearBlock(784, grid_width=3)\n",
    "# #         self.la1 = PairBilinearBlock(784, grid_width=3)\n",
    "#         self.bn1 = nn.BatchNorm1d(1024)\n",
    "#         self.fc = nn.Linear(1024, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bias(x)\n",
    "        x = self.la1(x)\n",
    "#         x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 256, 2, 3, 3])\n",
      "torch.Size([9, 256, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FactorNet2(\n",
       "  (bias): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (la1): FusedPairBilinearBlock(\n",
       "    (fused_pair_bilinear): Fused2x2BiLinear()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = FactorNet2().to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 300/300 [00:08<00:00, 33.92it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0,  Loss:3.2544753551483154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:00<00:00, 59.48it/s]\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:63.77%, Test Acc:70.41%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 9/300 [00:00<00:10, 27.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11, NAN Grid back grad; loss 2.1953336815572767e+23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 11/300 [00:00<00:13, 21.76it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1,  Loss:2.1953336815572767e+23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:00<00:00, 63.06it/s]\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:69.91%, Test Acc:64.15%\n",
      "\n",
      "0, NAN Grid back grad; loss 1.398152815470647e+27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-b4ba346f83ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mtrain_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtrain_count\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mtrain_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train_acc = 0\n",
    "    train_count = 0\n",
    "    i = -1\n",
    "    for xx, yy in tqdm(train_loader):\n",
    "        i += 1 \n",
    "        xx = xx.view(xx.shape[0], -1)\n",
    "        xx, yy = xx.to(device), yy.to(device)\n",
    "\n",
    "        yout = model(xx)\n",
    "        \n",
    "        if torch.any(torch.isnan(yout.data)):\n",
    "            print(f\"{i},Yout; NAN\", flush=True)\n",
    "            break\n",
    "            \n",
    "        loss = criterion(yout, yy)\n",
    "        \n",
    "        if torch.any(torch.isnan(loss.data)):\n",
    "            print(f\"{i},loss; NAN\", flush=True)\n",
    "            break\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if torch.any(torch.isnan(model.la1.fused_pair_bilinear.Y.grad)):\n",
    "            print(f\"{i}, NAN Grid back grad; loss {float(loss)}\", flush=True)\n",
    "            break\n",
    "        if torch.any(torch.isnan(model.la1.fused_pair_bilinear.pairW.grad)):\n",
    "            print(f\"{i}, NAN Weight back grad; loss {float(loss)}\", flush=True)\n",
    "            break\n",
    "\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(float(loss))\n",
    "\n",
    "        outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "        correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "        train_acc += correct\n",
    "        train_count += len(outputs)\n",
    "        \n",
    "        if torch.any(torch.isnan(model.la1.fused_pair_bilinear.Y)):\n",
    "            print(f\"{i}, NAN Grid update; loss {float(loss)}\", flush=True)\n",
    "            break\n",
    "        if torch.any(torch.isnan(model.la1.fused_pair_bilinear.pairW)):\n",
    "            print(f\"{i}, NAN Weight update; loss {float(loss)}\", flush=True)\n",
    "            break\n",
    "\n",
    "    train_accs.append(float(train_acc)/train_count*100)\n",
    "    train_acc = 0\n",
    "    train_count = 0\n",
    "\n",
    "    print(f'Epoch: {epoch},  Loss:{float(loss)}')\n",
    "    test_count = 0\n",
    "    test_acc = 0\n",
    "    for xx, yy in tqdm(test_loader):\n",
    "        xx = xx.view(xx.shape[0], -1)\n",
    "        xx, yy = xx.to(device), yy.to(device)\n",
    "        with torch.no_grad():\n",
    "            yout = model(xx)\n",
    "        outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "        correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "        test_acc += correct\n",
    "        test_count += len(xx)\n",
    "    test_accs.append(float(test_acc)/test_count*100)\n",
    "    print(f'Train Acc:{train_accs[-1]:.2f}%, Test Acc:{test_accs[-1]:.2f}%')\n",
    "    print()\n",
    "\n",
    "### after each class index is finished training\n",
    "print(f'\\t-> Train Acc {max(train_accs)} ; Test Acc {max(test_accs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0163, device='cuda:0')"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.la1.fused_pair_bilinear.Y.data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:0')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.any(torch.isnan(model.la1.fused_pair_bilinear.Y.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "yout = model(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:0')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.any(torch.isnan(yout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 1, 4, 5, 8, 3, 5, 7, 6, 4, 5, 9, 0, 0, 6, 9, 4, 1, 7, 0, 4, 1, 6, 8,\n",
       "        6, 6, 5, 0, 2, 1, 8, 1, 9, 6, 5, 6, 6, 4, 0, 8, 6, 3, 4, 2, 4, 0, 7, 4,\n",
       "        8, 9, 7, 9, 1, 1, 5, 7, 7, 3, 7, 1, 8, 4, 0, 2, 7, 3, 5, 4, 4, 9, 9, 8,\n",
       "        4, 1, 9, 0, 7, 9, 0, 3, 6, 0, 2, 5, 0, 9, 5, 9, 0, 9, 1, 3, 4, 5, 4, 9,\n",
       "        4, 3, 0, 5, 8, 2, 6, 2, 5, 4, 5, 7, 7, 4, 1, 8, 0, 8, 8, 9, 3, 1, 2, 7,\n",
       "        7, 3, 1, 4, 4, 0, 6, 3, 8, 4, 5, 4, 0, 3, 2, 0, 5, 3, 4, 0, 3, 5, 5, 8,\n",
       "        9, 1, 3, 0, 1, 2, 2, 2, 6, 7, 3, 0, 7, 6, 8, 7, 1, 2, 1, 2, 4, 4, 5, 9,\n",
       "        0, 0, 8, 8, 9, 3, 6, 5, 3, 4, 8, 3, 9, 8, 0, 9, 3, 4, 5, 5, 1, 5, 9, 7,\n",
       "        7, 7, 6, 8, 3, 5, 6, 6], device='cuda:0')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(yout, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(134, device='cuda:0')"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yout.argmax()//10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1605e+29, -7.1041e+28, -1.0751e+29,  9.6640e+27, -1.3566e+29,\n",
       "         1.1653e+29,  1.0879e+29,  1.7212e+29, -1.9855e+28,  1.7179e+29],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yout[134]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1605e+29, -7.1041e+28, -1.0751e+29,  9.6640e+27, -1.3566e+29,\n",
       "         1.1653e+29,  1.0879e+29,  1.7212e+29, -1.9855e+28,  1.7179e+29],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yout[134]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[ 3.8739e-03, -7.1850e-03, -9.2713e-03],\n",
       "           [ 5.0880e-01,  4.9659e-01,  5.0152e-01],\n",
       "           [ 1.0121e+00,  1.0031e+00,  1.0011e+00]],\n",
       "\n",
       "          [[-2.2405e-03,  5.0718e-01,  1.0115e+00],\n",
       "           [-5.2883e-03,  4.9287e-01,  9.8861e-01],\n",
       "           [-8.6327e-03,  4.9361e-01,  9.9927e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 2.1228e-03, -7.1292e-04, -8.8196e-03],\n",
       "           [ 5.0610e-01,  5.0579e-01,  4.9202e-01],\n",
       "           [ 1.0066e+00,  1.0050e+00,  9.9415e-01]],\n",
       "\n",
       "          [[-1.5808e-03,  5.0555e-01,  1.0099e+00],\n",
       "           [-4.6353e-03,  5.0876e-01,  1.0075e+00],\n",
       "           [-4.6236e-03,  5.0164e-01,  1.0042e+00]]],\n",
       "\n",
       "\n",
       "         [[[-4.0497e-03,  3.1962e-03, -1.1414e-02],\n",
       "           [ 5.0873e-01,  4.9291e-01,  5.1071e-01],\n",
       "           [ 1.0103e+00,  9.9000e-01,  1.0000e+00]],\n",
       "\n",
       "          [[-1.8174e-03,  5.0718e-01,  1.0113e+00],\n",
       "           [-1.7545e-03,  4.9468e-01,  9.8872e-01],\n",
       "           [-9.1505e-03,  5.0697e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[ 5.7747e-03,  1.6783e-03,  6.3968e-04],\n",
       "           [ 5.0821e-01,  5.0801e-01,  5.0089e-01],\n",
       "           [ 1.0078e+00,  1.0080e+00,  1.0002e+00]],\n",
       "\n",
       "          [[ 1.2220e-03,  5.0750e-01,  1.0042e+00],\n",
       "           [ 7.9333e-04,  5.0590e-01,  1.0049e+00],\n",
       "           [-1.0170e-02,  4.8871e-01,  1.0021e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 3.7518e-03,  1.9155e-03, -1.5652e-03],\n",
       "           [ 5.0493e-01,  5.0033e-01,  4.9700e-01],\n",
       "           [ 1.0032e+00,  1.0016e+00,  1.0031e+00]],\n",
       "\n",
       "          [[-7.0945e-03,  5.0780e-01,  1.0072e+00],\n",
       "           [-4.1136e-03,  5.0432e-01,  1.0090e+00],\n",
       "           [-4.1467e-03,  4.9808e-01,  1.0054e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 8.6279e-03, -5.7989e-03, -3.5809e-03],\n",
       "           [ 5.0906e-01,  5.0636e-01,  5.0715e-01],\n",
       "           [ 1.0015e+00,  1.0079e+00,  1.0100e+00]],\n",
       "\n",
       "          [[-7.2307e-03,  4.9972e-01,  1.0053e+00],\n",
       "           [ 2.1584e-03,  5.0652e-01,  1.0083e+00],\n",
       "           [-2.4006e-03,  5.0859e-01,  1.0116e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[-2.7277e-03,  3.2876e-03, -5.3106e-04],\n",
       "           [ 5.0623e-01,  5.0842e-01,  5.0946e-01],\n",
       "           [ 1.0055e+00,  1.0117e+00,  1.0108e+00]],\n",
       "\n",
       "          [[-7.0886e-04,  5.0482e-01,  1.0062e+00],\n",
       "           [ 4.2480e-03,  5.0891e-01,  1.0059e+00],\n",
       "           [ 1.0948e-03,  5.0911e-01,  1.0005e+00]]],\n",
       "\n",
       "\n",
       "         [[[-1.9261e-03, -7.1370e-03, -1.0766e-02],\n",
       "           [ 5.0466e-01,  5.0520e-01,  4.9725e-01],\n",
       "           [ 1.0094e+00,  1.0095e+00,  1.0073e+00]],\n",
       "\n",
       "          [[-3.7561e-03,  5.0887e-01,  1.0118e+00],\n",
       "           [ 2.6261e-03,  5.0588e-01,  1.0105e+00],\n",
       "           [-9.6051e-03,  4.8931e-01,  1.0010e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 1.2857e-03, -7.5019e-03, -6.6959e-03],\n",
       "           [ 5.0911e-01,  4.9171e-01,  5.0973e-01],\n",
       "           [ 1.0102e+00,  9.9272e-01,  1.0048e+00]],\n",
       "\n",
       "          [[-2.6751e-03,  5.0491e-01,  1.0115e+00],\n",
       "           [-5.8509e-03,  4.9645e-01,  9.8829e-01],\n",
       "           [ 2.4669e-03,  5.0976e-01,  1.0049e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[-6.0188e-03, -6.3389e-03, -1.7730e-03],\n",
       "           [ 5.0339e-01,  4.9746e-01,  4.9858e-01],\n",
       "           [ 1.0098e+00,  1.0054e+00,  1.0013e+00]],\n",
       "\n",
       "          [[ 5.0044e-03,  5.0903e-01,  1.0037e+00],\n",
       "           [-6.2238e-03,  5.0719e-01,  1.0052e+00],\n",
       "           [ 5.9113e-03,  5.0953e-01,  1.0054e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 1.9923e-03,  1.6868e-03,  4.8960e-03],\n",
       "           [ 5.0086e-01,  5.0327e-01,  5.0301e-01],\n",
       "           [ 1.0008e+00,  1.0061e+00,  1.0012e+00]],\n",
       "\n",
       "          [[ 5.6634e-03,  5.0945e-01,  1.0112e+00],\n",
       "           [-5.8359e-04,  5.0387e-01,  1.0063e+00],\n",
       "           [-4.1251e-03,  4.9557e-01,  9.9933e-01]]],\n",
       "\n",
       "\n",
       "         [[[-6.1368e-03, -5.8157e-03,  5.4839e-04],\n",
       "           [ 5.0413e-01,  5.0546e-01,  5.0684e-01],\n",
       "           [ 1.0071e+00,  1.0095e+00,  1.0096e+00]],\n",
       "\n",
       "          [[-7.8390e-03,  5.0264e-01,  1.0068e+00],\n",
       "           [-5.9887e-03,  5.0770e-01,  1.0106e+00],\n",
       "           [-4.2521e-03,  5.0484e-01,  1.0074e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 2.7471e-03, -5.8900e-03, -3.3923e-03],\n",
       "           [ 5.0811e-01,  4.9171e-01,  5.0208e-01],\n",
       "           [ 1.0120e+00,  9.8809e-01,  1.0000e+00]],\n",
       "\n",
       "          [[-4.4148e-03,  5.0789e-01,  1.0105e+00],\n",
       "           [ 3.8651e-03,  4.9827e-01,  9.8982e-01],\n",
       "           [-8.2856e-03,  5.0855e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[-4.7712e-03, -7.8765e-04, -8.5125e-03],\n",
       "           [ 5.0639e-01,  4.9365e-01,  4.9149e-01],\n",
       "           [ 1.0115e+00,  9.8846e-01,  9.9957e-01]],\n",
       "\n",
       "          [[-1.1042e-03,  5.0755e-01,  1.0111e+00],\n",
       "           [-6.7578e-03,  5.0632e-01,  1.0100e+00],\n",
       "           [-1.2818e-02,  5.1352e-01,  1.0009e+00]]],\n",
       "\n",
       "\n",
       "         [[[-4.2210e-03,  7.1580e-03, -6.6906e-03],\n",
       "           [ 5.0080e-01,  5.0943e-01,  5.0318e-01],\n",
       "           [ 1.0065e+00,  1.0065e+00,  9.9974e-01]],\n",
       "\n",
       "          [[-3.9952e-03,  5.0538e-01,  1.0084e+00],\n",
       "           [-4.2115e-03,  5.0552e-01,  1.0119e+00],\n",
       "           [-1.7008e-03,  5.0063e-01,  1.0112e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[-7.0055e-03, -2.2304e-03, -5.7440e-03],\n",
       "           [ 4.9506e-01,  5.0709e-01,  5.0152e-01],\n",
       "           [ 1.0097e+00,  1.0068e+00,  1.0105e+00]],\n",
       "\n",
       "          [[-6.0680e-03,  5.0569e-01,  1.0090e+00],\n",
       "           [-4.6240e-03,  5.0574e-01,  1.0094e+00],\n",
       "           [-9.0193e-03,  4.9196e-01,  9.9279e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 6.8388e-03,  5.6375e-03, -5.9970e-03],\n",
       "           [ 5.0997e-01,  5.1092e-01,  4.9826e-01],\n",
       "           [ 1.0094e+00,  1.0100e+00,  1.0008e+00]],\n",
       "\n",
       "          [[ 1.9762e-03,  5.0716e-01,  1.0096e+00],\n",
       "           [ 3.2516e-03,  4.9863e-01,  9.9458e-01],\n",
       "           [-1.0265e-02,  4.8963e-01,  9.9697e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 6.9305e-03, -7.0140e-03,  3.7667e-03],\n",
       "           [ 5.1045e-01,  5.0396e-01,  5.0955e-01],\n",
       "           [ 1.0062e+00,  1.0025e+00,  1.0078e+00]],\n",
       "\n",
       "          [[-5.7817e-03,  5.0722e-01,  1.0087e+00],\n",
       "           [-9.2615e-03,  5.0611e-01,  1.0110e+00],\n",
       "           [-5.8023e-03,  5.0101e-01,  1.0057e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 4.8295e-03, -6.7547e-03, -6.5866e-03],\n",
       "           [ 5.0847e-01,  5.0108e-01,  4.9489e-01],\n",
       "           [ 1.0120e+00,  1.0111e+00,  9.9990e-01]],\n",
       "\n",
       "          [[-2.3302e-03,  5.0487e-01,  1.0099e+00],\n",
       "           [-5.2279e-03,  4.9312e-01,  9.8959e-01],\n",
       "           [ 3.0743e-03,  5.0250e-01,  9.9460e-01]]],\n",
       "\n",
       "\n",
       "         [[[-5.6923e-03,  3.6879e-03, -9.5689e-03],\n",
       "           [ 5.0381e-01,  5.0322e-01,  4.8996e-01],\n",
       "           [ 1.0111e+00,  9.9452e-01,  9.9810e-01]],\n",
       "\n",
       "          [[ 1.5245e-03,  5.0892e-01,  1.0122e+00],\n",
       "           [-3.6260e-03,  4.9916e-01,  1.0063e+00],\n",
       "           [ 4.0377e-03,  5.0554e-01,  1.0019e+00]]],\n",
       "\n",
       "\n",
       "         [[[-3.6029e-03,  5.6698e-03, -7.3938e-03],\n",
       "           [ 5.0464e-01,  5.0960e-01,  4.9766e-01],\n",
       "           [ 1.0076e+00,  1.0061e+00,  9.9993e-01]],\n",
       "\n",
       "          [[ 1.7794e-03,  5.0789e-01,  1.0083e+00],\n",
       "           [-1.8107e-03,  5.0518e-01,  9.9698e-01],\n",
       "           [-4.5790e-03,  4.9639e-01,  9.9890e-01]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[-3.6970e-03,  2.4594e-03, -1.1298e-03],\n",
       "           [ 5.0824e-01,  5.0851e-01,  4.9744e-01],\n",
       "           [ 1.0121e+00,  1.0117e+00,  1.0087e+00]],\n",
       "\n",
       "          [[-5.1148e-03,  5.0403e-01,  1.0050e+00],\n",
       "           [-3.5023e-03,  5.0383e-01,  1.0055e+00],\n",
       "           [-1.0307e-02,  4.9082e-01,  9.9737e-01]]],\n",
       "\n",
       "\n",
       "         [[[-7.5901e-03, -1.3009e-03,  3.4449e-04],\n",
       "           [ 4.9621e-01,  5.0541e-01,  5.0792e-01],\n",
       "           [ 1.0064e+00,  1.0077e+00,  1.0098e+00]],\n",
       "\n",
       "          [[ 8.5764e-03,  5.1062e-01,  1.0051e+00],\n",
       "           [-2.5973e-03,  5.0658e-01,  1.0089e+00],\n",
       "           [-7.5496e-04,  5.0589e-01,  1.0106e+00]]],\n",
       "\n",
       "\n",
       "         [[[-5.0072e-03, -6.8965e-03,  4.0084e-03],\n",
       "           [ 5.0433e-01,  5.0178e-01,  5.0924e-01],\n",
       "           [ 1.0109e+00,  1.0112e+00,  1.0013e+00]],\n",
       "\n",
       "          [[-7.1589e-03,  5.0389e-01,  1.0067e+00],\n",
       "           [-2.0485e-03,  5.0750e-01,  1.0110e+00],\n",
       "           [-5.8941e-03,  4.9186e-01,  1.0065e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 4.2379e-03, -5.2726e-03, -9.4778e-03],\n",
       "           [ 5.0798e-01,  4.9883e-01,  4.9221e-01],\n",
       "           [ 1.0113e+00,  1.0096e+00,  1.0090e+00]],\n",
       "\n",
       "          [[-6.3790e-03,  5.0340e-01,  1.0123e+00],\n",
       "           [ 2.2968e-03,  5.0884e-01,  9.9951e-01],\n",
       "           [ 6.0216e-03,  5.1137e-01,  1.0096e+00]]],\n",
       "\n",
       "\n",
       "         [[[-2.2283e-03, -7.1779e-03, -1.0136e-02],\n",
       "           [ 5.0612e-01,  4.9728e-01,  4.9137e-01],\n",
       "           [ 1.0105e+00,  1.0091e+00,  9.9999e-01]],\n",
       "\n",
       "          [[ 6.5463e-03,  5.1033e-01,  1.0112e+00],\n",
       "           [-1.4670e-03,  5.0934e-01,  1.0100e+00],\n",
       "           [-6.3131e-03,  5.0300e-01,  1.0038e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 2.2493e-03, -2.3190e-03, -9.3639e-03],\n",
       "           [ 5.0810e-01,  5.0309e-01,  4.9134e-01],\n",
       "           [ 1.0065e+00,  1.0054e+00,  1.0022e+00]],\n",
       "\n",
       "          [[-4.7918e-03,  5.0539e-01,  1.0099e+00],\n",
       "           [-1.3765e-03,  5.0777e-01,  1.0105e+00],\n",
       "           [-1.0408e-03,  5.0311e-01,  1.0031e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[-7.6581e-04,  3.4900e-03, -4.8500e-03],\n",
       "           [ 5.1101e-01,  5.1008e-01,  5.0929e-01],\n",
       "           [ 1.0018e+00,  1.0022e+00,  1.0022e+00]],\n",
       "\n",
       "          [[-6.9503e-03,  5.0558e-01,  1.0099e+00],\n",
       "           [ 9.0831e-03,  4.9606e-01,  9.9004e-01],\n",
       "           [ 3.4717e-04,  4.9976e-01,  9.9865e-01]]],\n",
       "\n",
       "\n",
       "         [[[-7.9344e-03, -8.9175e-03, -1.0694e-02],\n",
       "           [ 4.9960e-01,  4.9071e-01,  4.8955e-01],\n",
       "           [ 1.0116e+00,  1.0110e+00,  9.9868e-01]],\n",
       "\n",
       "          [[ 8.1760e-03,  5.0978e-01,  1.0105e+00],\n",
       "           [ 5.4131e-03,  5.0970e-01,  1.0097e+00],\n",
       "           [-1.1124e-02,  4.8938e-01,  1.0048e+00]]],\n",
       "\n",
       "\n",
       "         [[[-8.1870e-03, -6.3868e-03, -7.8573e-03],\n",
       "           [ 5.0661e-01,  4.9947e-01,  4.8831e-01],\n",
       "           [ 1.0125e+00,  1.0122e+00,  9.9356e-01]],\n",
       "\n",
       "          [[-2.9084e-03,  5.0566e-01,  1.0081e+00],\n",
       "           [-4.5987e-03,  5.0522e-01,  1.0101e+00],\n",
       "           [-1.0092e-02,  5.0844e-01,  1.0107e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 1.0860e-03, -3.4550e-04,  5.3849e-04],\n",
       "           [ 5.0761e-01,  5.0724e-01,  5.0399e-01],\n",
       "           [ 1.0102e+00,  1.0119e+00,  1.0108e+00]],\n",
       "\n",
       "          [[-3.8183e-03,  5.0279e-01,  1.0023e+00],\n",
       "           [-4.8226e-03,  5.0183e-01,  1.0024e+00],\n",
       "           [-1.6640e-03,  5.1173e-01,  1.0112e+00]]],\n",
       "\n",
       "\n",
       "         [[[-4.0612e-03, -1.7095e-03, -1.0858e-02],\n",
       "           [ 5.0536e-01,  4.9769e-01,  4.8987e-01],\n",
       "           [ 1.0114e+00,  9.9063e-01,  1.0010e+00]],\n",
       "\n",
       "          [[ 2.8383e-03,  5.0807e-01,  1.0117e+00],\n",
       "           [-7.8657e-03,  4.9372e-01,  9.9500e-01],\n",
       "           [-8.6105e-03,  4.9129e-01,  9.9841e-01]]],\n",
       "\n",
       "\n",
       "         [[[-2.2867e-03,  5.0478e-03, -1.6746e-03],\n",
       "           [ 5.0655e-01,  5.0778e-01,  4.9687e-01],\n",
       "           [ 1.0076e+00,  1.0039e+00,  1.0022e+00]],\n",
       "\n",
       "          [[-2.3439e-03,  5.0042e-01,  1.0010e+00],\n",
       "           [-1.0247e-03,  5.0494e-01,  1.0030e+00],\n",
       "           [ 4.4360e-03,  5.0679e-01,  1.0038e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[-6.6373e-03, -7.5709e-03, -1.7476e-03],\n",
       "           [ 5.0778e-01,  5.0579e-01,  5.0887e-01],\n",
       "           [ 1.0119e+00,  1.0112e+00,  1.0067e+00]],\n",
       "\n",
       "          [[-2.2274e-03,  5.0613e-01,  1.0093e+00],\n",
       "           [-7.5078e-03,  4.9761e-01,  1.0091e+00],\n",
       "           [-1.1068e-02,  4.9402e-01,  1.0017e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 7.3583e-04,  1.4751e-03,  8.9991e-03],\n",
       "           [ 5.0790e-01,  5.0836e-01,  5.1128e-01],\n",
       "           [ 1.0119e+00,  1.0097e+00,  1.0031e+00]],\n",
       "\n",
       "          [[ 9.0251e-04,  5.0726e-01,  1.0084e+00],\n",
       "           [ 9.7070e-03,  5.0988e-01,  1.0118e+00],\n",
       "           [-1.1096e-02,  4.9021e-01,  1.0050e+00]]],\n",
       "\n",
       "\n",
       "         [[[-5.0096e-03, -1.3796e-03,  4.9093e-03],\n",
       "           [ 5.0410e-01,  5.0768e-01,  5.0991e-01],\n",
       "           [ 1.0102e+00,  1.0100e+00,  1.0105e+00]],\n",
       "\n",
       "          [[-6.2256e-03,  5.0347e-01,  1.0049e+00],\n",
       "           [-3.6623e-03,  5.0494e-01,  1.0081e+00],\n",
       "           [ 4.0655e-04,  5.0386e-01,  1.0090e+00]]]]], device='cuda:0')"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.la1.fused_pair_bilinear.Y.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
