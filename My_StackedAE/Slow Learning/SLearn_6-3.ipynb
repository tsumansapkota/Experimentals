{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE parameters\n",
    "num_label = 10\n",
    "label_hid1 = 5\n",
    "label_hid2 = 3\n",
    "label_hid3 = 2\n",
    "label_hid4 = 1\n",
    "\n",
    "num_image = 784 # 28*28\n",
    "image_hid1 = 392\n",
    "image_hid2 = 196\n",
    "image_hid3 = 98\n",
    "image_hid4 = 49\n",
    "image_hid5 = 25\n",
    "image_hid6 = 12\n",
    "image_hid7 = 6\n",
    "image_hid8 = 3\n",
    "image_hid9 = 2\n",
    "image_hid10 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tie_weight = tf.transpose\n",
    "act_func = lambda X:X\n",
    "initializer = tf.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLACEHOLDER DEFINATION\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_image])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, num_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Encoded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMAGE COMPRESSION\n",
    "wi1 = tf.Variable(np.load('../save/weights/w1.npy'))\n",
    "wi2 = tf.Variable(np.load('../save/weights/w2.npy'))\n",
    "wi3 = tf.Variable(np.load('../save/weights/w3.npy'))\n",
    "wi4 = tf.Variable(np.load('../save/weights/w4.npy'))\n",
    "wi5 = tf.Variable(np.load('../save/weights/w5.npy'))\n",
    "wi6 = tf.Variable(np.load('../save/weights/w6.npy'))\n",
    "wi7 = tf.Variable(np.load('../save/weights/w7.npy'))\n",
    "wi8 = tf.Variable(np.load('../save/weights/w8.npy'))\n",
    "wi9 = tf.Variable(np.load('../save/weights/w9.npy'))\n",
    "wi10 = tf.Variable(np.load('../save/weights/w10.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_layer1 = act_func(tf.matmul(X, wi1))\n",
    "img_layer2 = act_func(tf.matmul(img_layer1, wi2))\n",
    "img_layer3 = act_func(tf.matmul(img_layer2, wi3))\n",
    "img_layer4 = act_func(tf.matmul(img_layer3, wi4))\n",
    "img_layer5 = act_func(tf.matmul(img_layer4, wi5))\n",
    "img_layer6 = act_func(tf.matmul(img_layer5, wi6))\n",
    "img_layer7 = act_func(tf.matmul(img_layer6, wi7))\n",
    "img_layer8 = act_func(tf.matmul(img_layer7, wi8))\n",
    "img_layer9 = act_func(tf.matmul(img_layer8, wi9))\n",
    "img_layer10 = act_func(tf.matmul(img_layer9, wi10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LABEL DECOMPRESSION\n",
    "wl1 = tf.Variable(np.load('../save/weights/lw1.npy'))\n",
    "wl2 = tf.Variable(np.load('../save/weights/lw2.npy'))\n",
    "wl3 = tf.Variable(np.load('../save/weights/lw3.npy'))\n",
    "wl4 = tf.Variable(np.load('../save/weights/lw4.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_layer1 = act_func(tf.matmul(Y, wl1))\n",
    "lbl_layer2 = act_func(tf.matmul(lbl_layer1, wl2))\n",
    "lbl_layer3 = act_func(tf.matmul(lbl_layer2, wl3))\n",
    "lbl_layer4 = act_func(tf.matmul(lbl_layer3, wl4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Parameters for activation learning\n",
    "initializer = tf.random_uniform_initializer()\n",
    "\n",
    "# input_no = image_hid8\n",
    "# output_no = label_hid3\n",
    "\n",
    "inpt = img_layer7\n",
    "yval = lbl_layer2\n",
    "\n",
    "i=6\n",
    "j=2\n",
    "\n",
    "W1 = tf.Variable(np.load('../save/weights/slearn_parms0_{}-{}.npy'.format(i, j)))\n",
    "# W1 = tf.Variable(tf.matmul(wi8,pW1))\n",
    "W2 = tf.Variable(np.load('../save/weights/slearn_parms1_{}-{}.npy'.format(i, j)))\n",
    "pW3 = tf.Variable(np.load('../save/weights/slearn_parms2_{}-{}.npy'.format(i, j)))\n",
    "W3 = tf.Variable(tf.matmul(pW3, tf.transpose(wl3)))\n",
    "\n",
    "b1 = tf.Variable(np.load('../save/weights/slearn_parms3_{}-{}.npy'.format(i, j)))\n",
    "b2 = tf.Variable(np.load('../save/weights/slearn_parms4_{}-{}.npy'.format(i, j)))\n",
    "pb3 = tf.Variable(np.load('../save/weights/slearn_parms5_{}-{}.npy'.format(i, j)).reshape(-1,1).T)\n",
    "b3 = tf.Variable(tf.matmul(pb3,tf.transpose(wl3)))\n",
    "\n",
    "a1 = tf.nn.relu(tf.matmul(inpt, W1) + b1)\n",
    "a2 = tf.nn.relu(tf.matmul(a1, W2) + b2)\n",
    "a3 = tf.matmul(a2, W3) + b3\n",
    "\n",
    "output = a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_77:0' shape=(6, 155) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_84:0' shape=(1, 3) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print(W1)\n",
    "print(b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Encode-Decode Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_layer1 = tf.matmul(output,tf.transpose(wl4))\n",
    "# out_layer2 = tf.matmul(output,tf.transpose(wl3))\n",
    "out_layer3 = tf.matmul(output,tf.transpose(wl2))\n",
    "out_layer4 = tf.matmul(out_layer3,tf.transpose(wl1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTIONS\n",
    "loss = tf.reduce_mean(tf.square(output - yval))\n",
    "# loss = tf.reduce_mean(tf.abs(output_layer - X))\n",
    "# cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y,logits=yout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss, var_list=[W1, W2, W3, b1, b2, b3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARATION\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Complete. Training Loss: 2.836690664291382\n",
      "TEST ACCURACY: \n",
      "0.6968\n",
      "TRAIN ACCURACY: \n",
      "0.6963818\n",
      "\n",
      "Epoch 1 Complete. Training Loss: 3.2186217308044434\n",
      "TEST ACCURACY: \n",
      "0.712\n",
      "\n",
      "Epoch 2 Complete. Training Loss: 2.7517735958099365\n",
      "TEST ACCURACY: \n",
      "0.7097\n",
      "\n",
      "Epoch 3 Complete. Training Loss: 3.5914199352264404\n",
      "TEST ACCURACY: \n",
      "0.7254\n",
      "\n",
      "Epoch 4 Complete. Training Loss: 3.3564698696136475\n",
      "TEST ACCURACY: \n",
      "0.7356\n",
      "\n",
      "Epoch 5 Complete. Training Loss: 2.8551828861236572\n",
      "TEST ACCURACY: \n",
      "0.7365\n",
      "TRAIN ACCURACY: \n",
      "0.7332\n",
      "\n",
      "Epoch 6 Complete. Training Loss: 3.182136058807373\n",
      "TEST ACCURACY: \n",
      "0.7425\n",
      "\n",
      "Epoch 7 Complete. Training Loss: 3.6404759883880615\n",
      "TEST ACCURACY: \n",
      "0.7451\n",
      "\n",
      "Epoch 8 Complete. Training Loss: 4.590766429901123\n",
      "TEST ACCURACY: \n",
      "0.7437\n",
      "\n",
      "Epoch 9 Complete. Training Loss: 3.0159895420074463\n",
      "TEST ACCURACY: \n",
      "0.7477\n",
      "\n",
      "Epoch 10 Complete. Training Loss: 2.856785535812378\n",
      "TEST ACCURACY: \n",
      "0.7484\n",
      "TRAIN ACCURACY: \n",
      "0.74694544\n",
      "\n",
      "Epoch 11 Complete. Training Loss: 2.5638813972473145\n",
      "TEST ACCURACY: \n",
      "0.7496\n",
      "\n",
      "Epoch 12 Complete. Training Loss: 3.5517473220825195\n",
      "TEST ACCURACY: \n",
      "0.7436\n",
      "\n",
      "Epoch 13 Complete. Training Loss: 3.3470041751861572\n",
      "TEST ACCURACY: \n",
      "0.7454\n",
      "\n",
      "Epoch 14 Complete. Training Loss: 3.2997872829437256\n",
      "TEST ACCURACY: \n",
      "0.7503\n",
      "\n",
      "Epoch 15 Complete. Training Loss: 3.392531156539917\n",
      "TEST ACCURACY: \n",
      "0.745\n",
      "TRAIN ACCURACY: \n",
      "0.7447636\n",
      "\n",
      "Epoch 16 Complete. Training Loss: 3.5664544105529785\n",
      "TEST ACCURACY: \n",
      "0.7464\n",
      "\n",
      "Epoch 17 Complete. Training Loss: 2.1012048721313477\n",
      "TEST ACCURACY: \n",
      "0.7487\n",
      "\n",
      "Epoch 18 Complete. Training Loss: 2.94458270072937\n",
      "TEST ACCURACY: \n",
      "0.7472\n",
      "\n",
      "Epoch 19 Complete. Training Loss: 3.2064476013183594\n",
      "TEST ACCURACY: \n",
      "0.7494\n",
      "\n",
      "Epoch 20 Complete. Training Loss: 4.0841779708862305\n",
      "TEST ACCURACY: \n",
      "0.7509\n",
      "TRAIN ACCURACY: \n",
      "0.75052726\n",
      "\n",
      "Epoch 21 Complete. Training Loss: 2.9976751804351807\n",
      "TEST ACCURACY: \n",
      "0.7533\n",
      "\n",
      "Epoch 22 Complete. Training Loss: 2.818493127822876\n",
      "TEST ACCURACY: \n",
      "0.7459\n",
      "\n",
      "Epoch 23 Complete. Training Loss: 3.473963499069214\n",
      "TEST ACCURACY: \n",
      "0.7548\n",
      "\n",
      "Epoch 24 Complete. Training Loss: 2.972269058227539\n",
      "TEST ACCURACY: \n",
      "0.7507\n",
      "\n",
      "Epoch 25 Complete. Training Loss: 2.4197661876678467\n",
      "TEST ACCURACY: \n",
      "0.7539\n",
      "TRAIN ACCURACY: \n",
      "0.75334543\n",
      "\n",
      "Epoch 26 Complete. Training Loss: 3.344228744506836\n",
      "TEST ACCURACY: \n",
      "0.7585\n",
      "\n",
      "Epoch 27 Complete. Training Loss: 3.3808176517486572\n",
      "TEST ACCURACY: \n",
      "0.7556\n",
      "\n",
      "Epoch 28 Complete. Training Loss: 3.477654218673706\n",
      "TEST ACCURACY: \n",
      "0.7548\n",
      "\n",
      "Epoch 29 Complete. Training Loss: 4.1259355545043945\n",
      "TEST ACCURACY: \n",
      "0.7588\n",
      "\n",
      "Epoch 30 Complete. Training Loss: 3.0183212757110596\n",
      "TEST ACCURACY: \n",
      "0.7539\n",
      "TRAIN ACCURACY: \n",
      "0.75514543\n",
      "\n",
      "Epoch 31 Complete. Training Loss: 3.0574698448181152\n",
      "TEST ACCURACY: \n",
      "0.7561\n",
      "\n",
      "Epoch 32 Complete. Training Loss: 3.4297804832458496\n",
      "TEST ACCURACY: \n",
      "0.7554\n",
      "\n",
      "Epoch 33 Complete. Training Loss: 3.2000765800476074\n",
      "TEST ACCURACY: \n",
      "0.7593\n",
      "\n",
      "Epoch 34 Complete. Training Loss: 3.164055585861206\n",
      "TEST ACCURACY: \n",
      "0.7545\n",
      "\n",
      "Epoch 35 Complete. Training Loss: 3.0704901218414307\n",
      "TEST ACCURACY: \n",
      "0.7589\n",
      "TRAIN ACCURACY: \n",
      "0.7571273\n",
      "\n",
      "Epoch 36 Complete. Training Loss: 2.670436143875122\n",
      "TEST ACCURACY: \n",
      "0.758\n",
      "\n",
      "Epoch 37 Complete. Training Loss: 3.3215696811676025\n",
      "TEST ACCURACY: \n",
      "0.7548\n",
      "\n",
      "Epoch 38 Complete. Training Loss: 3.5768015384674072\n",
      "TEST ACCURACY: \n",
      "0.7585\n",
      "\n",
      "Epoch 39 Complete. Training Loss: 2.8963117599487305\n",
      "TEST ACCURACY: \n",
      "0.7614\n",
      "\n",
      "Epoch 40 Complete. Training Loss: 2.6789839267730713\n",
      "TEST ACCURACY: \n",
      "0.7602\n",
      "TRAIN ACCURACY: \n",
      "0.7594909\n",
      "\n",
      "Epoch 41 Complete. Training Loss: 2.7684829235076904\n",
      "TEST ACCURACY: \n",
      "0.7599\n",
      "\n",
      "Epoch 42 Complete. Training Loss: 2.8525898456573486\n",
      "TEST ACCURACY: \n",
      "0.7592\n",
      "\n",
      "Epoch 43 Complete. Training Loss: 3.45103120803833\n",
      "TEST ACCURACY: \n",
      "0.7602\n",
      "\n",
      "Epoch 44 Complete. Training Loss: 3.2916758060455322\n",
      "TEST ACCURACY: \n",
      "0.7601\n",
      "\n",
      "Epoch 45 Complete. Training Loss: 3.355276107788086\n",
      "TEST ACCURACY: \n",
      "0.7614\n",
      "TRAIN ACCURACY: \n",
      "0.75929093\n",
      "\n",
      "Epoch 46 Complete. Training Loss: 2.8353447914123535\n",
      "TEST ACCURACY: \n",
      "0.7586\n",
      "\n",
      "Epoch 47 Complete. Training Loss: 3.807565212249756\n",
      "TEST ACCURACY: \n",
      "0.7597\n",
      "\n",
      "Epoch 48 Complete. Training Loss: 3.3995156288146973\n",
      "TEST ACCURACY: \n",
      "0.7588\n",
      "\n",
      "Epoch 49 Complete. Training Loss: 3.4011337757110596\n",
      "TEST ACCURACY: \n",
      "0.7641\n",
      "\n",
      "Epoch 50 Complete. Training Loss: 3.349881887435913\n",
      "TEST ACCURACY: \n",
      "0.7629\n",
      "TRAIN ACCURACY: \n",
      "0.7607818\n",
      "\n",
      "Epoch 51 Complete. Training Loss: 2.7234151363372803\n",
      "TEST ACCURACY: \n",
      "0.7639\n",
      "\n",
      "Epoch 52 Complete. Training Loss: 3.097402334213257\n",
      "TEST ACCURACY: \n",
      "0.7615\n",
      "\n",
      "Epoch 53 Complete. Training Loss: 3.370317220687866\n",
      "TEST ACCURACY: \n",
      "0.76\n",
      "\n",
      "Epoch 54 Complete. Training Loss: 2.7453675270080566\n",
      "TEST ACCURACY: \n",
      "0.7649\n",
      "\n",
      "Epoch 55 Complete. Training Loss: 4.081544399261475\n",
      "TEST ACCURACY: \n",
      "0.7656\n",
      "TRAIN ACCURACY: \n",
      "0.7630182\n",
      "\n",
      "Epoch 56 Complete. Training Loss: 3.139211654663086\n",
      "TEST ACCURACY: \n",
      "0.7641\n",
      "\n",
      "Epoch 57 Complete. Training Loss: 3.7961792945861816\n",
      "TEST ACCURACY: \n",
      "0.7653\n",
      "\n",
      "Epoch 58 Complete. Training Loss: 3.268160820007324\n",
      "TEST ACCURACY: \n",
      "0.7614\n",
      "\n",
      "Epoch 59 Complete. Training Loss: 3.550328016281128\n",
      "TEST ACCURACY: \n",
      "0.7666\n",
      "\n",
      "Epoch 60 Complete. Training Loss: 3.1081974506378174\n",
      "TEST ACCURACY: \n",
      "0.7654\n",
      "TRAIN ACCURACY: \n",
      "0.76321816\n",
      "\n",
      "Epoch 61 Complete. Training Loss: 3.510134220123291\n",
      "TEST ACCURACY: \n",
      "0.7663\n",
      "\n",
      "Epoch 62 Complete. Training Loss: 3.474562168121338\n",
      "TEST ACCURACY: \n",
      "0.7639\n",
      "\n",
      "Epoch 63 Complete. Training Loss: 2.788421392440796\n",
      "TEST ACCURACY: \n",
      "0.767\n",
      "\n",
      "Epoch 64 Complete. Training Loss: 3.479259729385376\n",
      "TEST ACCURACY: \n",
      "0.7675\n",
      "\n",
      "Epoch 65 Complete. Training Loss: 3.1504251956939697\n",
      "TEST ACCURACY: \n",
      "0.7662\n",
      "TRAIN ACCURACY: \n",
      "0.7600727\n",
      "\n",
      "Epoch 66 Complete. Training Loss: 2.800138473510742\n",
      "TEST ACCURACY: \n",
      "0.764\n",
      "\n",
      "Epoch 67 Complete. Training Loss: 3.5020856857299805\n",
      "TEST ACCURACY: \n",
      "0.7673\n",
      "\n",
      "Epoch 68 Complete. Training Loss: 2.8135993480682373\n",
      "TEST ACCURACY: \n",
      "0.767\n",
      "\n",
      "Epoch 69 Complete. Training Loss: 2.8230831623077393\n",
      "TEST ACCURACY: \n",
      "0.7662\n",
      "\n",
      "Epoch 70 Complete. Training Loss: 2.91851544380188\n",
      "TEST ACCURACY: \n",
      "0.7653\n",
      "TRAIN ACCURACY: \n",
      "0.7612727\n",
      "\n",
      "Epoch 71 Complete. Training Loss: 2.7494819164276123\n",
      "TEST ACCURACY: \n",
      "0.7693\n",
      "\n",
      "Epoch 72 Complete. Training Loss: 2.9030768871307373\n",
      "TEST ACCURACY: \n",
      "0.7689\n",
      "\n",
      "Epoch 73 Complete. Training Loss: 2.905238628387451\n",
      "TEST ACCURACY: \n",
      "0.7709\n",
      "\n",
      "Epoch 74 Complete. Training Loss: 3.1888036727905273\n",
      "TEST ACCURACY: \n",
      "0.769\n",
      "\n",
      "Epoch 75 Complete. Training Loss: 3.32350754737854\n",
      "TEST ACCURACY: \n",
      "0.7678\n",
      "TRAIN ACCURACY: \n",
      "0.76254547\n",
      "\n",
      "Epoch 76 Complete. Training Loss: 2.824348211288452\n",
      "TEST ACCURACY: \n",
      "0.7662\n",
      "\n",
      "Keyboard Interrupted\n",
      "Finished Training the Model\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "try:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        # Epoch == Entire Training Set\n",
    "        for epoch in range(num_epochs):\n",
    "            num_batches = mnist.train.num_examples // batch_size\n",
    "\n",
    "            # 150 batch size\n",
    "            for iteration in range(num_batches):\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_size+3*epoch)\n",
    "                sess.run(train, feed_dict={X: X_batch, Y: y_batch})\n",
    "\n",
    "            training_loss = loss.eval(feed_dict={X: X_batch, Y: y_batch})   \n",
    "            print(\"Epoch {} Complete. Training Loss: {}\".format(epoch,training_loss))\n",
    "            saver.save(sess, \"../save/slearn_6-3.ckpt\")\n",
    "            \n",
    "            matches = tf.equal(tf.argmax(out_layer4, 1), tf.argmax(Y,1))\n",
    "            acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "\n",
    "            print('TEST ACCURACY: ')\n",
    "            print (sess.run(acc, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))\n",
    "            if epoch % 5 == 0:\n",
    "                print('TRAIN ACCURACY: ')\n",
    "                print (sess.run(acc, feed_dict={X:mnist.train.images, Y:mnist.train.labels}))\n",
    "            print()\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print('Keyboard Interrupted')\n",
    "finally:\n",
    "    print('Finished Training the Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../save/slearn_6-2.ckpt\n",
      "TRAIN ACCURACY: \n",
      "0.76345456\n",
      "TEST ACCURACY: \n",
      "0.7662\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_test_labels = 10\n",
    "start_point = 15\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "#     sess.run(init)\n",
    "    saver.restore(sess,\"../save/slearn_6-3.ckpt\")\n",
    "    \n",
    "    params = sess.run([W1, W2, W3, b1, b2, b3])\n",
    "    \n",
    "    matches = tf.equal(tf.argmax(out_layer4, 1), tf.argmax(Y,1))\n",
    "    acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "    \n",
    "    print('TRAIN ACCURACY: ')\n",
    "    print (sess.run(acc, feed_dict={X:mnist.train.images, Y:mnist.train.labels}))\n",
    "\n",
    "    print('TEST ACCURACY: ')\n",
    "    print (sess.run(acc, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 6.27279431e-02,  6.97199553e-02,  3.36577222e-02,\n",
       "          4.68604676e-02,  5.89018799e-02,  3.01080912e-01,\n",
       "          1.84429586e-01,  6.93862885e-03,  4.93403524e-04,\n",
       "          1.04396887e-01,  1.01631790e-01,  4.69973534e-02,\n",
       "          1.36196688e-02,  6.91744015e-02,  2.37899385e-02,\n",
       "         -8.44902471e-02,  2.81018186e-02,  1.51729584e-02,\n",
       "          2.37421647e-01,  1.83578078e-02,  5.27659431e-02,\n",
       "          4.13591042e-02,  5.25669716e-02,  7.68910721e-02,\n",
       "          4.76556011e-02,  4.19994816e-02, -5.78982174e-01,\n",
       "         -7.78746784e-01,  8.76460597e-02,  5.73289115e-03,\n",
       "          2.13504806e-01,  7.41538629e-02,  3.54237109e-02,\n",
       "          4.46166769e-02,  3.15570869e-02,  3.59759084e-03,\n",
       "          2.35630628e-02,  3.30958784e-01,  7.37297237e-02,\n",
       "          1.79421872e-01,  1.38268946e-02,  4.51459177e-02,\n",
       "          2.33768675e-04,  2.25564465e-02,  2.37490516e-02,\n",
       "          7.18264803e-02,  1.04620177e-02,  2.48926431e-02,\n",
       "          4.41824980e-02,  6.30536899e-02,  3.85236405e-02,\n",
       "          6.84295148e-02,  4.63074535e-01, -1.31627887e-01,\n",
       "          3.04017421e-02,  1.56584978e-02,  3.75987142e-02,\n",
       "          4.14332449e-02,  6.70411065e-02, -7.84698784e-01,\n",
       "          5.10849282e-02, -9.15918291e-01,  4.40853350e-02,\n",
       "          5.21532409e-02,  5.24915829e-02,  6.15046173e-02,\n",
       "          7.45566115e-02,  2.18607157e-01,  6.93668500e-02,\n",
       "          6.81728721e-02,  1.78238243e-01,  7.64758065e-02,\n",
       "          5.28183505e-02,  4.13193889e-02,  2.00935565e-02,\n",
       "          4.02266486e-03,  4.17002812e-02, -5.43810964e-01,\n",
       "          5.39184697e-02, -3.65386724e-01,  4.26802039e-02,\n",
       "          2.93477457e-02,  3.04889768e-01,  4.20311093e-02,\n",
       "          5.65715060e-02,  7.18106776e-02,  5.30468449e-02,\n",
       "          3.68554667e-02,  4.15683314e-02,  5.47224544e-02,\n",
       "          4.04368853e-03,  7.07960576e-02,  5.60822710e-02,\n",
       "          2.42605135e-02, -5.14315605e-01,  3.83245237e-02,\n",
       "          3.81909020e-04,  3.22549753e-02,  7.01455921e-02,\n",
       "         -8.55775476e-01,  6.67187721e-02,  2.55024843e-02,\n",
       "          2.38055110e-01,  3.14604081e-02,  3.75244617e-02,\n",
       "         -6.67828321e-02,  4.04541135e-01,  5.77659570e-02,\n",
       "          2.28680149e-02,  1.34557057e-02, -1.48559943e-01,\n",
       "          2.83572674e-02,  3.77425849e-02,  2.99139172e-02,\n",
       "          1.91986859e-01, -5.84407270e-01, -4.04598415e-02,\n",
       "          5.69091797e-01, -1.04783759e-01,  4.12734374e-02,\n",
       "          5.73534258e-02,  4.81274389e-02, -9.78559703e-02,\n",
       "         -7.31405057e-03,  3.87509987e-02,  8.68878141e-03,\n",
       "          2.14283854e-01,  2.41496675e-02,  4.66756150e-02,\n",
       "          1.84660748e-01,  3.41823697e-02,  3.05936076e-02,\n",
       "          9.58733037e-02,  1.83236971e-02,  1.06277987e-02,\n",
       "          2.04411790e-01,  3.73646319e-02,  2.84971073e-02,\n",
       "         -1.15690082e-01,  6.74672127e-02,  4.44693156e-02,\n",
       "          4.43600491e-02,  4.04446423e-02,  1.69686988e-01,\n",
       "          3.34634371e-02,  6.82806820e-02,  4.90549505e-02,\n",
       "          4.85288724e-02, -2.20285699e-01,  6.98383898e-02,\n",
       "          1.89063158e-02,  7.48453569e-03, -1.02170832e-01,\n",
       "          2.19816267e-02,  7.14364499e-02],\n",
       "        [ 9.45488065e-02,  1.05087742e-01,  5.07317334e-02,\n",
       "          3.20546776e-01,  8.87818336e-02, -8.29860419e-02,\n",
       "         -1.74271941e-01,  1.04584815e-02,  7.43698911e-04,\n",
       "         -4.46545631e-02,  3.28007847e-01,  7.08383471e-02,\n",
       "          2.05287039e-02,  1.04265444e-01,  3.58581841e-02,\n",
       "         -6.08343668e-02,  4.23574112e-02,  2.28699520e-02,\n",
       "          3.21066916e-01,  2.76704244e-02,  7.95332417e-02,\n",
       "          6.23399019e-02,  7.92333186e-02,  1.15896650e-01,\n",
       "          7.18305036e-02,  6.33051246e-02,  5.75659990e-01,\n",
       "          2.72450924e-01,  2.57869720e-01,  8.64109211e-03,\n",
       "          2.93958247e-01,  1.11770906e-01,  5.33935800e-02,\n",
       "          6.72499835e-02,  4.75654751e-02,  5.42258937e-03,\n",
       "          3.55162136e-02,  3.40909064e-01,  1.11131594e-01,\n",
       "          7.58308232e-01,  2.08410490e-02,  6.80476874e-02,\n",
       "          3.52355681e-04,  3.39989625e-02,  3.57965529e-02,\n",
       "          1.08262867e-01,  1.57692265e-02,  3.75202708e-02,\n",
       "          6.65955544e-02,  9.50397849e-02,  5.80660515e-02,\n",
       "          1.03142671e-01,  1.83679685e-01,  1.17053218e-01,\n",
       "          4.58240472e-02,  2.36017965e-02,  5.66719286e-02,\n",
       "          6.24516532e-02,  1.01049960e-01,  1.38012946e-01,\n",
       "          7.69994706e-02,  5.08696914e-01,  6.64491057e-02,\n",
       "          7.86097273e-02,  7.91197121e-02,  9.27048922e-02,\n",
       "          1.12377964e-01,  3.61923338e-03,  1.04555503e-01,\n",
       "          1.02755845e-01,  2.93362886e-01,  1.15270719e-01,\n",
       "          7.96122402e-02,  6.22800365e-02,  3.02866921e-02,\n",
       "          6.06329553e-03,  6.28541559e-02,  1.71423554e-01,\n",
       "          3.52397323e-01,  4.51279469e-02,  6.43311739e-02,\n",
       "          4.42353711e-02,  1.93535924e-01,  6.33528009e-02,\n",
       "          8.52693021e-02,  1.08239055e-01,  7.99566358e-02,\n",
       "          5.55516407e-02,  6.26552626e-02,  8.24822634e-02,\n",
       "          6.09498471e-03,  1.06709719e-01,  8.45318809e-02,\n",
       "          3.65674719e-02,  5.20551205e-01,  5.77659309e-02,\n",
       "          5.75645186e-04,  4.86174040e-02,  1.05729304e-01,\n",
       "         -1.70119122e-01,  4.15295541e-01,  3.84394750e-02,\n",
       "          3.78602266e-01, -4.58133250e-01,  5.65599948e-02,\n",
       "          6.10979378e-01,  9.87749994e-01,  8.70696753e-02,\n",
       "          3.44685838e-02,  2.02815663e-02, -2.22810060e-01,\n",
       "          4.27424386e-02,  5.68887740e-02,  4.50887531e-02,\n",
       "          3.51675808e-01,  6.47623539e-01,  6.05751395e-01,\n",
       "          4.74316955e-01, -2.94680029e-01,  6.22107685e-02,\n",
       "          8.64478797e-02,  7.25416988e-02,  9.95168805e-01,\n",
       "          3.15307230e-01,  5.84087484e-02,  1.30964583e-02,\n",
       "          3.40278685e-01,  3.64004001e-02,  7.03533888e-02,\n",
       "          3.35324079e-01,  5.15225194e-02,  4.61132377e-02,\n",
       "          2.51040339e-01,  2.76190080e-02,  1.60191059e-02,\n",
       "          4.77115601e-01,  5.63190952e-02,  4.29532193e-02,\n",
       "         -1.18947282e-01,  1.01692222e-01,  6.70278594e-02,\n",
       "          6.68631718e-02,  6.09615445e-02,  3.02716076e-01,\n",
       "          5.04388846e-02,  1.02918357e-01,  7.39397258e-02,\n",
       "          7.31467605e-02, -3.75148594e-01,  1.05266273e-01,\n",
       "          2.84971762e-02,  1.12813180e-02, -1.72808662e-01,\n",
       "          3.31325457e-02,  1.07674994e-01],\n",
       "        [ 1.13779828e-01,  1.26462370e-01,  6.10504597e-02,\n",
       "         -1.84619889e-01,  1.06839873e-01,  2.29307726e-01,\n",
       "          2.34783426e-01,  1.25857135e-02,  8.94965779e-04,\n",
       "         -1.26468658e-01,  3.03254157e-01,  8.52467120e-02,\n",
       "          2.47041993e-02,  1.25472799e-01,  4.31516618e-02,\n",
       "         -1.95921451e-01,  5.09728156e-02,  2.75216494e-02,\n",
       "          2.68800586e-01,  3.32985260e-02,  9.57101285e-02,\n",
       "          7.50197023e-02,  9.53492224e-02,  1.39469787e-01,\n",
       "          8.64406750e-02,  7.61812478e-02, -4.05300647e-01,\n",
       "         -7.50042349e-02,  3.26296806e-01,  1.03986738e-02,\n",
       "         -1.48479998e-01,  1.34504870e-01,  6.42537251e-02,\n",
       "          8.09284821e-02,  5.72401956e-02,  6.52553281e-03,\n",
       "          4.27401438e-02,  2.38238409e-01,  1.33735538e-01,\n",
       "         -2.82390267e-01,  2.50800774e-02,  8.18884596e-02,\n",
       "          4.24024067e-04,  4.09142748e-02,  4.30774987e-02,\n",
       "          1.30283296e-01,  1.89766549e-02,  4.51518223e-02,\n",
       "          8.01409483e-02,  1.14370674e-01,  6.98765665e-02,\n",
       "          1.24121688e-01,  3.42865102e-03, -2.60258973e-01,\n",
       "          5.51445596e-02,  2.84023546e-02,  6.81988746e-02,\n",
       "          7.51541853e-02,  1.21603303e-01, -2.10312739e-01,\n",
       "          9.26610082e-02, -3.03327441e-01,  7.99647123e-02,\n",
       "          9.45987701e-02,  9.52124745e-02,  1.11560874e-01,\n",
       "          1.35235399e-01, -5.16415119e-01,  1.25821888e-01,\n",
       "          1.23656169e-01,  2.77887553e-01,  1.38716549e-01,\n",
       "          9.58051980e-02,  7.49476627e-02,  3.64469364e-02,\n",
       "          7.29655754e-03,  7.56385475e-02,  2.49242276e-01,\n",
       "          4.43902612e-02,  3.53312492e-02,  7.74159953e-02,\n",
       "          5.32327592e-02,  1.35882035e-01,  7.62386173e-02,\n",
       "          1.02612890e-01,  1.30254656e-01,  9.62196440e-02,\n",
       "          6.68507218e-02,  7.53992125e-02,  9.92589593e-02,\n",
       "          7.33469194e-03,  1.28414273e-01,  1.01725481e-01,\n",
       "          4.40052226e-02,  4.53763962e-01,  6.95153996e-02,\n",
       "          6.92730187e-04,  5.85060716e-02,  1.27234414e-01,\n",
       "         -2.50471592e-01,  1.81933790e-01,  4.62579839e-02,\n",
       "          1.37445971e-01, -1.58590242e-01,  6.80641904e-02,\n",
       "          2.32711568e-01,  1.54317141e-01,  1.04779474e-01,\n",
       "          4.14794162e-02,  2.44067907e-02, -3.34848702e-01,\n",
       "          5.14361709e-02,  6.84598386e-02,  5.42597175e-02,\n",
       "          2.84202069e-01,  7.84023330e-02,  8.98691654e-01,\n",
       "         -7.18461692e-01, -3.43167782e-01,  7.48643130e-02,\n",
       "          1.04031190e-01,  8.72965232e-02, -2.81682521e-01,\n",
       "          5.22364199e-01,  7.02889562e-02,  1.57602485e-02,\n",
       "          1.72155350e-01,  4.38041613e-02,  8.46631154e-02,\n",
       "          3.06307495e-01,  6.20021001e-02,  5.54925762e-02,\n",
       "          3.07769030e-01,  3.32366526e-02,  1.92773584e-02,\n",
       "          1.65495187e-01,  6.77742809e-02,  5.16898148e-02,\n",
       "          6.47394061e-01,  1.22376204e-01,  8.06611925e-02,\n",
       "          8.04629996e-02,  7.33609945e-02,  2.04130918e-01,\n",
       "          6.06980510e-02,  1.23851731e-01,  8.89789015e-02,\n",
       "          8.80246684e-02, -3.85132849e-01,  1.26677200e-01,\n",
       "          3.42934430e-02,  1.35759134e-02,  7.67696202e-02,\n",
       "          3.98716331e-02,  1.29575863e-01],\n",
       "        [-3.08001548e-01, -3.42333108e-01, -1.65263355e-01,\n",
       "         -2.67694145e-01, -2.89215118e-01,  4.32868004e-01,\n",
       "         -3.05656284e-01, -3.40694785e-02, -2.42266897e-03,\n",
       "         -1.83066338e-01, -5.26138186e-01, -2.30762497e-01,\n",
       "         -6.68741688e-02, -3.39654386e-01, -1.16811380e-01,\n",
       "         -2.74924427e-01, -1.37983218e-01, -7.45010003e-02,\n",
       "         -6.57822907e-01, -9.01389867e-02, -2.59086937e-01,\n",
       "         -2.03078046e-01, -2.58109957e-01, -3.77544165e-01,\n",
       "         -2.33994588e-01, -2.06222355e-01,  1.68653965e-01,\n",
       "         -5.04061162e-01, -4.17186141e-01, -2.81491671e-02,\n",
       "         -9.27592397e-01, -3.64104182e-01, -1.73934579e-01,\n",
       "         -2.19073102e-01, -1.54948980e-01, -1.76645909e-02,\n",
       "         -1.15697399e-01, -7.22927570e-01, -3.62021536e-01,\n",
       "         -2.76482493e-01, -6.78916797e-02, -2.21671730e-01,\n",
       "         -1.14783156e-03, -1.10754788e-01, -1.16610624e-01,\n",
       "         -3.52676392e-01, -5.13697267e-02, -1.22225799e-01,\n",
       "         -2.16941237e-01, -3.09600979e-01, -1.89155594e-01,\n",
       "         -3.35996926e-01,  9.81771424e-02,  3.38518173e-01,\n",
       "         -1.49276108e-01, -7.68850595e-02, -1.84614107e-01,\n",
       "         -2.03442082e-01, -3.29179674e-01, -6.05095208e-01,\n",
       "         -2.50832975e-01, -2.83082724e-01, -2.16464147e-01,\n",
       "         -2.56078511e-01, -2.57739812e-01, -3.01994860e-01,\n",
       "         -3.66081715e-01,  2.57290184e-01, -3.40599328e-01,\n",
       "         -3.34736735e-01, -6.01781785e-01, -3.75505179e-01,\n",
       "         -2.59344310e-01, -2.02883020e-01, -9.86617208e-02,\n",
       "         -1.97517537e-02, -2.04753250e-01,  4.48146090e-02,\n",
       "         -3.73709261e-01, -2.05684394e-01, -2.09564805e-01,\n",
       "         -1.44100860e-01, -2.77991056e-01, -2.06377655e-01,\n",
       "         -2.77772695e-01, -3.52598846e-01, -2.60466188e-01,\n",
       "         -1.80964649e-01, -2.04105362e-01, -2.68693656e-01,\n",
       "         -1.98549815e-02, -3.47616911e-01, -2.75370479e-01,\n",
       "         -1.19121961e-01, -1.08384991e+00, -1.88177913e-01,\n",
       "         -1.87521789e-03, -1.58375710e-01, -3.44423056e-01,\n",
       "         -3.89037430e-02, -5.11101663e-01, -1.25220165e-01,\n",
       "         -5.11648417e-01, -6.22225523e-01, -1.84249505e-01,\n",
       "          4.27205563e-01,  9.61085409e-02, -2.83637583e-01,\n",
       "         -1.12284623e-01, -6.60690963e-02, -3.75112444e-01,\n",
       "         -1.39237493e-01, -1.85320511e-01, -1.46880835e-01,\n",
       "         -7.79061317e-01, -4.58168209e-01,  5.64005859e-02,\n",
       "         -1.09249258e+00, -4.89469707e-01, -2.02657402e-01,\n",
       "         -2.81612009e-01, -2.36311331e-01, -5.50115228e-01,\n",
       "         -7.09406137e-01, -1.90271929e-01, -4.26629335e-02,\n",
       "         -6.21752501e-01, -1.18577704e-01, -2.29182735e-01,\n",
       "         -6.77171946e-01, -1.67839438e-01, -1.50218189e-01,\n",
       "         -5.02541006e-01, -8.99714902e-02, -5.21837324e-02,\n",
       "         -5.90184808e-01, -1.83464721e-01, -1.39924124e-01,\n",
       "         -1.50100812e-01, -3.31271946e-01, -2.18349531e-01,\n",
       "         -2.17813030e-01, -1.98587939e-01, -4.69862729e-01,\n",
       "         -1.64309397e-01, -3.35266173e-01, -2.40865543e-01,\n",
       "         -2.38282442e-01, -4.34195518e-01, -3.42914671e-01,\n",
       "         -9.28322226e-02, -3.67499515e-02, -4.85191733e-01,\n",
       "         -1.07932359e-01, -3.50761324e-01],\n",
       "        [-1.10541619e-01, -1.22863196e-01, -5.93129471e-02,\n",
       "         -8.59607309e-02, -1.03799179e-01,  1.04650827e-02,\n",
       "         -3.72507751e-01, -1.22275222e-02, -8.69494805e-04,\n",
       "         -2.15556085e-01, -1.31836250e-01, -8.28205645e-02,\n",
       "         -2.40011103e-02, -1.21901818e-01, -4.19235565e-02,\n",
       "         -1.86468929e-01, -4.95221168e-02, -2.67383792e-02,\n",
       "         -2.24724337e-01, -3.23508456e-02, -9.29861963e-02,\n",
       "         -7.28846267e-02, -9.26355645e-02, -1.35500431e-01,\n",
       "         -8.39805529e-02, -7.40131140e-02, -2.63285518e-01,\n",
       "          1.49201676e-01, -1.55986786e-01, -1.01027247e-02,\n",
       "         -1.43696457e-01, -1.30676821e-01, -6.24250472e-02,\n",
       "         -7.86252394e-02, -5.56111224e-02, -6.33981451e-03,\n",
       "         -4.15237434e-02, -2.44130701e-01, -1.29929394e-01,\n",
       "         -1.94506312e-03, -2.43662931e-02, -7.95578882e-02,\n",
       "         -4.11956280e-04, -3.97498459e-02, -4.18515019e-02,\n",
       "         -1.26575410e-01, -1.84365716e-02, -4.38667871e-02,\n",
       "         -7.78601170e-02, -1.11115657e-01, -6.78878576e-02,\n",
       "         -1.20589152e-01,  3.39521244e-02,  1.94727898e-01,\n",
       "         -5.35751320e-02, -2.75940169e-02, -6.62579238e-02,\n",
       "         -7.30152801e-02, -1.18142448e-01, -3.01923186e-01,\n",
       "         -9.00238454e-02, -3.28508377e-01, -7.76888952e-02,\n",
       "         -9.19064656e-02, -9.25027132e-02, -1.08385824e-01,\n",
       "         -1.31386563e-01, -9.27743077e-01, -1.22240953e-01,\n",
       "         -1.20136879e-01, -1.98459744e-01, -1.34768635e-01,\n",
       "         -9.30785537e-02, -7.28146359e-02, -3.54096517e-02,\n",
       "         -7.08889589e-03, -7.34858513e-02,  7.74219707e-02,\n",
       "         -1.38761953e-01,  1.04107961e-01, -7.52127245e-02,\n",
       "         -5.17177396e-02,  5.14040589e-01, -7.40688592e-02,\n",
       "         -9.96925086e-02, -1.26547590e-01, -9.34812129e-02,\n",
       "         -6.49481416e-02, -7.32533336e-02, -9.64340419e-02,\n",
       "         -7.12594483e-03, -1.24759555e-01, -9.88303572e-02,\n",
       "         -4.27528210e-02, -6.64289445e-02, -6.75369799e-02,\n",
       "         -6.73014903e-04, -5.68409786e-02, -1.23613305e-01,\n",
       "         -5.48947044e-03, -1.81537166e-01, -4.49414663e-02,\n",
       "         -2.72682279e-01, -5.80170788e-02, -6.61270618e-02,\n",
       "         -7.88084120e-02,  8.09954584e-01, -1.01797417e-01,\n",
       "         -4.02989052e-02, -2.37121712e-02, -4.25240517e-01,\n",
       "         -4.99722809e-02, -6.65114447e-02, -5.27154729e-02,\n",
       "         -2.89344877e-01, -1.19694963e-01, -6.52702570e-01,\n",
       "          2.59028494e-01,  2.59817928e-01, -7.27336556e-02,\n",
       "         -1.01070441e-01, -8.48120376e-02, -3.90136428e-02,\n",
       "         -9.26441014e-01, -6.82885200e-02, -1.53117087e-02,\n",
       "         -2.00477809e-01, -4.25574854e-02, -8.22535828e-02,\n",
       "         -2.17757195e-01, -6.02375083e-02, -5.39132431e-02,\n",
       "         -1.10996567e-01, -3.22907344e-02, -1.87287200e-02,\n",
       "         -2.34628141e-01, -6.58454075e-02, -5.02187088e-02,\n",
       "         -2.92489022e-01, -1.18893355e-01, -7.83655643e-02,\n",
       "         -7.81730041e-02, -7.12731257e-02, -1.39576018e-01,\n",
       "         -5.89705743e-02, -1.20326884e-01, -8.64465460e-02,\n",
       "         -8.55194703e-02, -2.03349635e-01, -1.23071939e-01,\n",
       "         -3.33174430e-02, -1.31895402e-02, -5.22556663e-01,\n",
       "         -3.87368761e-02, -1.25888094e-01],\n",
       "        [ 7.14536458e-02,  7.94182718e-02,  3.83396409e-02,\n",
       "         -6.00406706e-01,  6.70953616e-02,  4.88666564e-01,\n",
       "         -2.66238153e-01,  7.90381804e-03,  5.62037807e-04,\n",
       "         -1.78642243e-01,  2.61731774e-01,  5.35348654e-02,\n",
       "          1.55142173e-02,  7.87968338e-02,  2.70992070e-02,\n",
       "         -9.67591032e-02,  3.20108905e-02,  1.72835756e-02,\n",
       "          2.59304017e-01,  2.09114477e-02,  6.01058900e-02,\n",
       "          4.71123159e-02,  5.98792359e-02,  8.75869244e-02,\n",
       "          5.42846769e-02,  4.78417724e-02, -1.07229307e-01,\n",
       "          7.18393475e-02,  1.80820733e-01,  6.53035752e-03,\n",
       "         -2.18905002e-01,  8.44689533e-02,  4.03512865e-02,\n",
       "          5.08230291e-02,  3.59467976e-02,  4.09802934e-03,\n",
       "          2.68407743e-02,  2.72754818e-01,  8.39858279e-02,\n",
       "         -1.14393950e-01,  1.57502647e-02,  5.14258817e-02,\n",
       "          2.66286806e-04,  2.56941337e-02,  2.70526335e-02,\n",
       "          8.18178207e-02,  1.19173247e-02,  2.83553042e-02,\n",
       "          5.03284521e-02,  7.18246996e-02,  4.38824296e-02,\n",
       "          7.79483169e-02, -2.56899521e-02,  3.34440738e-01,\n",
       "          3.46307456e-02,  1.78366527e-02,  4.28288393e-02,\n",
       "          4.71967645e-02,  7.63667822e-02,  3.32079709e-01,\n",
       "          5.81910312e-02,  3.37226033e-01,  5.02177700e-02,\n",
       "          5.94079532e-02,  5.97933680e-02,  7.00601339e-02,\n",
       "          8.49277154e-02,  2.20243946e-01,  7.90160447e-02,\n",
       "          7.76559785e-02,  2.58220196e-01,  8.71138871e-02,\n",
       "          6.01655878e-02,  4.70670760e-02,  2.28886493e-02,\n",
       "          4.58223280e-03,  4.75009456e-02,  1.79796829e-03,\n",
       "          2.47964263e-01,  8.84802267e-02,  4.86171842e-02,\n",
       "          3.34301293e-02, -2.21962899e-01,  4.78778109e-02,\n",
       "          6.44408166e-02,  8.17998052e-02,  6.04258701e-02,\n",
       "          4.19822074e-02,  4.73506451e-02,  6.23345524e-02,\n",
       "          4.60618129e-03,  8.06440637e-02,  6.38835281e-02,\n",
       "          2.76352465e-02, -1.04986876e-01,  4.36556116e-02,\n",
       "          4.35034075e-04,  3.67417671e-02,  7.99031109e-02,\n",
       "          7.74650455e-01,  1.96862847e-01,  2.90499814e-02,\n",
       "          1.86915994e-01,  2.65516192e-01,  4.27442603e-02,\n",
       "          5.28543815e-02, -1.10717990e-01,  6.58014268e-02,\n",
       "          2.60490440e-02,  1.53274462e-02,  1.64877728e-01,\n",
       "          3.23018655e-02,  4.29927185e-02,  3.40750553e-02,\n",
       "          3.06367517e-01, -1.39680311e-01, -2.50022680e-01,\n",
       "          7.94992819e-02,  3.44837636e-01,  4.70147282e-02,\n",
       "          6.53315037e-02,  5.48221543e-02, -3.19426894e-01,\n",
       "          2.66885728e-01,  4.41414118e-02,  9.89742577e-03,\n",
       "          2.63389587e-01,  2.75089759e-02,  5.31683639e-02,\n",
       "          2.07852185e-01,  3.89372669e-02,  3.48492898e-02,\n",
       "          2.49003351e-01,  2.08725948e-02,  1.21061662e-02,\n",
       "          1.89863920e-01,  4.25621942e-02,  3.24611664e-02,\n",
       "          2.19803393e-01,  7.68521652e-02,  5.06551638e-02,\n",
       "          5.05307019e-02,  4.60706428e-02,  2.02302769e-01,\n",
       "          3.81183252e-02,  7.77787790e-02,  5.58786802e-02,\n",
       "          5.52794188e-02,  6.20954514e-01,  7.95531794e-02,\n",
       "          2.15362571e-02,  8.52566306e-03, -2.43004993e-01,\n",
       "          2.50393562e-02,  8.13735276e-02]], dtype=float32),\n",
       " array([[ 0.12928236,  0.00709345,  0.2270143 , ..., -0.11943059,\n",
       "          0.70338863,  0.65438473],\n",
       "        [ 0.5483187 ,  0.02092948,  0.75045013, ...,  0.6519301 ,\n",
       "          0.2481015 , -0.07473479],\n",
       "        [ 0.6927793 ,  0.39143902,  0.849975  , ..., -0.02917881,\n",
       "          0.7804612 ,  0.8315323 ],\n",
       "        ...,\n",
       "        [ 1.0741067 , -0.73746186,  0.13457963, ...,  0.21906231,\n",
       "          0.2692553 , -0.2048554 ],\n",
       "        [ 0.40146548,  0.82552207,  0.22120453, ...,  0.37094074,\n",
       "          0.6960912 ,  0.29183385],\n",
       "        [ 0.7315277 ,  0.3760425 ,  0.20362827, ...,  0.39146245,\n",
       "          0.6885634 ,  0.46912655]], dtype=float32),\n",
       " array([[-0.02193033,  0.20462446,  1.4557061 ],\n",
       "        [-1.6052178 ,  0.8849268 ,  1.2448972 ],\n",
       "        [ 0.13785496,  1.2514919 ,  0.7921882 ],\n",
       "        [ 0.08641402,  0.6892805 ,  1.2131819 ],\n",
       "        [-0.11112709, -0.90254074, -1.4823998 ],\n",
       "        [ 1.4405588 , -2.7355027 , -1.8185669 ],\n",
       "        [ 0.5263429 , -1.4951218 , -0.83612007],\n",
       "        [ 2.1509273 , -1.900049  ,  1.2664292 ],\n",
       "        [ 0.0059005 , -0.95694053, -2.51791   ],\n",
       "        [-1.5621365 , -0.52165407, -1.991539  ],\n",
       "        [-0.8575131 ,  0.46886086,  0.4169286 ],\n",
       "        [-0.13685274,  0.9953852 ,  1.0463353 ],\n",
       "        [-0.0619151 ,  0.91423833,  0.672661  ],\n",
       "        [-0.69493455,  1.3039125 ,  1.098444  ],\n",
       "        [-0.48170802,  1.4280084 ,  0.5424413 ],\n",
       "        [ 1.0867591 ,  0.90816134, -0.18087168],\n",
       "        [ 1.033532  ,  0.9849469 ,  2.4871545 ],\n",
       "        [-0.00719385,  0.48923475,  0.4922362 ],\n",
       "        [ 0.27522624,  1.5901332 ,  0.8479462 ],\n",
       "        [-0.7426934 ,  1.2090043 , -0.35502782],\n",
       "        [ 0.03078144, -2.1221979 , -2.08307   ],\n",
       "        [-1.160103  ,  2.2083893 , -0.1796924 ],\n",
       "        [ 0.4672469 , -1.3025665 , -2.1027918 ],\n",
       "        [ 0.7820811 , -1.5759784 , -0.87615824],\n",
       "        [ 0.1021359 ,  0.60054976,  0.50805324],\n",
       "        [ 0.39910188,  0.68860877,  1.364221  ],\n",
       "        [ 1.184757  , -0.72319883, -1.5328736 ],\n",
       "        [-0.5491887 ,  0.832845  ,  0.7226137 ],\n",
       "        [-2.1925404 ,  0.43073145,  0.634913  ],\n",
       "        [-0.6171089 , -0.2786777 , -1.5510032 ],\n",
       "        [-0.8821803 ,  0.23911677,  0.5503265 ]], dtype=float32),\n",
       " array([-0.28383029, -0.31458557, -0.15186486,  0.31620264, -0.26639026,\n",
       "         0.6925992 ,  0.52916116, -0.03149702, -0.00305842, -0.35175923,\n",
       "         0.13541721, -0.21263544, -0.06160916, -0.31217527, -0.10744675,\n",
       "         0.6047094 , -0.12664579, -0.06842119, -0.04330331, -0.08283355,\n",
       "        -0.23805155, -0.1870908 , -0.23751526, -0.3471313 , -0.21489719,\n",
       "        -0.18942702, -0.63831514, -0.84191936,  0.05194964, -0.02596493,\n",
       "        -0.31447604, -0.33435127, -0.15995553, -0.20170794, -0.14258547,\n",
       "        -0.01627403, -0.10622326, -0.08091658, -0.3327444 ,  0.64246714,\n",
       "        -0.06249094, -0.20345554, -0.00187809, -0.10168617, -0.10713416,\n",
       "        -0.32413107, -0.04729197, -0.11250894, -0.19976912, -0.28495467,\n",
       "        -0.17469911, -0.30920935,  1.0456896 ,  0.5539024 , -0.13741975,\n",
       "        -0.07063854, -0.16984136, -0.18727449, -0.30248228, -0.4527247 ,\n",
       "        -0.23112178,  0.3078405 , -0.19898465, -0.23572895, -0.23653843,\n",
       "        -0.27749056, -0.33645573, -0.3837324 , -0.31371927, -0.30730048,\n",
       "         0.03483503, -0.34579587, -0.23848681, -0.1865959 , -0.09067887,\n",
       "        -0.01844228, -0.18869911, -0.40722778,  0.29200387,  0.69502145,\n",
       "        -0.19297187, -0.13239871, -0.53149635, -0.19007044, -0.25604025,\n",
       "        -0.32444695, -0.2393859 , -0.16607864, -0.18790606, -0.24759218,\n",
       "        -0.01826485, -0.31940347, -0.25278816, -0.10948997, -0.11010415,\n",
       "        -0.17285003, -0.00264572, -0.14541171, -0.31624797, -0.3136641 ,\n",
       "        -0.00314215, -0.11497195, -0.06430664, -0.18368259, -0.17002307,\n",
       "         0.94308156, -0.1673885 , -0.26094735, -0.10315148, -0.06065961,\n",
       "        -0.16025904, -0.12804289, -0.17049542, -0.13507186, -0.13124591,\n",
       "        -0.06159161,  0.08041932, -0.22870962, -0.6009555 , -0.18614395,\n",
       "        -0.25848284, -0.21713065, -0.5370844 ,  1.1290488 , -0.17476822,\n",
       "        -0.0395139 ,  0.04845734, -0.1089573 , -0.21143371, -0.23421483,\n",
       "        -0.15427087, -0.13839707,  0.13148999, -0.08271152, -0.04803753,\n",
       "        -0.02969478, -0.16844337, -0.12841488,  0.251677  , -0.30410913,\n",
       "        -0.20056294, -0.20006253, -0.18251912,  0.11210488, -0.15121455,\n",
       "        -0.30812535, -0.22134608, -0.21894099, -0.18071161, -0.31525296,\n",
       "        -0.08532681, -0.03395198,  0.59042966, -0.09930573, -0.32260868],\n",
       "       dtype=float32),\n",
       " array([-0.15827918,  0.45958072, -0.7490588 ,  0.6354527 ,  0.8777603 ,\n",
       "        -0.7302024 ,  1.573461  , -1.2501152 , -0.06232604, -1.3450382 ,\n",
       "        -0.170423  , -0.29966292, -0.05741176,  0.40465623, -0.28665763,\n",
       "         0.59289634, -1.9006723 ,  0.07475471, -1.1395144 ,  0.4927689 ,\n",
       "        -1.3806654 , -0.10069096, -2.1741824 ,  1.5171554 ,  0.14704527,\n",
       "         0.39978042,  0.04457605, -0.2889082 ,  1.0811911 , -0.82404494,\n",
       "        -0.17152475], dtype=float32),\n",
       " array([[ 0.70961154, -1.5612065 , -1.942615  ]], dtype=float32)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(params)):\n",
    "    np.save('../save/weights/slearn_parms{}_6-3.npy'.format(i),params[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
