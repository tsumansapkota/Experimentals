{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os, sys, pathlib, random, time, pickle, copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nflib\n",
    "from nflib.flows import SequentialFlow, NormalizingFlow, ActNorm, ActNorm2D, AffineConstantFlow\n",
    "import nflib.coupling_flows as icf\n",
    "import nflib.inn_flow as inn\n",
    "import nflib.res_flow as irf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar_train = transforms.Compose([\n",
    "#     transforms.RandomCrop(size=32, padding=4),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(\n",
    "#         mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "#         std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "#     ),\n",
    "# ])\n",
    "\n",
    "# cifar_test = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(\n",
    "#         mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "#         std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "#     ),\n",
    "# ])\n",
    "\n",
    "# train_dataset = datasets.CIFAR10(root=\"../../../../../_Datasets/cifar10/\", train=True, download=True, transform=cifar_train)\n",
    "# test_dataset = datasets.CIFAR10(root=\"../../../../../_Datasets/cifar10/\", train=False, download=True, transform=cifar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_train = transforms.Compose([\n",
    "    transforms.RandomCrop(size=32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5071, 0.4865, 0.4409],\n",
    "        std=[0.2009, 0.1984, 0.2023],\n",
    "    ),\n",
    "])\n",
    "\n",
    "cifar_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5071, 0.4865, 0.4409],\n",
    "        std=[0.2009, 0.1984, 0.2023],\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR100(root=\"../../../../../_Datasets/cifar100/\", train=True, download=True, transform=cifar_train)\n",
    "test_dataset = datasets.CIFAR100(root=\"../../../../../_Datasets/cifar100/\", train=False, download=True, transform=cifar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 32, 32])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "actf = irf.Swish\n",
    "flows = [\n",
    "#     ActNorm2D(3),\n",
    "    nn.BatchNorm2d(3),\n",
    "    irf.ConvResidualFlow(3, [32, 32], kernels=5, activation=actf),\n",
    "    irf.InvertiblePooling(2),\n",
    "#     ActNorm2D(12),\n",
    "    nn.BatchNorm2d(12),\n",
    "    irf.ConvResidualFlow(12, [64, 64], kernels=5, activation=actf),\n",
    "#     ActNorm2D(12),\n",
    "    nn.BatchNorm2d(12),\n",
    "    irf.ConvResidualFlow(12, [64, 64], kernels=5, activation=actf),\n",
    "    irf.InvertiblePooling(2),\n",
    "#     ActNorm2D(48),\n",
    "    nn.BatchNorm2d(48),\n",
    "    irf.ConvResidualFlow(48, [128, 128], kernels=5, activation=actf),\n",
    "#     ActNorm2D(48),\n",
    "    nn.BatchNorm2d(48),\n",
    "    irf.ConvResidualFlow(48, [128, 128], kernels=5, activation=actf),\n",
    "    irf.InvertiblePooling(2),\n",
    "#     ActNorm2D(192),\n",
    "    nn.BatchNorm2d(192),\n",
    "    irf.ConvResidualFlow(192, [256, 256], kernels=5, activation=actf),\n",
    "#     ActNorm2D(192),\n",
    "    nn.BatchNorm2d(192),\n",
    "    irf.ConvResidualFlow(192, [256, 256], kernels=5, activation=actf),\n",
    "    nn.BatchNorm2d(192),\n",
    "    irf.Flatten(img_size=(192, 4, 4)),\n",
    "#     ActNorm(3072),\n",
    "#     nn.BatchNorm1d(3072),\n",
    "#     nn.Linear(3072, 3072, bias=False),\n",
    "    nn.BatchNorm1d(3072),\n",
    "        ]\n",
    "\n",
    "# backbone = SequentialFlow(flows)\n",
    "backbone = nn.Sequential(*flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): ConvResidualFlow(\n",
       "    (resblock): ModuleList(\n",
       "      (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (3): Swish()\n",
       "      (4): Conv2d(32, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (2): InvertiblePooling()\n",
       "  (3): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): ConvResidualFlow(\n",
       "    (resblock): ModuleList(\n",
       "      (0): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (3): Swish()\n",
       "      (4): Conv2d(64, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (5): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ConvResidualFlow(\n",
       "    (resblock): ModuleList(\n",
       "      (0): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (3): Swish()\n",
       "      (4): Conv2d(64, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (7): InvertiblePooling()\n",
       "  (8): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): ConvResidualFlow(\n",
       "    (resblock): ModuleList(\n",
       "      (0): Conv2d(48, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (3): Swish()\n",
       "      (4): Conv2d(128, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (10): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): ConvResidualFlow(\n",
       "    (resblock): ModuleList(\n",
       "      (0): Conv2d(48, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (3): Swish()\n",
       "      (4): Conv2d(128, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (12): InvertiblePooling()\n",
       "  (13): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ConvResidualFlow(\n",
       "    (resblock): ModuleList(\n",
       "      (0): Conv2d(192, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (3): Swish()\n",
       "      (4): Conv2d(256, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (15): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): ConvResidualFlow(\n",
       "    (resblock): ModuleList(\n",
       "      (0): Conv2d(192, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (3): Swish()\n",
       "      (4): Conv2d(256, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (17): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (18): Flatten()\n",
       "  (19): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 3072]), 3072)"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone(xx.to(device)).shape, 32*32*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  9947519\n"
     ]
    }
   ],
   "source": [
    "print(\"number of params: \", sum(p.numel() for p in backbone.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_children(module):\n",
    "    child = list(module.children())\n",
    "    if len(child) == 0:\n",
    "        return [module]\n",
    "    children = []\n",
    "    for ch in child:\n",
    "        grand_ch = get_children(ch)\n",
    "        children+=grand_ch\n",
    "    return children\n",
    "\n",
    "def remove_spectral_norm(model):\n",
    "    for child in get_children(model):\n",
    "        if hasattr(child, 'weight'):\n",
    "            print(\"Yes\", child)\n",
    "            try:\n",
    "                nn.utils.remove_spectral_norm(child)\n",
    "                print(\"Success\")\n",
    "            except:\n",
    "                print(\"Failed\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_spectral_norm(backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32]) torch.Size([128, 3072])\n"
     ]
    }
   ],
   "source": [
    "for xx, yy in train_loader:\n",
    "    tt = backbone(xx.to(device))\n",
    "    print(xx.shape, tt.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectedClassifier_Linear(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim, num_sets, output_dim, inv_temp=1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_sets = num_sets\n",
    "        self.inv_temp = nn.Parameter(torch.ones(1)*inv_temp)\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, num_sets)\n",
    "#         self.linear.bias.data *= 0\n",
    "#         self.linear.weight.data *= 0.1\n",
    "#         self.cls_weight = nn.Parameter(torch.randn(num_sets, output_dim)/output_dim)\n",
    "\n",
    "        init_val = torch.randn(num_sets, output_dim)\n",
    "        for ns in range(num_sets):\n",
    "            init_val[ns, ns%output_dim] = 5\n",
    "        self.cls_weight = nn.Parameter(init_val)\n",
    "        \n",
    "        self.cls_confidence = None\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hard=False):\n",
    "#         self.cls_weight.data = torch.abs(self.cls_weight.data/self.cls_weight.data.sum(dim=1, keepdim=True))\n",
    "        \n",
    "        x = self.linear(x)*torch.exp(self.inv_temp)\n",
    "        if hard:\n",
    "            x = torch.softmax(x*1e5, dim=1)\n",
    "        else:\n",
    "            x = torch.softmax(x, dim=1)\n",
    "#             x = torch.softmax(x*self.inv_temp, dim=1)\n",
    "        self.cls_confidence = x\n",
    "#         c = torch.softmax(self.cls_weight, dim=1)\n",
    "        c = self.cls_weight\n",
    "        return x@c ## since both are normalized, it is also normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectedClassifier_SoftKMeans(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim, num_sets, output_dim, inv_temp=1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_sets = num_sets\n",
    "        self.inv_temp = nn.Parameter(torch.ones(1)*inv_temp)\n",
    "        \n",
    "        self.centers = nn.Parameter(torch.rand(num_sets, input_dim)*2-1)\n",
    "        \n",
    "#         self.cls_weight = nn.Parameter(torch.ones(num_sets, output_dim)/output_dim)\n",
    "\n",
    "        init_val = torch.randn(num_sets, output_dim)\n",
    "        for ns in range(num_sets):\n",
    "            init_val[ns, ns%output_dim] = 5\n",
    "        self.cls_weight = nn.Parameter(init_val)\n",
    "\n",
    "        self.cls_confidence = None\n",
    "        self.ln = nn.LayerNorm(self.num_sets, elementwise_affine=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hard=False):\n",
    "#         self.cls_weight.data = torch.abs(self.cls_weight.data/self.cls_weight.data.sum(dim=1, keepdim=True))\n",
    "        \n",
    "        dists = torch.cdist(x, self.centers)\n",
    "        ### correction to make diagonal of unit square 1 in nD space\n",
    "        dists = dists/np.sqrt(self.input_dim)\n",
    "\n",
    "        dists = self.ln(dists)\n",
    "        dists = dists*torch.exp(self.inv_temp)\n",
    "        \n",
    "#         dists = dists/self.input_dim\n",
    "#         dists = dists/dists.norm(dim=1, keepdim=True)\n",
    "        if hard:\n",
    "            x = torch.softmax(-dists*1e5, dim=1)\n",
    "        else:\n",
    "            x = torch.softmax(-dists, dim=1)\n",
    "#             x = torch.softmax(-dists*self.inv_temp, dim=1)\n",
    "        self.cls_confidence = x\n",
    "#         c = torch.softmax(self.cls_weight, dim=1)\n",
    "        c = self.cls_weight\n",
    "        return x@c ## since both are normalized, it is also normalized\n",
    "#         return torch.softmax(x@self.cls_weight, dim=1)\n",
    "\n",
    "    def set_centroid_to_data_randomly(self, data_loader, model):\n",
    "        num_centers = self.centers.shape[0]\n",
    "        xxs, yys = [], []\n",
    "        count = 0\n",
    "        for xx, yy in data_loader:\n",
    "            yout = model(xx.to(device)).data.cpu()\n",
    "            xxs.append(yout)\n",
    "            yys.append(yy)\n",
    "            count += len(xx)\n",
    "            if count >= num_centers:\n",
    "                break\n",
    "        \n",
    "        yout = torch.cat(xxs, dim=0)\n",
    "        yy = torch.cat(yys, dim=0)\n",
    "        \n",
    "        yout = yout[:num_centers].to(self.centers.device)\n",
    "        yy = yy[:num_centers].to(self.centers.device)\n",
    "        \n",
    "        self.centers.data = yout\n",
    "        \n",
    "        init_val = torch.randn(self.num_sets, self.output_dim)#/self.output_dim\n",
    "        for ns in range(num_centers):\n",
    "            init_val[ns, yy[ns]] = 5.\n",
    "        self.cls_weight.data = init_val.to(self.cls_weight.device)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### for cifar 10\n",
    "# classifier = ConnectedClassifier_SoftKMeans(3072, 100, 10)\n",
    "# classifier = ConnectedClassifier_Linear(3072, 100, 10)\n",
    "\n",
    "#### for cifar 100\n",
    "classifier = ConnectedClassifier_SoftKMeans(3072, 500, 100, inv_temp=0)\n",
    "# classifier = ConnectedClassifier_Linear(3072, 500, 100, inv_temp=0)\n",
    "# classifier = ConnectedClassifier_Linear(3072, 500, 100, )\n",
    "\n",
    "#### for MLP based classification\n",
    "# classifier = nn.Sequential(nn.Linear(3072, 3072), nn.SELU(), nn.Linear(3072, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.set_centroid_to_data_randomly(train_loader, backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  9947519\n",
      "number of params:  1586001\n"
     ]
    }
   ],
   "source": [
    "print(\"number of params: \", sum(p.numel() for p in backbone.parameters()))\n",
    "print(\"number of params: \", sum(p.numel() for p in classifier.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "### debug linear classifier\n",
    "yout = classifier(torch.randn(10, 3072).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([-0.0832, -0.0626, -0.0566, -0.0523, -0.0478, -0.0411, -0.0366, -0.0301,\n",
       "        -0.0286, -0.0270, -0.0177, -0.0161, -0.0138, -0.0136, -0.0134, -0.0091,\n",
       "        -0.0086, -0.0075, -0.0073, -0.0069, -0.0068, -0.0068, -0.0063, -0.0063,\n",
       "        -0.0039,  0.0025,  0.0041,  0.0045,  0.0094,  0.0156,  0.0160,  0.0193,\n",
       "         0.0197,  0.0200,  0.0217,  0.0225,  0.0249,  0.0287,  0.0291,  0.0301,\n",
       "         0.0318,  0.0321,  0.0332,  0.0349,  0.0355,  0.0362,  0.0370,  0.0374,\n",
       "         0.0398,  0.0421,  0.0461,  0.0472,  0.0495,  0.0499,  0.0502,  0.0511,\n",
       "         0.0517,  0.0535,  0.0546,  0.0562,  0.0580,  0.0618,  0.0651,  0.0663,\n",
       "         0.0675,  0.0726,  0.0734,  0.0745,  0.0776,  0.0800,  0.0815,  0.0830,\n",
       "         0.0860,  0.0866,  0.0882,  0.0900,  0.0922,  0.0929,  0.0939,  0.1003,\n",
       "         0.1011,  0.1025,  0.1048,  0.1054,  0.1077,  0.1095,  0.1106,  0.1123,\n",
       "         0.1184,  0.1209,  0.1233,  0.1417,  0.1441,  0.1453,  0.1536,  0.1640,\n",
       "         0.1690,  0.1740,  0.1926,  0.2048], device='cuda:0',\n",
       "       grad_fn=<SortBackward0>),\n",
       "indices=tensor([93, 71, 98, 64, 92, 27, 49, 77, 80, 48, 72, 99, 83, 28, 41, 26, 78, 36,\n",
       "        21, 53,  3, 70,  9, 73, 47, 22, 16, 30, 24, 95, 19, 44, 35, 69, 96, 14,\n",
       "        62,  6,  8, 46, 50,  7, 32, 52, 10, 25, 94, 51, 60, 17, 65, 89,  0, 37,\n",
       "        75, 20,  2, 68, 85, 82, 81, 61, 57, 40, 55,  1, 56, 76, 29, 45, 54, 34,\n",
       "        58, 13, 33, 59, 18, 84, 38, 88, 97, 67, 23, 86, 66, 11, 43, 12,  5, 90,\n",
       "        79,  4, 31, 91, 87, 63, 74, 39, 15, 42], device='cuda:0'))"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "yout[i].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([3.0278e-05, 3.3608e-05, 3.8403e-05, 3.8621e-05, 4.5161e-05, 4.7087e-05,\n",
       "        4.7437e-05, 5.1392e-05, 5.2420e-05, 7.6403e-05, 9.4347e-05, 1.0151e-04,\n",
       "        1.0331e-04, 1.0731e-04, 1.2773e-04, 1.3065e-04, 1.4096e-04, 1.4781e-04,\n",
       "        1.5580e-04, 1.5842e-04, 1.5904e-04, 1.6153e-04, 1.6203e-04, 1.7211e-04,\n",
       "        1.7308e-04, 1.8065e-04, 1.8561e-04, 1.9515e-04, 2.0109e-04, 2.0505e-04,\n",
       "        2.0999e-04, 2.1302e-04, 2.1433e-04, 2.2519e-04, 2.3412e-04, 2.4682e-04,\n",
       "        2.5914e-04, 2.5943e-04, 2.6600e-04, 2.7200e-04, 2.8443e-04, 2.9012e-04,\n",
       "        2.9746e-04, 2.9959e-04, 3.0043e-04, 3.1509e-04, 3.2080e-04, 3.2269e-04,\n",
       "        3.3399e-04, 3.3474e-04, 3.4041e-04, 3.4125e-04, 3.5525e-04, 3.6157e-04,\n",
       "        3.6179e-04, 3.7181e-04, 3.7595e-04, 3.7719e-04, 3.8021e-04, 3.8124e-04,\n",
       "        3.8465e-04, 3.8831e-04, 3.9240e-04, 4.1332e-04, 4.1978e-04, 4.3027e-04,\n",
       "        4.3379e-04, 4.5319e-04, 4.5752e-04, 4.5937e-04, 4.6889e-04, 4.7223e-04,\n",
       "        4.8378e-04, 4.8663e-04, 4.8999e-04, 4.9940e-04, 5.2343e-04, 5.2700e-04,\n",
       "        5.2993e-04, 5.3623e-04, 5.3763e-04, 5.4187e-04, 5.5834e-04, 5.7597e-04,\n",
       "        5.7781e-04, 5.7940e-04, 5.8005e-04, 5.8948e-04, 6.2775e-04, 6.4136e-04,\n",
       "        6.7244e-04, 6.8551e-04, 6.8866e-04, 6.9110e-04, 6.9591e-04, 7.0014e-04,\n",
       "        7.0121e-04, 7.0482e-04, 7.1462e-04, 7.2180e-04, 7.2641e-04, 7.3376e-04,\n",
       "        7.3867e-04, 7.4967e-04, 7.5604e-04, 7.5950e-04, 7.6397e-04, 7.6947e-04,\n",
       "        7.7554e-04, 7.8441e-04, 7.8653e-04, 7.9773e-04, 8.0207e-04, 8.0256e-04,\n",
       "        8.0426e-04, 8.1349e-04, 8.2242e-04, 8.2422e-04, 8.2486e-04, 8.2797e-04,\n",
       "        8.3645e-04, 8.4457e-04, 8.5145e-04, 8.5680e-04, 8.5985e-04, 8.7300e-04,\n",
       "        8.7582e-04, 8.8212e-04, 8.8875e-04, 8.9871e-04, 9.0706e-04, 9.1068e-04,\n",
       "        9.1142e-04, 9.1611e-04, 9.2080e-04, 9.3502e-04, 9.4873e-04, 9.7047e-04,\n",
       "        9.7124e-04, 9.8151e-04, 9.8388e-04, 9.8930e-04, 9.9492e-04, 9.9533e-04,\n",
       "        1.0159e-03, 1.0315e-03, 1.0370e-03, 1.0427e-03, 1.0482e-03, 1.0848e-03,\n",
       "        1.0924e-03, 1.0927e-03, 1.0930e-03, 1.0941e-03, 1.1047e-03, 1.1048e-03,\n",
       "        1.1057e-03, 1.1158e-03, 1.1158e-03, 1.1166e-03, 1.1173e-03, 1.1346e-03,\n",
       "        1.1403e-03, 1.1491e-03, 1.1686e-03, 1.1721e-03, 1.1895e-03, 1.1904e-03,\n",
       "        1.1928e-03, 1.1968e-03, 1.2084e-03, 1.2455e-03, 1.2760e-03, 1.2894e-03,\n",
       "        1.3268e-03, 1.3419e-03, 1.3822e-03, 1.3903e-03, 1.3942e-03, 1.4474e-03,\n",
       "        1.4665e-03, 1.4706e-03, 1.4857e-03, 1.5100e-03, 1.5292e-03, 1.5587e-03,\n",
       "        1.5720e-03, 1.5845e-03, 1.5890e-03, 1.6080e-03, 1.6224e-03, 1.6225e-03,\n",
       "        1.6227e-03, 1.6369e-03, 1.6456e-03, 1.6486e-03, 1.6579e-03, 1.6641e-03,\n",
       "        1.6872e-03, 1.6919e-03, 1.6954e-03, 1.7131e-03, 1.7151e-03, 1.7152e-03,\n",
       "        1.7173e-03, 1.7199e-03, 1.7261e-03, 1.7281e-03, 1.7359e-03, 1.7579e-03,\n",
       "        1.7604e-03, 1.7765e-03, 1.7837e-03, 1.7841e-03, 1.7912e-03, 1.7912e-03,\n",
       "        1.8413e-03, 1.8433e-03, 1.8487e-03, 1.8512e-03, 1.8579e-03, 1.8601e-03,\n",
       "        1.8624e-03, 1.8683e-03, 1.8949e-03, 1.8982e-03, 1.9002e-03, 1.9028e-03,\n",
       "        1.9143e-03, 1.9269e-03, 1.9462e-03, 1.9471e-03, 1.9518e-03, 1.9575e-03,\n",
       "        1.9602e-03, 1.9691e-03, 1.9723e-03, 1.9824e-03, 1.9870e-03, 1.9896e-03,\n",
       "        1.9959e-03, 1.9964e-03, 2.0017e-03, 2.0028e-03, 2.0098e-03, 2.0133e-03,\n",
       "        2.0178e-03, 2.0262e-03, 2.0859e-03, 2.0876e-03, 2.0945e-03, 2.1084e-03,\n",
       "        2.1096e-03, 2.1168e-03, 2.1192e-03, 2.1524e-03, 2.1546e-03, 2.1603e-03,\n",
       "        2.1735e-03, 2.1741e-03, 2.1799e-03, 2.1837e-03, 2.2024e-03, 2.2058e-03,\n",
       "        2.2087e-03, 2.2137e-03, 2.2148e-03, 2.2273e-03, 2.2289e-03, 2.2343e-03,\n",
       "        2.2566e-03, 2.2587e-03, 2.2636e-03, 2.2651e-03, 2.2675e-03, 2.2675e-03,\n",
       "        2.2694e-03, 2.2737e-03, 2.2969e-03, 2.3055e-03, 2.3189e-03, 2.3257e-03,\n",
       "        2.3290e-03, 2.3312e-03, 2.3518e-03, 2.3533e-03, 2.3539e-03, 2.3552e-03,\n",
       "        2.3744e-03, 2.3990e-03, 2.4064e-03, 2.4104e-03, 2.4114e-03, 2.4147e-03,\n",
       "        2.4181e-03, 2.4339e-03, 2.4411e-03, 2.4436e-03, 2.4441e-03, 2.4496e-03,\n",
       "        2.4509e-03, 2.4573e-03, 2.4579e-03, 2.4599e-03, 2.4603e-03, 2.4640e-03,\n",
       "        2.4657e-03, 2.4664e-03, 2.4763e-03, 2.5244e-03, 2.5270e-03, 2.5384e-03,\n",
       "        2.5493e-03, 2.5610e-03, 2.5636e-03, 2.5640e-03, 2.5673e-03, 2.5850e-03,\n",
       "        2.5909e-03, 2.5918e-03, 2.5927e-03, 2.5958e-03, 2.5979e-03, 2.5981e-03,\n",
       "        2.6001e-03, 2.6051e-03, 2.6083e-03, 2.6208e-03, 2.6242e-03, 2.6245e-03,\n",
       "        2.6292e-03, 2.6323e-03, 2.6379e-03, 2.6416e-03, 2.6734e-03, 2.6827e-03,\n",
       "        2.6829e-03, 2.6831e-03, 2.6880e-03, 2.7174e-03, 2.7191e-03, 2.7216e-03,\n",
       "        2.7261e-03, 2.7336e-03, 2.7507e-03, 2.7537e-03, 2.7582e-03, 2.7588e-03,\n",
       "        2.7701e-03, 2.7738e-03, 2.7873e-03, 2.7966e-03, 2.8216e-03, 2.8236e-03,\n",
       "        2.8273e-03, 2.8274e-03, 2.8284e-03, 2.8284e-03, 2.8308e-03, 2.8454e-03,\n",
       "        2.8459e-03, 2.8498e-03, 2.8619e-03, 2.8838e-03, 2.9106e-03, 2.9121e-03,\n",
       "        2.9176e-03, 2.9189e-03, 2.9299e-03, 2.9341e-03, 2.9423e-03, 2.9501e-03,\n",
       "        2.9554e-03, 2.9606e-03, 2.9628e-03, 2.9650e-03, 2.9701e-03, 2.9754e-03,\n",
       "        2.9763e-03, 2.9779e-03, 2.9809e-03, 2.9931e-03, 2.9987e-03, 3.0014e-03,\n",
       "        3.0020e-03, 3.0026e-03, 3.0063e-03, 3.0077e-03, 3.0092e-03, 3.0217e-03,\n",
       "        3.0272e-03, 3.0285e-03, 3.0293e-03, 3.0396e-03, 3.0437e-03, 3.0554e-03,\n",
       "        3.0567e-03, 3.0635e-03, 3.0735e-03, 3.0763e-03, 3.0800e-03, 3.0828e-03,\n",
       "        3.0844e-03, 3.1032e-03, 3.1092e-03, 3.1147e-03, 3.1178e-03, 3.1339e-03,\n",
       "        3.1379e-03, 3.1408e-03, 3.1447e-03, 3.1665e-03, 3.1851e-03, 3.1857e-03,\n",
       "        3.2101e-03, 3.2384e-03, 3.2544e-03, 3.2586e-03, 3.2669e-03, 3.2671e-03,\n",
       "        3.2990e-03, 3.3167e-03, 3.3209e-03, 3.3304e-03, 3.3328e-03, 3.3440e-03,\n",
       "        3.3608e-03, 3.3749e-03, 3.3753e-03, 3.3761e-03, 3.3832e-03, 3.3901e-03,\n",
       "        3.4020e-03, 3.4208e-03, 3.4310e-03, 3.4340e-03, 3.4400e-03, 3.4469e-03,\n",
       "        3.4557e-03, 3.4655e-03, 3.4734e-03, 3.4771e-03, 3.4831e-03, 3.5260e-03,\n",
       "        3.5264e-03, 3.5284e-03, 3.5481e-03, 3.5744e-03, 3.5792e-03, 3.5810e-03,\n",
       "        3.5945e-03, 3.6011e-03, 3.6096e-03, 3.6109e-03, 3.6238e-03, 3.6519e-03,\n",
       "        3.6560e-03, 3.6566e-03, 3.6775e-03, 3.6815e-03, 3.6834e-03, 3.6950e-03,\n",
       "        3.7039e-03, 3.7152e-03, 3.7423e-03, 3.7478e-03, 3.7641e-03, 3.7701e-03,\n",
       "        3.7962e-03, 3.8241e-03, 3.8563e-03, 3.8758e-03, 3.8942e-03, 3.9494e-03,\n",
       "        3.9880e-03, 3.9896e-03, 4.0029e-03, 4.0074e-03, 4.0167e-03, 4.0200e-03,\n",
       "        4.0411e-03, 4.0484e-03, 4.0539e-03, 4.0661e-03, 4.1239e-03, 4.1448e-03,\n",
       "        4.1451e-03, 4.2283e-03, 4.2381e-03, 4.2764e-03, 4.3313e-03, 4.3640e-03,\n",
       "        4.4775e-03, 4.4843e-03, 4.5973e-03, 4.6500e-03, 4.7516e-03, 4.8956e-03,\n",
       "        5.0018e-03, 5.0045e-03], device='cuda:0', grad_fn=<SortBackward0>),\n",
       "indices=tensor([328, 350, 461, 397, 304, 455, 379,   6, 163, 477, 139, 386,  74,  80,\n",
       "        228, 277, 493,  49, 419,  41, 150, 486, 111, 264, 406, 366, 227, 256,\n",
       "        222, 217, 231, 174, 138, 483, 441, 242, 302, 459, 255, 403, 142, 316,\n",
       "        118, 453, 254, 141, 205,  33, 288, 344,  26, 269, 121,  81, 398,  89,\n",
       "        267, 272, 289, 318, 190, 358, 109, 104, 471, 457, 127,  94, 310,  91,\n",
       "        230, 146,  77, 279,  57, 437,  29, 236, 220, 162, 424, 347, 253,  31,\n",
       "        466, 401, 213, 221, 367, 415, 384,  38, 411, 226,  86, 342,  47, 443,\n",
       "         13, 295, 425,  14,  90, 414, 487, 334, 186, 452, 375, 314, 329, 434,\n",
       "        481, 181, 311, 433,   1, 137, 387,  55, 422, 225,  19, 177, 114, 340,\n",
       "         48, 102,  23, 467, 335,  53, 448, 122, 343, 474, 286, 327, 212, 237,\n",
       "          0, 245,  60, 420,  54,  64, 303, 145, 470, 317, 285, 349,  82, 268,\n",
       "        410,  39, 182, 249, 332,  51, 247,  56, 321,   5, 417,  12,  93, 159,\n",
       "        278, 178, 394, 291, 389, 153, 193, 189, 409, 416, 207, 392, 273, 324,\n",
       "        179,  36, 251, 325, 259, 351, 378, 427, 223, 319,  32,   7, 326,   4,\n",
       "        216, 209, 374,  25, 444, 494,   3, 117, 103, 360, 248, 376, 266, 496,\n",
       "        218,  42, 263,  40, 450, 377, 356, 170,  75, 287,  18, 307,  69, 313,\n",
       "        115, 296, 362, 468, 196, 462, 176, 126, 293, 315, 372,  78, 464, 369,\n",
       "        331, 489, 370, 106, 160,  96, 204, 128, 173, 275, 136,  11,  46, 391,\n",
       "        246, 191, 348, 147,  63,  88,  79, 306, 233, 134, 149, 257,  65,  16,\n",
       "        364, 240, 124, 224, 472, 274, 400, 438,  84,   9, 202, 480, 168,  27,\n",
       "         83, 101, 423, 187, 158, 208, 175, 499, 388, 166,  59, 454, 497, 320,\n",
       "        426,   8, 201, 283, 492, 436, 271, 396, 113, 151, 131,  85, 235, 413,\n",
       "        261,  37, 346, 428, 197,  73, 100, 184, 171,  44, 210, 341, 322, 390,\n",
       "        238, 135,  34, 294, 418, 229, 270,  43,  62,  68, 431, 495, 119, 154,\n",
       "        485, 194, 333, 155, 262, 432,  28,  87, 140,  98, 305, 469, 447, 429,\n",
       "        116, 404,  72,  71, 290, 250, 475, 200, 405, 180,  21,   2, 144,  70,\n",
       "        354, 219, 345, 473, 284, 292, 280, 232, 152, 435, 108, 309, 371, 183,\n",
       "        357, 365, 143, 112,  30, 301, 395, 456, 385, 449, 439, 215, 192, 407,\n",
       "         17, 203,  22, 383, 312, 164, 156, 399,  67, 281, 120, 352, 478, 440,\n",
       "        199, 336, 359, 123,  66, 373, 458, 488, 282, 133, 491, 402,  50, 276,\n",
       "        380,  95, 165, 244,  24, 265, 446,  45, 368, 299, 239, 460,  58, 482,\n",
       "        110, 451, 130,  61,  35, 430,  10, 148,  20, 421, 107, 337, 490, 353,\n",
       "         76, 412, 408, 381,  99, 195, 463, 308, 211, 338, 129, 355, 169, 234,\n",
       "        260, 241, 188, 393, 498, 339, 323,  15, 442, 157, 185, 361, 382, 363,\n",
       "         92, 167, 172, 479, 484, 161, 298, 125, 476, 105, 445, 330, 300, 198,\n",
       "        252,  97, 214, 297, 243, 465, 206, 132, 258,  52], device='cuda:0'))"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.cls_confidence[i].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(backbone, classifier).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  11533520\n"
     ]
    }
   ],
   "source": [
    "print(\"number of params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'c100_inv_v5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'c10_inv_v0'\n",
    "# model_name = 'c10_ord_v0'\n",
    "# model_name = 'c100_inv_v0'\n",
    "# model_name = 'c100_ord_v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1,\n",
    "#                       momentum=0.9, weight_decay=5e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Following is copied from \n",
    "### https://github.com/kuangliu/pytorch-cifar/blob/master/main.py\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    print(f\"[Train] {epoch} Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = -1\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "    print(f\"[Test] {epoch} Loss: {test_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "    \n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'model': model.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('models'):\n",
    "            os.mkdir('models')\n",
    "        torch.save(state, f'./models/{model_name}.pth')\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "resume = False\n",
    "\n",
    "if resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('./models'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load(f'./models/{model_name}.pth')\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:39<00:00,  9.93it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 0 Loss: 4.507 | Acc: 7.066 3533/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:02<00:00, 29.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 0 Loss: 4.479 | Acc: 7.230 723/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:39<00:00,  9.88it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 1 Loss: 4.469 | Acc: 7.284 3642/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:02<00:00, 28.92it/s]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 1 Loss: 4.448 | Acc: 7.100 710/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:39<00:00,  9.87it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 2 Loss: 4.431 | Acc: 7.554 3777/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:02<00:00, 29.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 2 Loss: 4.405 | Acc: 7.790 779/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:39<00:00,  9.88it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 3 Loss: 4.401 | Acc: 8.034 4017/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:02<00:00, 29.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 3 Loss: 4.395 | Acc: 7.870 787/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:39<00:00,  9.80it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 4 Loss: 4.375 | Acc: 8.372 4186/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:02<00:00, 28.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 4 Loss: 4.362 | Acc: 8.580 858/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:39<00:00,  9.78it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 5 Loss: 4.339 | Acc: 8.674 4337/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:02<00:00, 28.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 5 Loss: 4.314 | Acc: 8.960 896/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:39<00:00,  9.78it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 6 Loss: 4.308 | Acc: 8.824 4412/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:02<00:00, 28.51it/s]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 6 Loss: 4.294 | Acc: 8.800 880/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:40<00:00,  9.77it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 7 Loss: 4.270 | Acc: 9.164 4582/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:02<00:00, 28.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 7 Loss: 4.246 | Acc: 9.540 954/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:40<00:00,  9.76it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 8 Loss: 4.232 | Acc: 9.806 4903/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:02<00:00, 28.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 8 Loss: 4.216 | Acc: 9.660 966/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:40<00:00,  9.73it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 9 Loss: 4.199 | Acc: 10.254 5127/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:02<00:00, 28.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 9 Loss: 4.197 | Acc: 10.230 1023/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:40<00:00,  9.76it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 10 Loss: 4.162 | Acc: 10.942 5471/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:02<00:00, 28.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 10 Loss: 4.139 | Acc: 11.010 1101/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:40<00:00,  9.77it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 11 Loss: 4.120 | Acc: 11.504 5752/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:02<00:00, 28.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 11 Loss: 4.088 | Acc: 12.350 1235/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:40<00:00,  9.74it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 12 Loss: 4.073 | Acc: 12.218 6109/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:02<00:00, 28.89it/s]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 12 Loss: 4.052 | Acc: 11.760 1176/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 215/391 [00:22<00:18,  9.72it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-494-43766c9b5561>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m## for 200 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-491-b15a53db0632>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    108\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Train the whole damn thing\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+EPOCHS): ## for 200 epochs\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.54"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## C100\n",
    "#### inv nn with MLP classifier: 59.3 Acc ;\n",
    "#### non-inv nn with MLP classifier: 54.67 Acc ;\n",
    "\n",
    "#### inv nn + ConnectedDist: 30.03 Acc;\n",
    "#### inv nn + ConnectedLin: 48.79 Acc; -> v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### benchmark on non-inv + MLP/ConC has very low performance.. verifying here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.1376], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.inv_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59.3, 197)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(f'./models/{model_name}.pth')\n",
    "best_acc = checkpoint['acc']\n",
    "start_epoch = checkpoint['epoch']\n",
    "\n",
    "best_acc, start_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard test accuracy with count per classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:02<00:00, 28.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Test Acc:54.97%\n",
      "[0, 0, 4, 0, 0, 11, 30, 1, 0, 0, 3, 11, 0, 3, 3, 0, 0, 4, 0, 0, 0, 0, 3, 0, 0, 0, 0, 20, 0, 1, 3, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 5, 0, 3, 0, 0, 0, 2, 0, 0, 0, 0, 18, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 86, 1, 0, 18, 0, 0, 0, 0, 0, 8, 0, 0, 0, 4, 0, 2, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 17, 33, 0, 0, 0, 3, 1, 0, 5, 2, 0, 0, 0, 0, 0, 0, 6, 15, 0, 31, 0, 20, 6, 0, 0, 0, 21, 0, 0, 3, 67, 0, 0, 7, 1, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 84, 0, 0, 0, 2, 0, 0, 0, 0, 18, 0, 0, 0, 0, 2, 0, 0, 0, 2, 5, 0, 0, 13, 0, 0, 0, 7, 0, 3, 0, 3, 11, 12, 0, 0, 0, 15, 0, 0, 0, 0, 7, 0, 0, 1, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 7, 0, 0, 44, 4, 0, 0, 51, 0, 22, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 15, 0, 0, 4, 0, 0, 1, 6, 0, 26, 0, 0, 0, 18, 0, 19, 0, 24, 0, 3, 98, 0, 0, 0, 0, 0, 0, 0, 73, 0, 16, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 27, 0, 2, 6, 3, 0, 3, 0, 0, 1, 0, 0, 0, 14, 0, 23, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 96, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 15, 0, 0, 0, 1, 0, 0, 0, 2, 86, 0, 1, 0, 0, 0, 0, 7, 0, 0, 0, 2, 41, 9, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 58, 0, 2, 0, 4, 25, 0, 1, 0, 13, 4, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 5, 0, 1, 0, 22, 2, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 22, 0, 0, 5, 0, 0, 0, 9, 0, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 109, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 7, 0, 0, 3, 0, 0, 0, 0, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 27, 9, 0, 0, 0, 0, 7, 38, 0, 0, 1, 0, 28, 1, 0, 0, 0, 16, 6, 2, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 1, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 4, 0, 0, 6, 30, 1, 0, 0, 14, 0, 5, 0, 18, 1, 0, 0, 0, 0, 0, 0, 32, 4, 1, 0, 0, 0, 0, 3, 0, 0, 0, 12, 0, 0, 1, 0, 0, 0, 4, 0, 0, 0, 90, 0, 2, 0, 0, 3, 3, 0, 0, 0, 11, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 22, 2, 0, 6, 50, 0, 0, 2, 0, 0, 0, 16, 0, 3, 0, 0, 3, 0, 0, 0, 57, 0, 0, 0, 0, 0, 0, 25, 0, 0, 0, 0, 3, 2, 0, 0, 34, 0, 0, 47, 0, 0, 0, 18, 1, 0, 1, 1, 3, 11, 0, 0, 6, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 41, 15, 16, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 21, 0, 0, 0, 0, 0, 35, 0, 0, 0, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 6, 0, 0, 1, 0, 13, 0, 0, 0, 81, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 9, 0, 0, 3, 0, 0, 0, 0, 0, 0, 24, 0, 0, 0, 4, 0, 0, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 9, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 2, 8, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 5, 0, 0, 0, 0, 90, 15, 0, 0, 1, 7, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 14, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 6, 0, 0, 0, 0, 3, 0, 0, 1, 0, 5, 0, 5, 0, 0, 2, 74, 0, 1, 0, 0, 0, 11, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 5, 0, 0, 2, 0, 0, 1, 0, 72, 0, 0, 0, 0, 5, 0, 0, 1, 0, 0, 4, 0, 3, 0, 0, 0, 0, 0, 42, 0, 66, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 92, 1, 0, 0, 0, 31, 45, 0, 0, 0, 0, 0, 15, 0, 3, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 26, 12, 0, 0, 0, 0, 4, 1, 1, 0, 10, 6, 0, 0, 0, 0, 8, 0, 0, 0, 49, 32, 0, 8, 1, 0, 3, 0, 5, 0, 11, 0, 0, 22, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 1, 26, 6, 0, 3, 0, 0, 0, 4, 1, 0, 0, 1, 0, 0, 20, 0, 0, 0, 6, 0, 4, 0, 0, 0, 0, 8, 0, 0, 0, 16, 1, 0, 0, 0, 0, 0, 0, 0, 0, 27, 0, 0, 0, 0, 58, 0, 0, 0, 0, 0, 0, 12, 0, 7, 0, 0, 0, 0, 0, 9, 0, 0, 0, 9, 0, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 10, 1, 6, 3, 5, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 6, 4, 55, 0, 13, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 1, 1, 0, 8, 0, 0, 94, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 22, 0, 7, 0, 0, 0, 21, 0, 1, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 4, 59, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 0, 1, 0, 30, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 59, 0, 0, 0, 2, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 3, 0, 24, 0, 55, 0, 0, 0, 0, 0, 0, 0, 0, 22, 0, 0, 0, 0, 0, 0, 9, 0, 2, 2, 0, 4, 0, 0, 1, 0, 24, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 74, 4, 68, 0, 0, 0, 3, 0, 2, 0, 4, 1, 0, 0, 0, 6, 4, 0, 0, 0, 5, 0, 0, 2, 0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 12, 0, 0, 0, 25, 0, 1, 1, 0, 0, 7, 2, 28, 3, 0, 3, 0, 0, 0, 0, 14, 0, 1, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 21, 0, 0, 0, 0, 0, 2, 0, 0, 17, 1, 8, 0, 1, 0, 12, 0, 0, 0, 0, 17, 0, 3, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 5, 36, 0, 2, 0, 0, 0, 0, 0, 0, 3, 0, 1, 5, 0, 0, 0, 2, 0, 0, 0, 0, 6, 7, 4, 0, 0, 0, 18, 0, 0, 0, 0, 0, 0, 0, 0, 21, 0, 0, 0, 0, 4, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, 0, 0, 0, 14, 0, 38, 0, 0, 0, 0, 67, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 60, 0, 0, 0, 0, 0, 0, 6, 0, 18, 0, 0, 0, 0, 0, 0, 1, 0, 0, 34, 0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 4, 0, 0, 0, 4, 0, 1, 0, 0, 0, 2, 0, 6, 0, 0, 0, 6, 0, 0, 0, 8, 1, 0, 57, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 7, 0, 0, 3, 0, 0, 0, 0, 0, 35, 0, 0, 2, 0, 0, 47, 0, 14, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 47, 0, 5, 0, 0, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 4, 3, 0, 0, 0, 0, 0, 3, 0, 1, 0, 11, 0, 0, 0, 36, 15, 10, 0, 0, 10, 53, 24, 0, 0, 0, 0, 0, 11, 2, 0, 0, 0, 55, 0, 2, 0, 0, 62, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 48, 37, 2, 14, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 0, 39, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 5, 0, 0, 0, 0, 8, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 1, 92, 7, 39, 0, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 32, 0, 0, 12, 0, 2, 0, 0, 0, 0, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 2, 0, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0, 1, 0, 0, 0, 34, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 4, 0, 3, 0, 0, 0, 2, 0, 2, 5, 3, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 8, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 7, 2, 15, 0, 16, 0, 0, 0, 0, 0, 2, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 61, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 0, 4, 9, 0, 32, 0, 0, 0, 0, 6, 61, 0, 0, 3, 7, 0, 12, 0, 0, 0, 0, 10, 0, 0, 0, 0, 1, 9, 21, 0, 6, 1, 0, 0, 0, 89, 0, 1, 0, 0, 0, 17, 0, 0, 0, 0, 3, 52, 0, 0, 21, 0, 30, 0, 1, 0, 1, 2, 0, 0, 0, 0, 25, 0, 0, 0, 0, 0, 80, 2, 0, 0, 0, 0, 0, 0, 14, 0, 0, 0, 0, 3, 0, 93, 12, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 5, 0, 3, 0, 0, 0, 0, 0, 12, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 13, 17, 18, 0, 0, 0, 0, 0, 0, 6, 0, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 59, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 9, 24, 0, 0, 1, 0, 0, 3, 0, 1, 1, 0, 0, 1, 3, 1, 0, 8, 0, 4, 0, 9, 0, 7, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 0, 0, 0, 11, 0, 0, 50, 0, 0, 0, 0, 0, 1, 0, 14, 0, 0, 0, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 5, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 12, 9, 0, 0, 0, 0, 0, 0, 15, 1, 0, 0, 9, 0, 4, 0, 0, 0, 0, 3, 0, 0, 0, 12, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 21, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 2, 0, 21, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 5, 0, 0, 2, 0, 29, 0, 2, 0, 9, 0, 0, 0, 0, 0, 0, 2, 6, 0, 0, 0, 8, 9, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 20, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 7, 0, 1, 0, 0, 0, 33, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 1, 14, 0, 15, 0, 0, 0, 18, 9, 0, 0, 3, 0, 0, 0, 12, 0, 0, 0, 0, 0, 2, 0, 0, 0, 3, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 6, 0, 0, 0, 43, 6, 0, 0, 0, 43, 0, 40, 0, 0, 0, 0, 5, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0, 0, 0, 0, 0, 0, 2, 2, 0, 9, 0, 0, 34, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 47, 17, 0, 0, 0, 0, 13, 0, 0, 1, 0, 1, 0, 0, 12, 0, 0, 0, 6, 0, 0, 0, 30, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 0, 0, 18, 0, 0, 0, 28, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 13, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 63, 0, 0, 0, 4, 0, 0, 0, 0, 5, 0, 6, 3, 0, 0, 3, 0, 0, 0, 9, 0, 0, 0, 0, 1, 0, 0, 6, 1, 1, 0, 0, 0, 0, 0, 3, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 26, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 2, 0, 1, 0, 4, 0, 0, 0, 0, 15, 0, 2, 0, 0, 0, 7, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 1, 0, 0, 4, 0, 0, 0, 1, 1, 9, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 50, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 17, 0, 0, 0, 31, 19, 0, 0, 0, 0, 0, 0, 4, 5, 0, 0, 0, 0, 0, 0, 3, 0, 71, 0, 6, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 5, 0, 4, 0, 0, 0, 0, 0, 0, 0, 6, 0, 3, 0, 35, 0, 0, 0, 0, 17, 6, 2, 18, 0, 0, 0, 1, 3, 0, 0, 0, 1, 1, 0, 0, 0, 0, 96, 0, 0, 0, 0, 23, 4, 51, 0, 0, 0, 43, 0, 0, 36, 0, 0, 0, 0, 54, 0, 0, 0, 3, 0, 0, 39, 0, 0, 0, 0, 0, 3, 35, 27, 0, 0, 0, 0, 16, 0, 0, 0, 7, 0, 0, 2, 1, 0, 0, 16, 0, 0, 0, 0, 0, 16, 0, 7, 0, 0, 0, 0, 1, 6, 38, 0, 0, 0, 9, 69, 0, 0, 0, 0, 1, 9, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_count = 0\n",
    "test_acc = 0\n",
    "set_count = torch.zeros(classifier.num_sets).to(device)\n",
    "for xx, yy in tqdm(test_loader):\n",
    "    xx, yy = xx.to(device), yy.to(device)\n",
    "    with torch.no_grad():\n",
    "        yout = classifier(backbone(xx), hard=True)\n",
    "        set_indx, count = torch.unique(torch.argmax(classifier.cls_confidence, dim=1), return_counts=True) \n",
    "        set_count[set_indx] += count\n",
    "    outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "    correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "    test_acc += correct\n",
    "    test_count += len(xx)\n",
    "\n",
    "print(f'Hard Test Acc:{float(test_acc)/test_count*100:.2f}%')\n",
    "print(set_count.type(torch.long).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.count_nonzero(set_count) ## tensor(810) for v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.cls_confidence[0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard train accuracy with count per classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:29<00:00, 13.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Train Acc:91.31%\n",
      "[0, 0, 0, 0, 4452, 25, 4597, 0, 4948, 0, 0, 0, 0, 162, 0, 0, 0, 0, 0, 4909, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5013, 4926, 0, 56, 0, 5022, 4125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 25, 0, 5073, 38, 4, 158, 0, 0, 0, 0, 0, 60, 9, 84, 0, 0, 119, 4967, 0, 25, 0, 0, 0, 0, 0, 0, 0, 0, 30, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 284, 0, 0, 0, 0, 0, 888, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_count = 0\n",
    "test_acc = 0\n",
    "set_count = torch.zeros(classifier.num_sets).to(device)\n",
    "for xx, yy in tqdm(train_loader):\n",
    "    xx, yy = xx.to(device), yy.to(device)\n",
    "    with torch.no_grad():\n",
    "        yout = classifier(backbone(xx), hard=True)\n",
    "        set_indx, count = torch.unique(torch.argmax(classifier.cls_confidence, dim=1), return_counts=True) \n",
    "        set_count[set_indx] += count\n",
    "    outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "    correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "    test_acc += correct\n",
    "    test_count += len(xx)\n",
    "\n",
    "print(f'Hard Train Acc:{float(test_acc)/test_count*100:.2f}%')\n",
    "print(set_count.type(torch.long).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(26, device='cuda:1')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Classifiers that enclose any data\n",
    "torch.count_nonzero(set_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 3, 7, 9, 5, 0, 2, 3, 3, 3, 7, 7, 5, 3, 5, 6, 3, 1, 8, 7, 3, 1, 5, 2,\n",
       "        3, 3, 3, 1, 6, 2, 1, 1, 5, 6, 9, 9, 2, 3, 4, 8, 3, 9, 8, 3, 5, 9, 9, 8,\n",
       "        3, 2, 2, 1, 8, 8, 8, 0, 6, 7, 2, 3, 5, 7, 0, 9, 1, 0, 6, 6, 5, 4, 1, 1,\n",
       "        9, 8, 2, 5, 1, 1, 7, 7, 8, 9, 2, 2, 9, 0, 9, 5, 4, 3, 3, 1, 5, 1, 5, 4,\n",
       "        5, 9, 8, 5], device='cuda:1')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### classifier with class representation\n",
    "torch.argmax(classifier.cls_weight, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
