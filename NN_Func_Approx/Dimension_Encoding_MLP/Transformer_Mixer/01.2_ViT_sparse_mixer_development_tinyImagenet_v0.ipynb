{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6287145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os, sys, pathlib, random, time, pickle, copy, json\n",
    "# from tqdm.autonotebook import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49924ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f4b8961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED = 147\n",
    "# SEED = 258\n",
    "SEED = 369\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0173cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "610ee608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiny_imagenet import TinyImageNetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cdebded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny_train = transforms.Compose([\n",
    "#     transforms.RandomCrop(size=64, padding=4),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(\n",
    "#         mean=[0.5]*3,\n",
    "#         std=[0.2]*3,\n",
    "#     ),\n",
    "# ])\n",
    "\n",
    "tiny_train = transforms.Compose([\n",
    "    transforms.RandAugment(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5]*3,\n",
    "        std=[0.2]*3,\n",
    "    ),\n",
    "])\n",
    "\n",
    "tiny_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5]*3,\n",
    "        std=[0.2]*3,\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42ec6ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyImageNet_Preload(data.Dataset):\n",
    "    \n",
    "    def __init__(self, root, mode='train', transform=None):\n",
    "        \n",
    "        dataset = datasets.ImageFolder(\n",
    "            root=os.path.join(root, mode),\n",
    "            transform=None\n",
    "        )\n",
    "        self.transform = transform\n",
    "        self.images, self.labels = [], []\n",
    "        print(\"Dataset Size:\",len(dataset))\n",
    "        for i in tqdm(range(len(dataset))):\n",
    "            x, y = dataset[i]\n",
    "            self.images.append(x)\n",
    "            self.labels.append(y)\n",
    "        del dataset\n",
    "            \n",
    "    def _add_channels(img, total_channels=3):\n",
    "        while len(img.shape) < 3:  # third axis is the channels\n",
    "            img = np.expand_dims(img, axis=-1)\n",
    "        while(img.shape[-1]) < 3:\n",
    "            img = np.concatenate([img, img[:, :, -1:]], axis=-1)\n",
    "        return img\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, lbl = self.images[idx], self.labels[idx]\n",
    "        return self.transform(img), lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c76d6f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 200000/200000 [00:21<00:00, 9313.34it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TinyImageNet_Preload(root=\"../../../../../_Datasets/tiny-imagenet-200\",\n",
    "                                     mode='train', transform=tiny_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e10d0f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = TinyImageNetDataset(root_dir=\"../../../../../_Datasets/tiny-imagenet-200/\", \n",
    "#                                     mode='train',\n",
    "#                                     transform=tiny_train, \n",
    "#                                     preload=False,\n",
    "#                                     download=False)\n",
    "\n",
    "# train_dataset = datasets.ImageFolder(\n",
    "#         root=os.path.join(\"../../../../../_Datasets/tiny-imagenet-200\", 'train'),\n",
    "#         transform=tiny_train\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fc8dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84e16847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = TinyImageNetDataset(root_dir=\"../../../../../_Datasets/tiny-imagenet-200/\", \n",
    "#                                     mode='val',\n",
    "#                                     transform=tiny_test, \n",
    "#                                     preload=False,\n",
    "#                                     download=False)\n",
    "\n",
    "# test_dataset = datasets.ImageFolder(\n",
    "#         root=os.path.join(\"../../../../../_Datasets/tiny-imagenet-200\", 'val'),\n",
    "#         transform=tiny_train\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e23e7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset.class_to_idx = train_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4fadca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48bba471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset.class_to_idx == train_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32c9260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"../../../../../_Datasets/tiny-imagenet-200/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cee7dbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 10000/10000 [00:01<00:00, 9483.48it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TinyImageNet_Preload(root=\"../../../../../_Datasets/tiny-imagenet-200\",\n",
    "                                     mode='val', transform=tiny_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939aa2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbdc0585",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=256, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45072db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 3, 64, 64]), torch.Size([256]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## demo of train loader\n",
    "xx, yy = iter(train_loader).next()\n",
    "xx.shape, yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb4fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35d48ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10,   9, 106,  24, 153, 161,  50, 122, 164, 181,  71, 138,   4,  48,\n",
       "         31, 134,  79,  77,   2, 120, 100,  18,  29, 166, 108, 108,  33,  52,\n",
       "        127,  90, 158,  44, 159,  69, 194,  96,  92,  64,  70,  62, 162, 141,\n",
       "         39,  20, 159, 153,  42, 170,  42, 103,  88,  74, 108,  51,  34,  99,\n",
       "        136, 172,  18,  64, 137,  21, 146, 102,  10,  62,   6, 114, 167, 149,\n",
       "        199, 175, 104, 143,  93, 135,  59, 186,  26, 180,  65, 138, 190,  86,\n",
       "         36,  59, 143,  49, 163,  10, 161, 108, 160,  13,  20,  56,  60, 117,\n",
       "         19,  27,   0,  16, 133, 180,  67, 102, 143,  64, 115, 102,  88,   1,\n",
       "         33, 121,   2,  31,  75, 191, 197,  43, 141,  46,  28,  81, 184,  19,\n",
       "        111, 188,  26, 153,  69,  34,  14, 189, 113,  17, 149,   4,  92, 110,\n",
       "        127,  40, 134, 125,  96, 139,  16, 112,  28,  32, 175, 164,  93,  35,\n",
       "        156,   9, 173, 162,  25, 114,   2,   2, 161, 197,  59,  38,  40,  70,\n",
       "         80, 191, 133,  48, 164,  45, 165, 140, 194,  22,  87, 159, 174,  43,\n",
       "        170, 148, 139, 131,  60, 157,  42,  90,  56,  66, 167,  56,  96,   3,\n",
       "        117,   3, 174, 159, 136, 193,  49,  97, 104, 164,  44, 116,  77, 121,\n",
       "        101,  99,  70, 148, 169,  72, 109, 151, 158,  54,  52, 177,  60,  21,\n",
       "        104,  84, 115,  50, 133,  62,  78, 116,  63,  49,  42,  18,  40,   5,\n",
       "         20,  49,  52, 133,  51,   8, 151,  73,  91,  76,  65, 172,   9,  92,\n",
       "        107, 110,  71, 145])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2e11b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcaa3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_lib import TransformerBlock, \\\n",
    "        Mixer_TransformerBlock_Encoder, \\\n",
    "        PositionalEncoding, \\\n",
    "        ViT_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b45f6816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixer_ViT_Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_dim:tuple, patch_size:tuple, hidden_expansion:float, num_blocks:int, num_classes:int, pos_emb=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_dim = image_dim ### must contain (C, H, W) or (H, W)\n",
    "        \n",
    "        ### find patch dim\n",
    "        d0 = int(image_dim[-2]/patch_size[0])\n",
    "        d1 = int(image_dim[-1]/patch_size[1])\n",
    "        assert d0*patch_size[0]==image_dim[-2], \"Image must be divisible into patch size\"\n",
    "        assert d1*patch_size[1]==image_dim[-1], \"Image must be divisible into patch size\"\n",
    "#         self.d0, self.d1 = d0, d1 ### number of patches in each axis\n",
    "        __patch_size = patch_size[0]*patch_size[1]*image_dim[0] ## number of channels in each patch\n",
    "    \n",
    "        ### find channel dim\n",
    "        channel_size = d0*d1 ## number of patches\n",
    "        \n",
    "        ### after the number of channels are changed\n",
    "        init_dim = __patch_size\n",
    "        final_dim = int(__patch_size*hidden_expansion/2)*2\n",
    "        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        #### rescale the patches (patch wise image non preserving transform, unlike bilinear interpolation)\n",
    "        self.channel_change = nn.Linear(init_dim, final_dim)\n",
    "        print(f\"ViT Mixer : Channels per patch -> Initial:{init_dim} Final:{final_dim}\")\n",
    "        \n",
    "        \n",
    "        self.channel_dim = final_dim\n",
    "        self.patch_dim = channel_size\n",
    "        \n",
    "        self.transformer_blocks = []\n",
    "        \n",
    "        f = self.get_factors(self.channel_dim)\n",
    "        print(f)\n",
    "        fi = np.abs(np.array(f) - np.sqrt(self.channel_dim)).argmin()\n",
    "        \n",
    "        _n_heads = f[fi]\n",
    "        \n",
    "        ## number of dims per channel -> channel_dim\n",
    "        print('Num patches', self.patch_dim)\n",
    "        print(self.channel_dim, _n_heads)\n",
    "        \n",
    "        ### Find the block size for sequence:\n",
    "        block_seq_size = int(2**np.ceil(np.log2(np.sqrt(self.patch_dim))))\n",
    "        print(f'Mixing with block: {block_seq_size}')\n",
    "        \n",
    "#         block = int(np.ceil(np.sqrt(self.patch_dim)))\n",
    "        for i in range(num_blocks):\n",
    "            L = Mixer_TransformerBlock_Encoder(self.patch_dim, block_seq_size, self.channel_dim, _n_heads, 0, 2)\n",
    "            self.transformer_blocks.append(L)\n",
    "        self.transformer_blocks = nn.Sequential(*self.transformer_blocks)\n",
    "        \n",
    "        self.linear = nn.Linear(self.patch_dim*self.channel_dim, num_classes)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(self.channel_dim, dropout=0)\n",
    "        if not pos_emb:\n",
    "            self.positional_encoding = nn.Identity()\n",
    "        \n",
    "        \n",
    "    def get_factors(self, n):\n",
    "        facts = []\n",
    "        for i in range(2, n+1):\n",
    "            if n%i == 0:\n",
    "                facts.append(i)\n",
    "        return facts\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.unfold(x).swapaxes(-1, -2)\n",
    "        x = self.channel_change(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.linear(x.view(bs, -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0642ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(2**np.ceil(np.log2(np.sqrt(16)))) ## 32/4 * 32/4 = 64 total patches.. mixed at the block of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "297726a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT Mixer : Channes per patch -> Initial:12 Final:28\n",
      "[2, 4, 7, 14, 28]\n",
      "Num patches 1024\n",
      "28 4\n",
      "Mixing with block: 32\n"
     ]
    }
   ],
   "source": [
    "vit_mixer = Mixer_ViT_Classifier((3, 64, 64), patch_size=[4, 4], hidden_expansion=2.4, num_blocks=3, num_classes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0c009dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mixer_ViT_Classifier(\n",
       "  (unfold): Unfold(kernel_size=[2, 2], dilation=1, padding=0, stride=[2, 2])\n",
       "  (channel_change): Linear(in_features=12, out_features=28, bias=True)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): Mixer_TransformerBlock_Encoder(\n",
       "      (sparse_transformers): ModuleList(\n",
       "        (0): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention_Sparse(\n",
       "            (values): Linear(in_features=28, out_features=28, bias=True)\n",
       "            (keys): Linear(in_features=28, out_features=28, bias=True)\n",
       "            (queries): Linear(in_features=28, out_features=28, bias=True)\n",
       "            (fc_out): Linear(in_features=28, out_features=28, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=28, out_features=56, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=56, out_features=28, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (1): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention_Sparse(\n",
       "            (values): Linear(in_features=28, out_features=28, bias=True)\n",
       "            (keys): Linear(in_features=28, out_features=28, bias=True)\n",
       "            (queries): Linear(in_features=28, out_features=28, bias=True)\n",
       "            (fc_out): Linear(in_features=28, out_features=28, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=28, out_features=56, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=56, out_features=28, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Mixer_TransformerBlock_Encoder(\n",
       "      (sparse_transformers): ModuleList(\n",
       "        (0): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention_Sparse(\n",
       "            (values): Linear(in_features=28, out_features=28, bias=True)\n",
       "            (keys): Linear(in_features=28, out_features=28, bias=True)\n",
       "            (queries): Linear(in_features=28, out_features=28, bias=True)\n",
       "            (fc_out): Linear(in_features=28, out_features=28, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=28, out_features=56, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=56, out_features=28, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (1): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention_Sparse(\n",
       "            (values): Linear(in_features=28, out_features=28, bias=True)\n",
       "            (keys): Linear(in_features=28, out_features=28, bias=True)\n",
       "            (queries): Linear(in_features=28, out_features=28, bias=True)\n",
       "            (fc_out): Linear(in_features=28, out_features=28, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=28, out_features=56, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=56, out_features=28, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Mixer_TransformerBlock_Encoder(\n",
       "      (sparse_transformers): ModuleList(\n",
       "        (0): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention_Sparse(\n",
       "            (values): Linear(in_features=28, out_features=28, bias=True)\n",
       "            (keys): Linear(in_features=28, out_features=28, bias=True)\n",
       "            (queries): Linear(in_features=28, out_features=28, bias=True)\n",
       "            (fc_out): Linear(in_features=28, out_features=28, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=28, out_features=56, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=56, out_features=28, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (1): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention_Sparse(\n",
       "            (values): Linear(in_features=28, out_features=28, bias=True)\n",
       "            (keys): Linear(in_features=28, out_features=28, bias=True)\n",
       "            (queries): Linear(in_features=28, out_features=28, bias=True)\n",
       "            (fc_out): Linear(in_features=28, out_features=28, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=28, out_features=56, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=56, out_features=28, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=28672, out_features=200, bias=True)\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70b3d019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_mixer(torch.randn(1, 3, 64, 64)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8987e68c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdasd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43masdasd\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'asdasd' is not defined"
     ]
    }
   ],
   "source": [
    "asdasd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbefc39",
   "metadata": {},
   "source": [
    "#### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31be60c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT Mixer : Channes per patch -> Initial:12 Final:36\n",
      "[2, 3, 4, 6, 9, 12, 18, 36]\n",
      "36 6\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "# model = ViT_Classifier((3, 64, 64), patch_size=[4, 4], hidden_expansion=2.4, num_blocks=8, num_classes=200, pos_emb=True)\n",
    "# model = Mixer_ViT_Classifier((3, 64, 64), patch_size=[4, 4], hidden_expansion=2.4, num_blocks=4, num_classes=200, pos_emb=True)\n",
    "\n",
    "model = ViT_Classifier((3, 64, 64), patch_size=[2, 2], hidden_expansion=3, num_blocks=8, num_classes=200, pos_emb=True)\n",
    "# model = Mixer_ViT_Classifier((3, 64, 64), patch_size=[2, 2], hidden_expansion=3, num_blocks=4, num_classes=200, pos_emb=True)\n",
    "\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e411715d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT_Classifier(\n",
       "  (unfold): Unfold(kernel_size=[2, 2], dilation=1, padding=0, stride=[2, 2])\n",
       "  (channel_change): Linear(in_features=12, out_features=36, bias=True)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (values): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (keys): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (queries): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (fc_out): Linear(in_features=36, out_features=36, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=36, out_features=72, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=72, out_features=36, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (values): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (keys): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (queries): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (fc_out): Linear(in_features=36, out_features=36, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=36, out_features=72, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=72, out_features=36, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (values): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (keys): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (queries): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (fc_out): Linear(in_features=36, out_features=36, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=36, out_features=72, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=72, out_features=36, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (values): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (keys): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (queries): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (fc_out): Linear(in_features=36, out_features=36, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=36, out_features=72, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=72, out_features=36, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (values): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (keys): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (queries): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (fc_out): Linear(in_features=36, out_features=36, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=36, out_features=72, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=72, out_features=36, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (values): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (keys): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (queries): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (fc_out): Linear(in_features=36, out_features=36, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=36, out_features=72, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=72, out_features=36, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (values): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (keys): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (queries): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (fc_out): Linear(in_features=36, out_features=36, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=36, out_features=72, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=72, out_features=36, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (values): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (keys): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (queries): Linear(in_features=36, out_features=36, bias=True)\n",
       "        (fc_out): Linear(in_features=36, out_features=36, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=36, out_features=72, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=72, out_features=36, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=36864, out_features=200, bias=True)\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6166a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer_blocks[0].attention.heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d63481e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer_blocks[0].attention.embed_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "196fe4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  7459580\n"
     ]
    }
   ],
   "source": [
    "print(\"number of params: \", sum(p.numel() for p in model.parameters())) \n",
    "\n",
    "########### 4x4 ###########\n",
    "## ViT   ||  6684362 (0->8) \n",
    "## SMViT ||  6684362 (0->2*4=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b49dd",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5cd35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'vit_mixer_tiny_s{SEED}'\n",
    "# model_name = f'vit_sparse_mixer_tiny_s{SEED}' ## sparse but with 8 layers total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66447d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13f63463",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAT ={'train_stat':[], 'test_stat':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4d9af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Following is copied from \n",
    "### https://github.com/kuangliu/pytorch-cifar/blob/master/main.py\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "    STAT['train_stat'].append((epoch, train_loss/(batch_idx+1), 100.*correct/total)) ### (Epochs, Loss, Acc)\n",
    "    print(f\"[Train] {epoch} Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32895df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = -1\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "    STAT['test_stat'].append((epoch, test_loss/(batch_idx+1), 100.*correct/total)) ### (Epochs, Loss, Acc)\n",
    "    print(f\"[Test] {epoch} Loss: {test_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "    \n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'model': model.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        if not os.path.isdir('models'):\n",
    "            os.mkdir('models')\n",
    "        torch.save(state, f'./models/{model_name}.pth')\n",
    "        best_acc = acc\n",
    "        \n",
    "    with open(f\"./output/{model_name}_data.json\", 'w') as f:\n",
    "        json.dump(STAT, f, indent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "888d57ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "resume = False\n",
    "\n",
    "if resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('./models'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load(f'./models/{model_name}.pth')\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a14a607d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                             | 0/782 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 6.00 GiB (GPU 0; 7.93 GiB total capacity; 6.26 GiB already allocated; 918.50 MiB free; 6.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Train the whole damn thing\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, start_epoch\u001b[38;5;241m+\u001b[39mEPOCHS): \u001b[38;5;66;03m## for 200 epochs\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     test(epoch)\n\u001b[1;32m      6\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     11\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/HDD2/TSuman/Notebooks/_GithubSync/Experimentals/NN_Func_Approx/Dimension_Encoding_MLP/Transformer_Mixer/transformers_lib.py:228\u001b[0m, in \u001b[0;36mViT_Classifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    226\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_change(x)\n\u001b[1;32m    227\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(x)\n\u001b[0;32m--> 228\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(x\u001b[38;5;241m.\u001b[39mview(bs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/HDD2/TSuman/Notebooks/_GithubSync/Experimentals/NN_Func_Approx/Dimension_Encoding_MLP/Transformer_Mixer/transformers_lib.py:125\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, query):\n\u001b[0;32m--> 125\u001b[0m     attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# Add skip connection, run through normalization and finally dropout\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(attention \u001b[38;5;241m+\u001b[39m query))\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/HDD2/TSuman/Notebooks/_GithubSync/Experimentals/NN_Func_Approx/Dimension_Encoding_MLP/Transformer_Mixer/transformers_lib.py:86\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[0;34m(self, values, keys, query, mask)\u001b[0m\n\u001b[1;32m     81\u001b[0m     energy \u001b[38;5;241m=\u001b[39m energy\u001b[38;5;241m.\u001b[39mmasked_fill(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-1e20\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Normalize energy values similarly to seq2seq + attention\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# so that they sum to 1. Also divide by scaling factor for\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# better stability\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m attention \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43menergy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# attention shape: (N, heads, query_len, key_len)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnhql,nlhd->nqhd\u001b[39m\u001b[38;5;124m\"\u001b[39m, [attention, values])\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m     90\u001b[0m     N, query_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim\n\u001b[1;32m     91\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 6.00 GiB (GPU 0; 7.93 GiB total capacity; 6.26 GiB already allocated; 918.50 MiB free; 6.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "### Train the whole damn thing\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+EPOCHS): ## for 200 epochs\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c527661e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "781"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)//256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efc32fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)//256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5bdb348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 3, 64, 64])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9ebe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Speeds\n",
    "\n",
    "####### 4*4 Patch ; total of 256 patches\n",
    "## Sparse (256/16) -> 782/782 [04:55<00:00, 3.37it/s]  || 40/40 [00:06<00:00, 6.65it/s]\n",
    "## Dense (256) ->     782/782 [07:39<00:00, 2.18it/s]  || 40/40 [00:09<00:00, 4.38it/s]\n",
    "\n",
    "####### 2*2 Patch ; total of 1024 patches\n",
    "## Sparse (1024/32) -> 48/782 [00:35<08:57,  1.37it/s]  || \n",
    "## Dense (1024)     -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7803b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f'./models/{model_name}.pth')\n",
    "best_acc = checkpoint['acc']\n",
    "start_epoch = checkpoint['epoch']\n",
    "\n",
    "best_acc, start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847f18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### the expansion is 2.4\n",
    "### 83.69 for 12 layers sparse vit\n",
    "### 82.46 for 12 layers vit\n",
    "### 82.57 for 10 layers vit\n",
    "### 84.84 for 9 = (3*3) layers sparse vit\n",
    "### 82.47 for 9 layers vit\n",
    "### 83.88 for 6 = (2*3) layers sparse vit\n",
    "### 81.36 for 6 layers vit\n",
    "### 81.68 for 3 = (1*3) layers sparse vit\n",
    "### 81.50 for 3 layers vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c6a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55759f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf1676",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./output/{model_name}_data.json\", 'r') as f:\n",
    "    STAT = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a339d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stat = np.array(STAT['train_stat'])\n",
    "test_stat = np.array(STAT['test_stat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_stat[:,1], label='train')\n",
    "plt.plot(test_stat[:,1], label='test')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"./output/plots/{model_name}_loss.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bee5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_stat[:,2], label='train')\n",
    "plt.plot(test_stat[:,2], label='test')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"./output/plots/{model_name}_accs.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a77b859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d174ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Experiments\n",
    "'''\n",
    "1. Sparse Attention\n",
    "2. Sparse MLP\n",
    "3. Sparse Att + MLP\n",
    "\n",
    "Datasets:\n",
    "A. Cifar-10/100 -> 4x4 vs 2x2 vs 1x1 patch\n",
    "B. Tiny-Imagenet -> 16x16 vs 4x4 vs 2x2 patch\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
