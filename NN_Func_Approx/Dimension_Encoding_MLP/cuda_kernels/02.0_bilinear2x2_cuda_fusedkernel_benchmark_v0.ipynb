{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import datasets\n",
    "import random, sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cuda:1\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "test_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"../../../../_Datasets/FMNIST/\", train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.FashionMNIST(root=\"../../../../_Datasets/FMNIST/\", train=False, download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "BS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BS, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BS, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## demo of train loader\n",
    "xx, yy = iter(train_loader).next()\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda -bmm2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bmm2x2_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMM2x2Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "#     @torch.jit.ignore\n",
    "    def forward(ctx, inputs, weights):\n",
    "        outputs = bmm2x2_cuda.forward(inputs, weights)\n",
    "        ctx.save_for_backward(inputs, weights)\n",
    "        return outputs[0]\n",
    "    \n",
    "    @staticmethod\n",
    "#     @torch.jit.ignore\n",
    "    def backward(ctx, grad_output):\n",
    "        inputs, weights = ctx.saved_tensors\n",
    "        del_input, del_weights = bmm2x2_cuda.backward(\n",
    "            inputs, \n",
    "            weights, \n",
    "            grad_output)\n",
    "    \n",
    "        return del_input, del_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairWeight2(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        assert input_dim%2 == 0, \"Input dim must be even number\"\n",
    "        self.weight = torch.eye(2).unsqueeze(0).repeat_interleave(input_dim//2, dim=0)\n",
    "        self.weight = nn.Parameter(self.weight)\n",
    "        self.bmmfunc = BMM2x2Function()\n",
    "        \n",
    "    @torch.jit.ignore\n",
    "    def bmm(self, x, w):\n",
    "        return BMM2x2Function.apply(x, w)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, dim = x.shape[0], x.shape[1]\n",
    "        x = x.view(bs, -1, 2)\n",
    "        x = self.bmm(x, self.weight)\n",
    "        x = x.view(bs, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda - Bilinear2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bilinear2x2_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLinear2x2Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, weights):\n",
    "        outputs = bilinear2x2_cuda.forward(inputs, weights)\n",
    "        ctx.save_for_backward(inputs, weights)\n",
    "        return outputs[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        inputs, weights = ctx.saved_tensors\n",
    "#         del_input, del_weights = bmm2x2_cuda.backward(\n",
    "#             grad_output.contiguous(), \n",
    "#             grad_cell.contiguous(), \n",
    "#             grad_output.contiguous())\n",
    "        del_input, del_weights = bilinear2x2_cuda.backward(\n",
    "            inputs, \n",
    "            weights, \n",
    "            grad_output)\n",
    "    \n",
    "        return del_input, del_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairBilinear2(nn.Module):\n",
    "    def __init__(self, dim, grid_width):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.grid_width = grid_width\n",
    "        \n",
    "        self.num_pairs = self.dim // 2\n",
    "        along_row = torch.linspace(0, 1, self.grid_width).reshape(1, -1).t()\n",
    "        along_col = torch.linspace(0, 1, self.grid_width).reshape(-1, 1).t()\n",
    "        \n",
    "        self.Y = torch.stack([along_row+along_col*0, along_row*0+along_col])\n",
    "        self.Y = torch.repeat_interleave(self.Y.unsqueeze(0), self.num_pairs, dim=0)\n",
    "        self.Y = nn.Parameter(self.Y)\n",
    "        \n",
    "        self.pairW = torch.eye(2).unsqueeze(0).repeat_interleave(self.num_pairs, dim=0)\n",
    "        self.pairW = nn.Parameter(self.pairW)\n",
    "    \n",
    "#     @torch.jit.ignore\n",
    "#     def pairbl2x2(self, x, w):\n",
    "#         return BiLinear2x2Function.apply(x, w)\n",
    "    \n",
    "#     @torch.jit.ignore\n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        \n",
    "############# This block ########################\n",
    "        ### this block is significantly faster\n",
    "    \n",
    "#         x = x.view(bs, -1, 2).transpose(0,1)\n",
    "#         x = torch.bmm(x, self.pairW)\n",
    "#         x = x.transpose(1,0)#.reshape(-1, 2)\n",
    "        \n",
    "############# OR This block ########################\n",
    "        x = x.view(bs, -1, 2)\n",
    "        x = BMM2x2Function.apply(x, self.pairW)\n",
    "####################################################\n",
    "#         x = x.view(bs, -1, 2)\n",
    "        x = BiLinear2x2Function.apply(x, self.Y)\n",
    "        x = x.view(bs, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fused 2x2 Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fused2x2ops_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedBiLinear2x2Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, weights, grids):\n",
    "        outputs, input_buffer = fused2x2ops_cuda.bilinear2x2_forward(inputs, weights, grids)\n",
    "        ctx.save_for_backward(input_buffer, weights, grids)\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_buffer, weights, grids = ctx.saved_tensors\n",
    "#         del_input, del_weights = bmm2x2_cuda.backward(\n",
    "#             grad_output.contiguous(), \n",
    "#             grad_cell.contiguous(), \n",
    "#             grad_output.contiguous())\n",
    "        del_input, del_weights, del_grids = fused2x2ops_cuda.bilinear2x2_backward(\n",
    "            input_buffer, \n",
    "            weights, \n",
    "            grids,\n",
    "            grad_output)\n",
    "    \n",
    "        return del_input, del_weights, del_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fused2x2BiLinear(nn.Module):\n",
    "    def __init__(self, dim, grid_width, num_layers = None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.grid_width = grid_width\n",
    "        \n",
    "        self.num_layers = int(np.ceil(np.log2(self.dim)))\n",
    "        if num_layers is not None:\n",
    "            self.num_layers = num_layers\n",
    "        \n",
    "        \n",
    "        self.num_pairs = self.dim // 2\n",
    "        \n",
    "#         along_row = torch.linspace(0, 1, self.grid_width).reshape(1, -1)\n",
    "#         along_col = torch.linspace(0, 1, self.grid_width).reshape(-1, 1)\n",
    "        \n",
    "        along_row = torch.linspace(0, 1, self.grid_width).reshape(1, -1).t()\n",
    "        along_col = torch.linspace(0, 1, self.grid_width).reshape(-1, 1).t()\n",
    "        \n",
    "        self.Y = torch.stack([along_row+along_col*0, along_row*0+along_col])\n",
    "        \n",
    "        ### repeat same for num_pairs\n",
    "        self.Y = torch.repeat_interleave(self.Y.unsqueeze(0), self.num_pairs, dim=0)\n",
    "        ### repeat same for num_Layers\n",
    "        self.Y = torch.repeat_interleave(self.Y.unsqueeze(0), self.num_layers, dim=0)\n",
    "        \n",
    "        print(self.Y.shape)\n",
    "        self.Y = nn.Parameter(self.Y)\n",
    "        \n",
    "        ### repeat same for num_pairs\n",
    "        self.pairW = torch.eye(2).unsqueeze(0).repeat_interleave(self.num_pairs, dim=0)\n",
    "        ### repeat same for num_Layers\n",
    "        self.pairW = self.pairW.unsqueeze(0).repeat_interleave(self.num_layers, dim=0)\n",
    "        \n",
    "        print(self.pairW.shape)\n",
    "        self.pairW = nn.Parameter(self.pairW)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = FusedBiLinear2x2Function.apply(x, self.pairW, self.Y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 2, 3, 3])\n",
      "torch.Size([1, 8, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "fbl = Fused2x2BiLinear(16, 3, num_layers=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 16).to(device)\n",
    "y = fbl(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1093,  0.7201,  1.9283,  0.7402,  0.1188,  1.2941, -0.0140,  0.6903,\n",
       "         -0.2115, -2.6138,  0.4018,  0.3808, -0.3816,  0.2621, -0.7745, -0.2235],\n",
       "        [-1.7910,  0.0609, -1.1728,  0.5823,  0.0888, -0.0838,  0.7788,  1.9868,\n",
       "          0.1321,  1.8169,  1.0044, -0.9013,  1.3042, -1.5051, -0.0267, -1.0646]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1093,  0.7201,  1.9283,  0.7402,  0.1188,  1.2941, -0.0140,  0.6903,\n",
       "         -0.2115, -2.6138,  0.4018,  0.3808, -0.3816,  0.2621, -0.7745, -0.2235],\n",
       "        [-1.7910,  0.0609, -1.1728,  0.5823,  0.0888, -0.0838,  0.7788,  1.9868,\n",
       "          0.1321,  1.8169,  1.0044, -0.9013,  1.3042, -1.5051, -0.0267, -1.0646]],\n",
       "       device='cuda:0', grad_fn=<FusedBiLinear2x2FunctionBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbl = PairBilinear2(16, 3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbl.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = pbl(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1093,  0.7201,  1.9283,  0.7402,  0.1188,  1.2941, -0.0140,  0.6903,\n",
       "         -0.2115, -2.6138,  0.4018,  0.3808, -0.3816,  0.2621, -0.7745, -0.2235],\n",
       "        [-1.7910,  0.0609, -1.1728,  0.5823,  0.0888, -0.0838,  0.7788,  1.9868,\n",
       "          0.1321,  1.8169,  1.0044, -0.9013,  1.3042, -1.5051, -0.0267, -1.0646]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1093,  0.7201,  1.9283,  0.7402,  0.1188,  1.2941, -0.0140,  0.6903,\n",
       "         -0.2115, -2.6138,  0.4018,  0.3808, -0.3816,  0.2621, -0.7745, -0.2235],\n",
       "        [-1.7910,  0.0609, -1.1728,  0.5823,  0.0888, -0.0838,  0.7788,  1.9868,\n",
       "          0.1321,  1.8169,  1.0044, -0.9013,  1.3042, -1.5051, -0.0267, -1.0646]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0873,  0.0172],\n",
       "         [ 0.0000, -0.0914, -0.0399],\n",
       "         [ 0.0000,  0.0464,  0.0429]],\n",
       "\n",
       "        [[ 0.0000,  0.0873,  0.0172],\n",
       "         [ 0.0000, -0.0914, -0.0399],\n",
       "         [ 0.0000,  0.0464,  0.0429]]], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.Y.grad[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0873,  0.0172],\n",
       "         [ 0.0000, -0.0914, -0.0399],\n",
       "         [ 0.0000,  0.0464,  0.0429]],\n",
       "\n",
       "        [[ 0.0000,  0.0873,  0.0172],\n",
       "         [ 0.0000, -0.0914, -0.0399],\n",
       "         [ 0.0000,  0.0464,  0.0429]]], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbl.Y.grad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fbl.Y.grad[0] - pbl.Y.grad.transpose(-1, -2)\n",
    "fbl.Y.grad[0] - pbl.Y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0236, 0.0236],\n",
       "        [0.0413, 0.0413]], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbl.pairW.grad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0236, 0.0236],\n",
       "        [0.0413, 0.0413]], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.pairW.grad[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbl.pairW[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.pairW[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedPairBilinearBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, grid_width):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_layers = int(np.ceil(np.log2(input_dim)))\n",
    "        \n",
    "        extra =  2**self.num_layers - input_dim\n",
    "        torch.manual_seed(123)\n",
    "        self.selector = torch.randperm(self.input_dim)[:extra]\n",
    "        \n",
    "        self.fused_pair_bilinear = Fused2x2BiLinear(2**self.num_layers, grid_width=grid_width, )#num_layers=5)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x should have dimension -> bs, M\n",
    "        '''\n",
    "        x = torch.cat((x, x[:, self.selector]), dim=1)\n",
    "        return self.fused_pair_bilinear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 2, 3, 3])\n",
      "torch.Size([10, 512, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "fpbl = FusedPairBilinearBlock(784, 3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1024])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = fpbl(torch.randn(10, 784).to(device))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules and Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasLayer(nn.Module):\n",
    "    def __init__(self, dim, init_val=0):\n",
    "        super().__init__()\n",
    "        self.bias = nn.Parameter(torch.ones(dim)*init_val)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x+self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizedPairBilinearSpline(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, grid_width):\n",
    "        super().__init__()\n",
    "        assert input_dim%2 == 0, \"Input dim must be even number\"\n",
    "        self.input_dim = input_dim\n",
    "        self.num_layers = int(np.ceil(np.log2(input_dim)))\n",
    "            \n",
    "        self.facto_nets = []\n",
    "        self.idx_revidx = []\n",
    "        for i in range(self.num_layers):\n",
    "            idrid = self.get_pair(self.input_dim, i+1)\n",
    "            net = PairBilinear2(self.input_dim, grid_width)\n",
    "            self.facto_nets.append(net)\n",
    "            self.idx_revidx.append(idrid)\n",
    "        self.facto_nets = nn.ModuleList(self.facto_nets)\n",
    "            \n",
    "#     @torch.jit.ignore\n",
    "    def get_pair(self, inp_dim, step=1):\n",
    "        dim = 2**int(np.ceil(np.log2(inp_dim)))\n",
    "        assert isinstance(step, int), \"Step must be integer\"\n",
    "\n",
    "        blocks = (2**step)\n",
    "        range_ = dim//blocks\n",
    "        adder_ = torch.arange(0, range_)*blocks\n",
    "\n",
    "        pairs_ = torch.Tensor([0, blocks//2])\n",
    "        repeat_ = torch.arange(0, blocks//2).reshape(-1,1)\n",
    "        block_map = (pairs_+repeat_).reshape(-1)\n",
    "\n",
    "        reorder_for_pair = (block_map+adder_.reshape(-1,1)).reshape(-1)\n",
    "        indx = reorder_for_pair.type(torch.long)\n",
    "        indx = indx[indx<inp_dim]\n",
    "\n",
    "        rev_indx = torch.argsort(indx)\n",
    "        return indx, rev_indx\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## swap first and then forward and reverse-swap\n",
    "        y = x\n",
    "#         for i in range(len(self.facto_nets)):\n",
    "        for i, fn in enumerate(self.facto_nets):\n",
    "            idx, revidx = self.idx_revidx[i]\n",
    "            y = y[:, idx]\n",
    "            y = fn(y) \n",
    "            y = y[:, revidx]\n",
    "#         y = x + y ## this is residual addition... remove if only want feed forward\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairBilinearBlock(FactorizedPairBilinearSpline):\n",
    "    \n",
    "    def __init__(self, input_dim, grid_width):\n",
    "        num_layers = int(np.ceil(np.log2(input_dim)))\n",
    "        extra =  2**num_layers - input_dim\n",
    "        torch.manual_seed(123)\n",
    "        self.selector = torch.randperm(input_dim)[:extra]\n",
    "        \n",
    "        super().__init__(2**num_layers, grid_width)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x should have dimension -> bs, M\n",
    "        '''\n",
    "        x = torch.cat((x, x[:, self.selector]), dim=1)\n",
    "        return super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfL = FactorizedPairBilinearSpline(784, 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5040,  2.9259, -0.2752,  ...,  0.8611, -1.0041,  0.2877],\n",
       "        [ 0.1645, -0.7405, -0.9497,  ...,  0.4982,  0.1717,  0.2237],\n",
       "        [-1.8787, -0.4780, -1.1443,  ...,  1.0046,  0.4900,  0.2338],\n",
       "        ...,\n",
       "        [-1.2427,  0.2971,  0.4793,  ..., -1.9474, -0.2935,  1.4595],\n",
       "        [ 0.4139, -0.8466, -0.0513,  ..., -0.5742, -0.8309,  0.6456],\n",
       "        [-1.3414,  0.4671, -1.7051,  ...,  0.7226,  0.0492,  0.7175]],\n",
       "       device='cuda:0', grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfL(torch.randn(100, 784).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "_a = torch.randn(100, 784).to(device)\n",
    "\n",
    "# %timeit pfL(_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): PairBilinear2()\n",
       "  (1): PairBilinear2()\n",
       "  (2): PairBilinear2()\n",
       "  (3): PairBilinear2()\n",
       "  (4): PairBilinear2()\n",
       "  (5): PairBilinear2()\n",
       "  (6): PairBilinear2()\n",
       "  (7): PairBilinear2()\n",
       "  (8): PairBilinear2()\n",
       "  (9): PairBilinear2()\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfL.facto_nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799680"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_count = sum([torch.numel(p) for p in pfL.parameters()])\n",
    "param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bias = BiasLayer(784)\n",
    "        self.la1 = FactorizedPairBilinearSpline(784, grid_width=3)\n",
    "        self.bn1 = nn.BatchNorm1d(784)\n",
    "        self.fc = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bias(x)\n",
    "        x = self.la1(x)\n",
    "#         x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.bias = nn.Linear(784, 512)\n",
    "#         self.la1 = FusedPairBilinearBlock(512, grid_width=3)\n",
    "#         self.bn1 = nn.BatchNorm1d(512)\n",
    "#         self.fc = nn.Linear(512, 10)\n",
    "\n",
    "        self.bias = BiasLayer(784)\n",
    "        self.la1 = FusedPairBilinearBlock(784, grid_width=3)\n",
    "#         self.la1 = PairBilinearBlock(784, grid_width=3)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.fc = nn.Linear(1024, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bias(x)\n",
    "        x = self.la1(x)\n",
    "#         x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class FactorNet2_debug(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.bias = nn.Linear(784, 512)\n",
    "#         self.la1 = FusedPairBilinearBlock(512, grid_width=3)\n",
    "#         self.bn1 = nn.BatchNorm1d(512)\n",
    "#         self.fc = nn.Linear(512, 10)\n",
    "\n",
    "        self.bias = BiasLayer(784)\n",
    "#         self.la1 = FusedPairBilinearBlock(784, grid_width=3)\n",
    "        self.la1 = PairBilinearBlock(784, grid_width=3)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.fc = nn.Linear(1024, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bias(x)\n",
    "        x = self.la1(x)\n",
    "#         x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinaryNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.la1 = nn.Linear(784, 784, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(784)\n",
    "        self.la2 = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.la1(x))\n",
    "        x = torch.relu(x)\n",
    "        x = self.la2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 2, 3, 3])\n",
      "torch.Size([10, 512, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "125722"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FactorNet2()\n",
    "param_count = sum([torch.numel(p) for p in model.parameters()])\n",
    "param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.la1.fused_pair_bilinear.Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(624074, 4.963920395793894)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OrdinaryNet()\n",
    "param_count1 = sum([torch.numel(p) for p in model.parameters()])\n",
    "param_count1, param_count1/param_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 2, 3, 3])\n",
      "torch.Size([10, 512, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FactorNet2(\n",
       "  (bias): BiasLayer()\n",
       "  (la1): FusedPairBilinearBlock(\n",
       "    (fused_pair_bilinear): Fused2x2BiLinear()\n",
       "  )\n",
       "  (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = FactorNet2().to(device)\n",
    "# model = OrdinaryNet().to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorNet2_debug(\n",
       "  (bias): BiasLayer()\n",
       "  (la1): PairBilinearBlock(\n",
       "    (facto_nets): ModuleList(\n",
       "      (0): PairBilinear2()\n",
       "      (1): PairBilinear2()\n",
       "      (2): PairBilinear2()\n",
       "      (3): PairBilinear2()\n",
       "      (4): PairBilinear2()\n",
       "      (5): PairBilinear2()\n",
       "      (6): PairBilinear2()\n",
       "      (7): PairBilinear2()\n",
       "      (8): PairBilinear2()\n",
       "      (9): PairBilinear2()\n",
       "    )\n",
       "  )\n",
       "  (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model_deb = FactorNet2_debug().to(device)\n",
    "model_deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_deb = torch.optim.Adam(model_deb.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  125722\n"
     ]
    }
   ],
   "source": [
    "print(\"number of params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784])\n",
      "torch.Size([10, 512, 2, 3, 3])\n",
      "torch.Size([10, 512, 2, 2])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([10, 1024])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  125722\n"
     ]
    }
   ],
   "source": [
    "print(\"number of params: \", sum(p.numel() for p in model_deb.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784])\n",
      "torch.Size([512, 2, 3, 3])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 3, 3])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 3, 3])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 3, 3])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 3, 3])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 3, 3])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 3, 3])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 3, 3])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 3, 3])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([512, 2, 3, 3])\n",
      "torch.Size([512, 2, 2])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([10, 1024])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in model_deb.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300 [00:00<01:31,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 2/300 [00:00<01:15,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 1.5234649026751867e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 3/300 [00:00<01:11,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 2.619203041831497e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▏         | 4/300 [00:00<01:08,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 4.2597308492986485e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 5/300 [00:01<01:07,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 7.820215978426859e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 6/300 [00:01<01:06,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 0.00010962785017909482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 7/300 [00:01<01:05,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 0.00013302225852385163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 8/300 [00:01<01:05,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 0.0001575818459969014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 9/300 [00:02<01:04,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 0.000198329784325324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 10/300 [00:02<01:04,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 0.00020950505859218538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▎         | 11/300 [00:02<01:04,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 0.0002351947478018701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 12/300 [00:02<01:03,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 0.000258908374235034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 13/300 [00:02<01:02,  4.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 0.0002811381418723613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▍         | 14/300 [00:03<01:02,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 0.0003019824798684567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 15/300 [00:03<01:02,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 0.0003342173877172172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 16/300 [00:03<01:01,  4.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 0.0003759414830710739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 17/300 [00:03<01:01,  4.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 0.0004014784062746912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 17/300 [00:04<01:07,  4.20it/s]\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/tsuman/Program_Files/Python/miniconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-64-6e0d593283de>\", line 23, in <module>\n",
      "    loss_.backward()\n",
      "  File \"/home/tsuman/Program_Files/Python/miniconda3/lib/python3.7/site-packages/torch/tensor.py\", line 245, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/home/tsuman/Program_Files/Python/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 147, in backward\n",
      "    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tsuman/Program_Files/Python/miniconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tsuman/Program_Files/Python/miniconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/tsuman/Program_Files/Python/miniconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/tsuman/Program_Files/Python/miniconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/tsuman/Program_Files/Python/miniconda3/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/tsuman/Program_Files/Python/miniconda3/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/tsuman/Program_Files/Python/miniconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/tsuman/Program_Files/Python/miniconda3/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/tsuman/Program_Files/Python/miniconda3/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/tsuman/Program_Files/Python/miniconda3/lib/python3.7/posixpath.py\", line 428, in _joinrealpath\n",
      "    newpath = join(path, name)\n",
      "  File \"/home/tsuman/Program_Files/Python/miniconda3/lib/python3.7/posixpath.py\", line 87, in join\n",
      "    if b.startswith(sep):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train_acc = 0\n",
    "    train_count = 0\n",
    "    for xx, yy in tqdm(train_loader):\n",
    "        xx = xx.view(xx.shape[0], -1)\n",
    "        xx, yy = xx.to(device), yy.to(device)\n",
    "\n",
    "        yout = model(xx)\n",
    "        loss = criterion(yout, yy)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        yout_ = model_deb(xx)\n",
    "        loss_ = criterion(yout_, yy)\n",
    "        optimizer_deb.zero_grad()\n",
    "        loss_.backward()\n",
    "        optimizer_deb.step()\n",
    "\n",
    "        losses.append(float(loss))\n",
    "\n",
    "        outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "        correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "        train_acc += correct\n",
    "        train_count += len(outputs)\n",
    "        \n",
    "        ########3 Find Similarities and Differences @@@@@@@@@@\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            diff = yout - yout_\n",
    "            print(f\"Diff: {diff.abs().mean()}\")\n",
    "        \n",
    "#         if torch.any(torch.isnan(model.la1.fused_pair_bilinear.Y)):\n",
    "#             print(f\"NAN Grid back; loss {float(loss)}\", flush=True)\n",
    "#         if torch.any(torch.isnan(model.la1.fused_pair_bilinear.pairW)):\n",
    "#             print(f\"NAN Weight back; loss {float(loss)}\", flush=True)\n",
    "#         sys.stdout.flush()\n",
    "        \n",
    "\n",
    "    train_accs.append(float(train_acc)/train_count*100)\n",
    "    train_acc = 0\n",
    "    train_count = 0\n",
    "\n",
    "    print(f'Epoch: {epoch},  Loss:{float(loss)}')\n",
    "    test_count = 0\n",
    "    test_acc = 0\n",
    "    for xx, yy in tqdm(test_loader):\n",
    "        xx = xx.view(xx.shape[0], -1)\n",
    "        xx, yy = xx.to(device), yy.to(device)\n",
    "        with torch.no_grad():\n",
    "            yout = model(xx)\n",
    "            yout_ = model_deb(xx)\n",
    "        outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "        correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "        test_acc += correct\n",
    "        test_count += len(xx)\n",
    "    test_accs.append(float(test_acc)/test_count*100)\n",
    "    print(f'Train Acc:{train_accs[-1]:.2f}%, Test Acc:{test_accs[-1]:.2f}%')\n",
    "    print()\n",
    "\n",
    "### after each class index is finished training\n",
    "print(f'\\t-> Train Acc {max(train_accs)} ; Test Acc {max(test_accs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512, 2, 3, 3])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.la1.fused_pair_bilinear.Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512, 2, 2])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.la1.fused_pair_bilinear.pairW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[[-4.8426e-03,  3.4685e-03,  0.0000e+00],\n",
       "           [ 5.0352e-01,  4.9969e-01,  5.0000e-01],\n",
       "           [ 1.0000e+00,  1.0000e+00,  1.0000e+00]],\n",
       "\n",
       "          [[-4.3923e-03,  5.0313e-01,  1.0000e+00],\n",
       "           [ 3.1356e-03,  4.9971e-01,  1.0000e+00],\n",
       "           [ 0.0000e+00,  5.0000e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[-2.8686e-03,  1.5614e-03,  1.5437e-03],\n",
       "           [ 4.9823e-01,  5.0178e-01,  5.0149e-01],\n",
       "           [ 1.0000e+00,  1.0000e+00,  1.0000e+00]],\n",
       "\n",
       "          [[ 6.8547e-04,  4.9926e-01,  9.9846e-01],\n",
       "           [ 3.2321e-03,  4.9949e-01,  9.9851e-01],\n",
       "           [ 0.0000e+00,  5.0000e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[-2.6297e-03, -8.4663e-04, -1.7677e-03],\n",
       "           [ 4.9717e-01,  4.9811e-01,  4.9824e-01],\n",
       "           [ 9.9889e-01,  9.9809e-01,  9.9864e-01]],\n",
       "\n",
       "          [[-3.3490e-03,  4.9816e-01,  9.9825e-01],\n",
       "           [-1.9572e-03,  4.9808e-01,  9.9826e-01],\n",
       "           [-1.1171e-03,  4.9805e-01,  9.9863e-01]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[ 4.2386e-03, -3.8866e-03, -4.6386e-03],\n",
       "           [ 4.9547e-01,  4.9575e-01,  4.9645e-01],\n",
       "           [ 9.9688e-01,  9.9760e-01,  1.0006e+00]],\n",
       "\n",
       "          [[ 4.1378e-04,  5.0529e-01,  1.0050e+00],\n",
       "           [-5.0358e-03,  4.9490e-01,  9.9735e-01],\n",
       "           [-3.3565e-03,  4.9523e-01,  9.9598e-01]]],\n",
       "\n",
       "\n",
       "         [[[-4.6123e-03, -4.2883e-03, -2.6082e-03],\n",
       "           [ 5.0341e-01,  4.9501e-01,  4.9544e-01],\n",
       "           [ 1.0054e+00,  9.9528e-01,  9.9577e-01]],\n",
       "\n",
       "          [[-2.4032e-03,  4.9506e-01,  9.9552e-01],\n",
       "           [-7.8934e-04,  4.9991e-01,  9.9610e-01],\n",
       "           [-1.1379e-03,  4.9995e-01,  9.9686e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 5.1361e-05,  6.5930e-04,  5.3716e-04],\n",
       "           [ 5.0542e-01,  5.0479e-01,  5.0327e-01],\n",
       "           [ 1.0055e+00,  1.0047e+00,  1.0034e+00]],\n",
       "\n",
       "          [[ 3.3198e-03,  4.9905e-01,  9.9851e-01],\n",
       "           [ 8.3640e-04,  4.9526e-01,  9.9695e-01],\n",
       "           [-4.3325e-03,  4.9547e-01,  9.9695e-01]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[-4.8282e-03,  3.3929e-03,  0.0000e+00],\n",
       "           [ 5.0374e-01,  4.9933e-01,  5.0000e-01],\n",
       "           [ 1.0000e+00,  1.0000e+00,  1.0000e+00]],\n",
       "\n",
       "          [[-2.8359e-03,  4.9912e-01,  1.0000e+00],\n",
       "           [ 3.7557e-03,  4.9953e-01,  1.0000e+00],\n",
       "           [ 0.0000e+00,  5.0000e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[-4.4704e-03, -2.5115e-03,  0.0000e+00],\n",
       "           [ 5.0338e-01,  5.0034e-01,  5.0000e-01],\n",
       "           [ 1.0000e+00,  1.0000e+00,  1.0000e+00]],\n",
       "\n",
       "          [[ 7.0486e-04,  4.9927e-01,  9.9846e-01],\n",
       "           [-2.0982e-03,  4.9851e-01,  1.0000e+00],\n",
       "           [ 0.0000e+00,  5.0000e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[-2.7746e-03, -3.5116e-04, -1.6863e-03],\n",
       "           [ 4.9747e-01,  4.9815e-01,  4.9826e-01],\n",
       "           [ 9.9826e-01,  9.9823e-01,  1.0000e+00]],\n",
       "\n",
       "          [[-3.3302e-03,  4.9859e-01,  1.0018e+00],\n",
       "           [ 6.5413e-05,  4.9937e-01,  1.0017e+00],\n",
       "           [ 8.4086e-04,  4.9948e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[-1.2914e-03,  4.4127e-03, -2.6047e-03],\n",
       "           [ 4.9817e-01,  5.0504e-01,  5.0462e-01],\n",
       "           [ 9.9503e-01,  1.0046e+00,  1.0043e+00]],\n",
       "\n",
       "          [[ 1.1711e-04,  5.0294e-01,  1.0043e+00],\n",
       "           [ 1.8307e-03,  5.0344e-01,  1.0044e+00],\n",
       "           [-2.8622e-03,  5.0325e-01,  1.0026e+00]]],\n",
       "\n",
       "\n",
       "         [[[-3.0754e-03, -3.9326e-03, -4.0079e-03],\n",
       "           [ 5.0455e-01,  4.9891e-01,  4.9528e-01],\n",
       "           [ 1.0054e+00,  9.9625e-01,  9.9528e-01]],\n",
       "\n",
       "          [[ 3.8762e-03,  5.0523e-01,  1.0052e+00],\n",
       "           [-4.4542e-03,  5.0528e-01,  1.0055e+00],\n",
       "           [-4.5120e-03,  5.0539e-01,  1.0054e+00]]],\n",
       "\n",
       "\n",
       "         [[[-2.2870e-03, -2.5436e-03, -2.8638e-03],\n",
       "           [ 4.9608e-01,  4.9872e-01,  4.9935e-01],\n",
       "           [ 9.9537e-01,  9.9926e-01,  9.9918e-01]],\n",
       "\n",
       "          [[ 2.6641e-03,  4.9569e-01,  9.9666e-01],\n",
       "           [-2.2307e-04,  4.9969e-01,  1.0004e+00],\n",
       "           [-1.6425e-03,  5.0040e-01,  9.9915e-01]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[-4.8267e-03,  3.1330e-04, -8.2327e-04],\n",
       "           [ 5.0384e-01,  4.9939e-01,  5.0001e-01],\n",
       "           [ 1.0000e+00,  1.0000e+00,  1.0000e+00]],\n",
       "\n",
       "          [[-2.8775e-03,  4.9722e-01,  9.9804e-01],\n",
       "           [ 1.8469e-03,  5.0211e-01,  1.0005e+00],\n",
       "           [ 0.0000e+00,  5.0000e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[-4.4069e-03, -1.3724e-03, -1.3338e-03],\n",
       "           [ 5.0347e-01,  5.0033e-01,  5.0018e-01],\n",
       "           [ 1.0000e+00,  1.0000e+00,  1.0000e+00]],\n",
       "\n",
       "          [[-3.4332e-03,  4.9763e-01,  9.9809e-01],\n",
       "           [ 3.3239e-03,  5.0240e-01,  1.0008e+00],\n",
       "           [ 0.0000e+00,  5.0000e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[-2.9492e-03,  2.1014e-03,  1.4427e-03],\n",
       "           [ 4.9900e-01,  5.0234e-01,  5.0129e-01],\n",
       "           [ 1.0000e+00,  1.0000e+00,  1.0000e+00]],\n",
       "\n",
       "          [[-3.4039e-03,  4.9839e-01,  1.0018e+00],\n",
       "           [ 3.1975e-03,  5.0314e-01,  1.0016e+00],\n",
       "           [ 0.0000e+00,  5.0000e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[-7.9362e-04,  4.2352e-03,  3.4575e-03],\n",
       "           [ 5.0480e-01,  5.0353e-01,  5.0352e-01],\n",
       "           [ 1.0015e+00,  1.0035e+00,  1.0029e+00]],\n",
       "\n",
       "          [[-3.4643e-03,  4.9509e-01,  9.9525e-01],\n",
       "           [-1.6564e-03,  5.0266e-01,  9.9616e-01],\n",
       "           [-3.1502e-04,  5.0417e-01,  9.9866e-01]]],\n",
       "\n",
       "\n",
       "         [[[-1.7063e-05,  2.0567e-03,  5.2569e-03],\n",
       "           [ 4.9528e-01,  4.9773e-01,  5.0053e-01],\n",
       "           [ 9.9591e-01,  1.0045e+00,  1.0042e+00]],\n",
       "\n",
       "          [[ 1.7054e-03,  5.0539e-01,  1.0054e+00],\n",
       "           [-4.2213e-03,  5.0384e-01,  1.0049e+00],\n",
       "           [-3.9344e-03,  5.0502e-01,  1.0050e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 1.8876e-05,  7.6979e-04, -1.1485e-04],\n",
       "           [ 5.0466e-01,  5.0409e-01,  5.0292e-01],\n",
       "           [ 1.0034e+00,  1.0047e+00,  1.0031e+00]],\n",
       "\n",
       "          [[-3.4069e-03,  4.9732e-01,  9.9843e-01],\n",
       "           [ 4.2205e-03,  4.9623e-01,  9.9644e-01],\n",
       "           [ 3.5411e-03,  4.9609e-01,  9.9666e-01]]]],\n",
       "\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "\n",
       "        [[[[-3.2279e-03, -2.3717e-03,  8.9603e-04],\n",
       "           [ 5.0352e-01,  5.0141e-01,  4.9833e-01],\n",
       "           [ 1.0000e+00,  1.0000e+00,  1.0000e+00]],\n",
       "\n",
       "          [[-3.9453e-03,  4.9652e-01,  9.9965e-01],\n",
       "           [ 3.5789e-03,  5.0286e-01,  9.9853e-01],\n",
       "           [ 0.0000e+00,  5.0000e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 2.6053e-03, -3.9884e-03, -3.8130e-03],\n",
       "           [ 4.9835e-01,  5.0295e-01,  5.0271e-01],\n",
       "           [ 1.0000e+00,  1.0000e+00,  1.0000e+00]],\n",
       "\n",
       "          [[ 2.2614e-03,  4.9546e-01,  9.9530e-01],\n",
       "           [-2.4369e-03,  5.0393e-01,  1.0043e+00],\n",
       "           [ 0.0000e+00,  5.0000e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[-7.3908e-04, -2.0059e-03, -2.3980e-03],\n",
       "           [ 5.0205e-01,  4.9991e-01,  4.9822e-01],\n",
       "           [ 1.0000e+00,  1.0000e+00,  1.0000e+00]],\n",
       "\n",
       "          [[ 4.9178e-04,  5.0462e-01,  1.0049e+00],\n",
       "           [-1.0109e-03,  4.9539e-01,  9.9513e-01],\n",
       "           [ 0.0000e+00,  5.0000e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[-2.5319e-03,  3.9300e-03, -2.0121e-03],\n",
       "           [ 4.9939e-01,  5.0301e-01,  4.9694e-01],\n",
       "           [ 1.0010e+00,  1.0019e+00,  1.0006e+00]],\n",
       "\n",
       "          [[-2.2898e-03,  4.9743e-01,  9.9717e-01],\n",
       "           [-2.6318e-03,  4.9603e-01,  9.9579e-01],\n",
       "           [-2.0985e-04,  4.9701e-01,  9.9586e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 2.3693e-03, -4.4508e-03, -3.6835e-03],\n",
       "           [ 5.0391e-01,  4.9674e-01,  4.9708e-01],\n",
       "           [ 1.0050e+00,  1.0046e+00,  1.0039e+00]],\n",
       "\n",
       "          [[-4.3490e-03,  5.0425e-01,  1.0035e+00],\n",
       "           [ 5.1924e-03,  5.0534e-01,  1.0054e+00],\n",
       "           [ 5.2707e-03,  5.0543e-01,  1.0054e+00]]],\n",
       "\n",
       "\n",
       "         [[[-2.0487e-03, -6.4693e-04, -1.5557e-03],\n",
       "           [ 5.0429e-01,  5.0051e-01,  5.0009e-01],\n",
       "           [ 1.0048e+00,  1.0003e+00,  9.9988e-01]],\n",
       "\n",
       "          [[-3.3757e-03,  4.9672e-01,  9.9744e-01],\n",
       "           [ 4.1035e-03,  4.9944e-01,  9.9765e-01],\n",
       "           [ 1.4114e-03,  4.9865e-01,  9.9743e-01]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 2.3763e-03, -4.8572e-03, -4.7948e-03],\n",
       "           [ 4.9935e-01,  5.0350e-01,  5.0311e-01],\n",
       "           [ 1.0000e+00,  1.0000e+00,  1.0000e+00]],\n",
       "\n",
       "          [[ 4.1059e-03,  4.9590e-01,  9.9550e-01],\n",
       "           [-4.1349e-03,  5.0322e-01,  1.0038e+00],\n",
       "           [ 0.0000e+00,  5.0000e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[-4.1141e-03,  1.6469e-03, -1.2484e-03],\n",
       "           [ 5.0325e-01,  4.9909e-01,  5.0057e-01],\n",
       "           [ 1.0000e+00,  1.0000e+00,  1.0000e+00]],\n",
       "\n",
       "          [[-1.7826e-03,  5.0382e-01,  1.0037e+00],\n",
       "           [ 1.4069e-03,  4.9672e-01,  9.9700e-01],\n",
       "           [ 0.0000e+00,  5.0000e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[-9.3823e-04, -1.9083e-03, -2.2815e-03],\n",
       "           [ 5.0124e-01,  5.0063e-01,  4.9852e-01],\n",
       "           [ 1.0000e+00,  1.0000e+00,  1.0000e+00]],\n",
       "\n",
       "          [[ 4.7004e-03,  4.9786e-01,  9.9748e-01],\n",
       "           [ 1.1874e-03,  5.0197e-01,  1.0007e+00],\n",
       "           [ 0.0000e+00,  5.0000e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[-2.3014e-03,  5.1622e-03,  4.6360e-03],\n",
       "           [ 5.0480e-01,  5.0429e-01,  5.0360e-01],\n",
       "           [ 1.0050e+00,  1.0034e+00,  1.0029e+00]],\n",
       "\n",
       "          [[-3.3665e-03,  4.9515e-01,  9.9527e-01],\n",
       "           [ 1.0835e-03,  5.0357e-01,  9.9718e-01],\n",
       "           [-1.1799e-03,  5.0288e-01,  9.9997e-01]]],\n",
       "\n",
       "\n",
       "         [[[-3.1970e-03,  1.2772e-03,  1.9418e-04],\n",
       "           [ 4.9733e-01,  5.0476e-01,  5.0514e-01],\n",
       "           [ 9.9663e-01,  1.0040e+00,  1.0031e+00]],\n",
       "\n",
       "          [[ 3.5238e-03,  5.0537e-01,  1.0055e+00],\n",
       "           [-4.5031e-03,  5.0050e-01,  1.0042e+00],\n",
       "           [-3.1864e-03,  5.0256e-01,  1.0038e+00]]],\n",
       "\n",
       "\n",
       "         [[[-4.0486e-03,  1.4691e-03,  2.4586e-03],\n",
       "           [ 4.9620e-01,  4.9868e-01,  5.0081e-01],\n",
       "           [ 9.9761e-01,  9.9909e-01,  9.9950e-01]],\n",
       "\n",
       "          [[ 1.3461e-03,  4.9766e-01,  9.9706e-01],\n",
       "           [ 2.7057e-03,  4.9860e-01,  9.9849e-01],\n",
       "           [ 1.1704e-04,  4.9825e-01,  9.9888e-01]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 5.0000e-01,  5.0000e-01,  5.0000e-01],\n",
       "           [ 1.0000e+00,  1.0000e+00,  1.0000e+00]],\n",
       "\n",
       "          [[ 3.9343e-03,  5.0025e-01,  9.9876e-01],\n",
       "           [-3.3021e-03,  4.9641e-01,  9.9721e-01],\n",
       "           [ 0.0000e+00,  5.0000e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 5.0000e-01,  5.0000e-01,  5.0000e-01],\n",
       "           [ 1.0000e+00,  1.0000e+00,  1.0000e+00]],\n",
       "\n",
       "          [[ 3.6873e-03,  4.9742e-01,  9.9646e-01],\n",
       "           [-2.5048e-03,  5.0246e-01,  1.0027e+00],\n",
       "           [ 0.0000e+00,  5.0000e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[-1.3935e-03, -1.8610e-03, -2.2588e-03],\n",
       "           [ 5.0012e-01,  5.0103e-01,  4.9819e-01],\n",
       "           [ 1.0000e+00,  1.0000e+00,  1.0000e+00]],\n",
       "\n",
       "          [[-3.1705e-03,  5.0167e-01,  1.0032e+00],\n",
       "           [ 2.7706e-03,  5.0221e-01,  1.0004e+00],\n",
       "           [ 0.0000e+00,  5.0000e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[-4.8064e-03, -2.0702e-03, -1.3203e-03],\n",
       "           [ 4.9733e-01,  4.9966e-01,  4.9941e-01],\n",
       "           [ 1.0046e+00,  9.9702e-01,  1.0012e+00]],\n",
       "\n",
       "          [[ 1.7981e-03,  5.0454e-01,  9.9971e-01],\n",
       "           [-3.5939e-03,  4.9500e-01,  9.9542e-01],\n",
       "           [-2.1732e-03,  4.9475e-01,  9.9539e-01]]],\n",
       "\n",
       "\n",
       "         [[[-3.4956e-03, -5.2288e-03, -4.6251e-03],\n",
       "           [ 4.9680e-01,  4.9603e-01,  5.0148e-01],\n",
       "           [ 9.9733e-01,  9.9809e-01,  1.0034e+00]],\n",
       "\n",
       "          [[-4.0827e-03,  5.0522e-01,  1.0053e+00],\n",
       "           [ 3.6087e-03,  5.0513e-01,  1.0054e+00],\n",
       "           [ 4.8667e-03,  5.0509e-01,  1.0054e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 3.5705e-03, -4.3742e-04, -1.0214e-03],\n",
       "           [ 4.9768e-01,  4.9713e-01,  4.9790e-01],\n",
       "           [ 9.9606e-01,  9.9730e-01,  9.9791e-01]],\n",
       "\n",
       "          [[ 2.5847e-03,  4.9754e-01,  9.9653e-01],\n",
       "           [ 2.1223e-03,  4.9868e-01,  9.9788e-01],\n",
       "           [-2.8505e-03,  4.9840e-01,  9.9795e-01]]]]], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.la1.fused_pair_bilinear.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 1.0032e+00,  2.8594e-03],\n",
       "          [ 3.1364e-03,  1.0028e+00]],\n",
       "\n",
       "         [[ 9.9887e-01,  1.4349e-03],\n",
       "          [ 1.5738e-03,  9.9914e-01]],\n",
       "\n",
       "         [[ 9.9724e-01, -2.1724e-03],\n",
       "          [-2.2809e-03,  9.9759e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 9.9701e-01, -4.8337e-03],\n",
       "          [-3.4174e-03,  1.0047e+00]],\n",
       "\n",
       "         [[ 1.0052e+00, -3.2165e-03],\n",
       "          [-4.8172e-03,  9.9543e-01]],\n",
       "\n",
       "         [[ 1.0055e+00, -2.7365e-03],\n",
       "          [ 4.5138e-03,  9.9565e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0035e+00,  3.5056e-03],\n",
       "          [ 3.1649e-03,  9.9912e-01]],\n",
       "\n",
       "         [[ 1.0031e+00, -2.1072e-03],\n",
       "          [-2.2906e-03,  9.9911e-01]],\n",
       "\n",
       "         [[ 9.9725e-01, -3.8963e-04],\n",
       "          [-1.4932e-03,  9.9862e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0037e+00,  2.3437e-03],\n",
       "          [ 4.7492e-03,  1.0047e+00]],\n",
       "\n",
       "         [[ 1.0052e+00,  2.2877e-03],\n",
       "          [-4.9567e-03,  1.0055e+00]],\n",
       "\n",
       "         [[ 9.9543e-01, -1.2543e-03],\n",
       "          [-2.2327e-03,  9.9568e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0037e+00,  2.3644e-03],\n",
       "          [-1.0591e-04,  9.9726e-01]],\n",
       "\n",
       "         [[ 1.0033e+00,  3.6232e-03],\n",
       "          [-2.1400e-03,  9.9762e-01]],\n",
       "\n",
       "         [[ 9.9939e-01,  3.4044e-03],\n",
       "          [ 2.0903e-03,  9.9863e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0037e+00, -1.1678e-03],\n",
       "          [ 4.1691e-03,  9.9543e-01]],\n",
       "\n",
       "         [[ 9.9702e-01,  3.0061e-03],\n",
       "          [ 4.5997e-03,  1.0055e+00]],\n",
       "\n",
       "         [[ 1.0047e+00,  3.2319e-03],\n",
       "          [ 4.1976e-03,  9.9569e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 1.0037e+00,  1.9341e-03],\n",
       "          [ 1.1568e-05,  9.9838e-01]],\n",
       "\n",
       "         [[ 1.0035e+00,  3.1947e-03],\n",
       "          [-4.7196e-03,  9.9535e-01]],\n",
       "\n",
       "         [[ 9.9962e-01, -4.9184e-03],\n",
       "          [-2.9465e-03,  1.0050e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0009e+00, -4.1507e-03],\n",
       "          [ 1.5999e-03,  9.9542e-01]],\n",
       "\n",
       "         [[ 1.0047e+00,  5.5237e-03],\n",
       "          [-1.8730e-03,  1.0055e+00]],\n",
       "\n",
       "         [[ 1.0047e+00,  2.6227e-03],\n",
       "          [-1.4788e-06,  9.9624e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0037e+00, -2.9228e-03],\n",
       "          [-4.7689e-03,  9.9557e-01]],\n",
       "\n",
       "         [[ 1.0026e+00, -3.4826e-03],\n",
       "          [-2.4983e-03,  1.0041e+00]],\n",
       "\n",
       "         [[ 9.9918e-01,  1.6616e-03],\n",
       "          [-2.4130e-03,  9.9743e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0050e+00,  1.5413e-03],\n",
       "          [ 5.2165e-03,  9.9540e-01]],\n",
       "\n",
       "         [[ 9.9810e-01, -3.7441e-03],\n",
       "          [ 4.7096e-03,  1.0055e+00]],\n",
       "\n",
       "         [[ 9.9656e-01,  1.2358e-03],\n",
       "          [-3.8569e-07,  9.9636e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0025e+00, -3.6090e-03],\n",
       "          [ 2.3835e-03,  9.9888e-01]],\n",
       "\n",
       "         [[ 1.0023e+00,  1.8852e-03],\n",
       "          [-2.8382e-03,  9.9651e-01]],\n",
       "\n",
       "         [[ 9.9924e-01,  1.7678e-03],\n",
       "          [-2.3472e-03,  1.0029e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0041e+00, -4.9087e-03],\n",
       "          [-2.1814e-03,  9.9540e-01]],\n",
       "\n",
       "         [[ 9.9787e-01,  5.4964e-03],\n",
       "          [-6.7313e-04,  1.0055e+00]],\n",
       "\n",
       "         [[ 9.9650e-01, -2.5521e-03],\n",
       "          [-3.9416e-03,  9.9638e-01]]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.la1.fused_pair_bilinear.pairW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.any(torch.isnan(model.la1.fused_pair_bilinear.Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe two models with same computation differ in gradients..\\nBelow code analyses models for the differences in the output.\\n\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The two models with same computation differ in gradients..\n",
    "Below code analyses models for the differences in the output.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 2, 3, 3])\n",
      "torch.Size([10, 512, 2, 2])\n",
      "number of params:  125722\n",
      "number of params:  125722\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = FactorNet2().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.03)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model_deb = FactorNet2_debug().to(device)\n",
    "optimizer_deb = torch.optim.Adam(model_deb.parameters(), lr=0.03)\n",
    "\n",
    "print(\"number of params: \", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "print(\"number of params: \", sum(p.numel() for p in model_deb.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 2, 2])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.la1.fused_pair_bilinear.pairW[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 2, 2])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_deb.la1.facto_nets[0].pairW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.5000],\n",
       "         [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.0000, 0.5000, 1.0000],\n",
       "         [0.0000, 0.5000, 1.0000],\n",
       "         [0.0000, 0.5000, 1.0000]]], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.la1.fused_pair_bilinear.Y[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.5000],\n",
       "         [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.0000, 0.5000, 1.0000],\n",
       "         [0.0000, 0.5000, 1.0000],\n",
       "         [0.0000, 0.5000, 1.0000]]], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_deb.la1.facto_nets[0].Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for xx, yy in tqdm(train_loader):\n",
    "    xx = xx.view(xx.shape[0], -1)\n",
    "    xx, yy = xx.to(device), yy.to(device)\n",
    "\n",
    "    yout = model(xx)\n",
    "    loss = criterion(yout, yy)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "    yout_ = model_deb(xx)\n",
    "    loss_ = criterion(yout_, yy)\n",
    "    optimizer_deb.zero_grad()\n",
    "    loss_.backward()\n",
    "#     optimizer_deb.step()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 0.06269387900829315\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    diff = yout - yout_\n",
    "    print(f\"Diff: {diff.abs().mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-8.0895e-04, -3.5383e-04, -3.9412e-04],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[ 2.0205e-05,  2.1356e-03,  2.1439e-03],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]], device='cuda:0')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.la1.fused_pair_bilinear.Y.grad[7][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.9456e-03, -1.0282e-04, -1.9194e-07],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[-6.9390e-05,  1.4888e-03, -3.9113e-06],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]], device='cuda:0')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_deb.la1.facto_nets[7].Y.grad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y grad diff 0.001012712367810309\n",
      "W grad diff 0.0014948141761124134\n",
      "Y diff 0.0\n",
      "W diff 0.0\n",
      "Y grad diff 0.001003076322376728\n",
      "W grad diff 0.00146401091478765\n",
      "Y diff 0.0\n",
      "W diff 0.0\n",
      "Y grad diff 0.0010161908576264977\n",
      "W grad diff 0.0014503607526421547\n",
      "Y diff 0.0\n",
      "W diff 0.0\n",
      "Y grad diff 0.001030481536872685\n",
      "W grad diff 0.0014273985289037228\n",
      "Y diff 0.0\n",
      "W diff 0.0\n",
      "Y grad diff 0.0010648692259564996\n",
      "W grad diff 0.0014739538310095668\n",
      "Y diff 0.0\n",
      "W diff 0.0\n",
      "Y grad diff 0.0012608177494257689\n",
      "W grad diff 0.0019115135073661804\n",
      "Y diff 0.0\n",
      "W diff 0.0\n",
      "Y grad diff 0.001483541214838624\n",
      "W grad diff 0.0022428082302212715\n",
      "Y diff 0.0\n",
      "W diff 0.0\n",
      "Y grad diff 0.0017222966998815536\n",
      "W grad diff 0.00259769125841558\n",
      "Y diff 0.0\n",
      "W diff 0.0\n",
      "Y grad diff 0.0021347692236304283\n",
      "W grad diff 0.0028894953429698944\n",
      "Y diff 0.0\n",
      "W diff 0.0\n",
      "Y grad diff 0.002208049176260829\n",
      "W grad diff 0.0030084566678851843\n",
      "Y diff 0.0\n",
      "W diff 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    diff = model.la1.fused_pair_bilinear.Y.grad[i] - model_deb.la1.facto_nets[i].Y.grad\n",
    "    print(f\"Y grad diff {torch.abs(diff).mean()}\")\n",
    "    \n",
    "    diff = model.la1.fused_pair_bilinear.pairW.grad[i] - model_deb.la1.facto_nets[i].pairW.grad\n",
    "    print(f\"W grad diff {torch.abs(diff).mean()}\")\n",
    "    \n",
    "    diff = model.la1.fused_pair_bilinear.Y[i] - model_deb.la1.facto_nets[i].Y\n",
    "    print(f\"Y diff {torch.abs(diff).mean()}\")\n",
    "    \n",
    "    diff = model.la1.fused_pair_bilinear.pairW[i] - model_deb.la1.facto_nets[i].pairW\n",
    "    print(f\"W diff {torch.abs(diff).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.5000],\n",
       "         [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.0000, 0.5000, 1.0000],\n",
       "         [0.0000, 0.5000, 1.0000],\n",
       "         [0.0000, 0.5000, 1.0000]]], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.la1.fused_pair_bilinear.Y[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.5000],\n",
       "         [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.0000, 0.5000, 1.0000],\n",
       "         [0.0000, 0.5000, 1.0000],\n",
       "         [0.0000, 0.5000, 1.0000]]], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_deb.la1.facto_nets[0].Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
