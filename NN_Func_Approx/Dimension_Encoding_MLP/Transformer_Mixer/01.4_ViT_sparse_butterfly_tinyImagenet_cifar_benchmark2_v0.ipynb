{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6287145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsuman/All_Files/Program_Files/miniconda3/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os, sys, pathlib, random, time, pickle, copy, json\n",
    "# from tqdm.autonotebook import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0173cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcaa3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_lib import TransformerBlock, \\\n",
    "        Mixer_TransformerBlock_Encoder, \\\n",
    "        PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "387412b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyImageNet_Preload(data.Dataset):\n",
    "#     https://gist.github.com/z-a-f/b862013c0dc2b540cf96a123a6766e54\n",
    "    \n",
    "    def __init__(self, root, mode='train', transform=None, preload=False):\n",
    "        super().__init__()\n",
    "        self.preload = preload\n",
    "        dataset = datasets.ImageFolder(\n",
    "            root=os.path.join(root, mode),\n",
    "            transform=None\n",
    "        )\n",
    "        self.transform = transform\n",
    "        self.images, self.labels = [], []\n",
    "        print(\"Dataset Size:\",len(dataset))\n",
    "        \n",
    "        if preload:\n",
    "            for i in tqdm(range(len(dataset))):\n",
    "                x, y = dataset[i]\n",
    "                self.images.append(x)\n",
    "                self.labels.append(y)\n",
    "                \n",
    "#         del dataset\n",
    "        self.dataset = dataset\n",
    "            \n",
    "    def _add_channels(img, total_channels=3):\n",
    "        while len(img.shape) < 3:  # third axis is the channels\n",
    "            img = np.expand_dims(img, axis=-1)\n",
    "        while(img.shape[-1]) < 3:\n",
    "            img = np.concatenate([img, img[:, :, -1:]], axis=-1)\n",
    "        return img\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.preload:\n",
    "            img, lbl = self.images[idx], self.labels[idx]\n",
    "        else:\n",
    "            img, lbl = self.dataset[idx]\n",
    "        return self.transform(img), lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2e11b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b45f6816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixer_ViT_Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_dim:tuple, patch_size:tuple, hidden_channel:int, num_blocks:int, num_classes:int, block_seq_size:int, block_mlp_size:int, forward_expansion:float=2.0, pos_emb=True, dropout:float=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_dim = image_dim ### must contain (C, H, W) or (H, W)\n",
    "        \n",
    "        ### find patch dim\n",
    "        d0 = int(image_dim[-2]/patch_size[0])\n",
    "        d1 = int(image_dim[-1]/patch_size[1])\n",
    "        assert d0*patch_size[0]==image_dim[-2], \"Image must be divisible into patch size\"\n",
    "        assert d1*patch_size[1]==image_dim[-1], \"Image must be divisible into patch size\"\n",
    "        \n",
    "#         self.d0, self.d1 = d0, d1 ### number of patches in each axis\n",
    "        __patch_size = patch_size[0]*patch_size[1]*image_dim[0] ## number of channels in each patch\n",
    "    \n",
    "        ### find channel dim\n",
    "        channel_size = d0*d1 ## number of patches\n",
    "        \n",
    "        ### after the number of channels are changed\n",
    "        init_dim = __patch_size\n",
    "        final_dim = hidden_channel\n",
    "        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        #### rescale the patches (patch wise image non preserving transform, unlike bilinear interpolation)\n",
    "        self.channel_change = nn.Linear(init_dim, final_dim)\n",
    "        print(f\"ViT Mixer : Channels per patch -> Initial:{init_dim} Final:{final_dim}\")\n",
    "        \n",
    "        \n",
    "        self.channel_dim = final_dim\n",
    "        self.patch_dim = channel_size\n",
    "        \n",
    "        self.transformer_blocks = []\n",
    "        \n",
    "        f = self.get_factors(self.channel_dim)\n",
    "        print(f)\n",
    "        fi = np.abs(np.array(f) - np.sqrt(self.channel_dim)).argmin()\n",
    "        \n",
    "        _n_heads = f[fi]\n",
    "        \n",
    "        ## number of dims per channel -> channel_dim\n",
    "#         print('Num patches:', self.patch_dim)\n",
    "        print(f'Sequence len: {self.patch_dim} ; Block size: {block_seq_size}')\n",
    "        print('Channel dim:', self.channel_dim, 'num heads:',_n_heads)\n",
    "        \n",
    "        if block_seq_size is None or block_seq_size<2:\n",
    "            ### Find the block size for sequence:\n",
    "            block_seq_size = int(2**np.ceil(np.log2(np.sqrt(self.patch_dim))))\n",
    "            \n",
    "        print(f'MLP dim: {self.channel_dim} ; Block size: {block_mlp_size}')\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            L = Mixer_TransformerBlock_Encoder(self.patch_dim, block_seq_size, self.channel_dim, _n_heads, dropout, forward_expansion, nn.GELU, block_mlp_size)\n",
    "            self.transformer_blocks.append(L)\n",
    "        self.transformer_blocks = nn.Sequential(*self.transformer_blocks)\n",
    "        \n",
    "        self.linear = nn.Linear(self.patch_dim*self.channel_dim, num_classes)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(self.channel_dim, dropout=0)\n",
    "        if not pos_emb:\n",
    "            self.positional_encoding = nn.Identity()\n",
    "        \n",
    "        \n",
    "    def get_factors(self, n):\n",
    "        facts = []\n",
    "        for i in range(2, n+1):\n",
    "            if n%i == 0:\n",
    "                facts.append(i)\n",
    "        return facts\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.unfold(x).swapaxes(-1, -2)\n",
    "        x = self.channel_change(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.linear(x.view(bs, -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7094b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b0ac9f5-f1e2-4dba-a8e3-06bb4d5acac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a356db1-2fad-40ba-8186-7ca3c40ddd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3efe0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67e27199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT Mixer : Channels per patch -> Initial:12 Final:64\n",
      "[2, 4, 8, 16, 32, 64]\n",
      "Sequence len: 256 ; Block size: 16\n",
      "Channel dim: 64 num heads: 8\n",
      "MLP dim: 64 ; Block size: None\n"
     ]
    }
   ],
   "source": [
    "model = Mixer_ViT_Classifier([3, 32, 32], [2, 2], 64, num_blocks=2, num_classes=10, \n",
    "                            block_seq_size=16, block_mlp_size=None, pos_emb=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f741268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1017ac1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT Mixer : Channels per patch -> Initial:3 Final:64\n",
      "[2, 4, 8, 16, 32, 64]\n",
      "Sequence len: 1024 ; Block size: 32\n",
      "Channel dim: 64 num heads: 8\n",
      "MLP dim: 64 ; Block size: None\n"
     ]
    }
   ],
   "source": [
    "model = Mixer_ViT_Classifier([3, 32, 32], [1, 1], 64, num_blocks=2, num_classes=10, \n",
    "                            block_seq_size=32, block_mlp_size=None, pos_emb=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ae575940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfold: 0\n",
      "channel_change: 256\n",
      "transformer_blocks: 133888\n",
      "linear: 655370\n",
      "positional_encoding: 0\n"
     ]
    }
   ],
   "source": [
    "# print(\"number of params: \", sum(p.numel() for p in model.parameters()))\n",
    "for name, m in model.named_children():\n",
    "#     print(name)\n",
    "    print(f\"{name}: {sum(p.numel() for p in m.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d642126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b258ea4-8037-457d-aea6-35e9e02d9be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  789514\n"
     ]
    }
   ],
   "source": [
    "print(\"number of params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e21965e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx = torch.randn(32, 3, 32, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "493d244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit model(xx).mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad3633e7-1a94-4f3e-a218-109083017442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit model(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d7dd09f-bc43-4e55-9c57-a8ecdce48302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     %timeit model(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9707501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Mixer_ViT_Classifier([3, 32, 32], [1, 1], 64, num_blocks=4, num_classes=10, \n",
    "#                             block_seq_size=1024, block_mlp_size=None, pos_emb=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "652815b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c38b267c-a70b-43df-89aa-1548eab48e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"number of params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cee9c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit model(xx).mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78b2a832-f765-45db-a180-5b0a48074860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit model(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "669dc9ed-77a8-4615-8b16-d4dc13c4412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     %timeit model(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "044f8dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d123fa46-4abe-4533-82aa-21139c782670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_skip(model_name, ep):\n",
    "    \n",
    "    ## if file of benchmark is there and the training is done for full epochs\n",
    "    filename = f\"./output/benchmark/{model_name}_data.json\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename) as f:\n",
    "            data = json.load(f)\n",
    "        ## data consists of lists and dicts.\n",
    "        epochs = data['train_stat'][-1][0]\n",
    "        if epochs >= ep-1:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c173ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(dataset:str, patch_size:int, num_layers:int, SEED:int, sparse_att:bool=False, sparse_mlp:bool=False, pos_emb:bool=False, cuda:int=0):\n",
    "    device = torch.device(f\"cuda:{cuda}\")\n",
    "    \n",
    "    if sparse_att:\n",
    "        assert num_layers%2 == 0, 'number of blocks on sparse transformer is (x2)/2 hence it must be even'\n",
    "        num_layers_ = num_layers//2\n",
    "    else:\n",
    "        num_layers_ = num_layers\n",
    "    \n",
    "    BS = 256\n",
    "    NC = -1\n",
    "    EPOCHS = 300\n",
    "    imsize = (3, 32, 32)\n",
    "    expansion_dict = {16:1024, 8:256, 4:128, 2:64, 1:64}\n",
    "    expansion = expansion_dict[patch_size]\n",
    "\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    ##### Data Transforms\n",
    "    if dataset == 'tiny':\n",
    "        NC = 200\n",
    "        EPOCHS = 400\n",
    "        imsize = (3, 64, 64)\n",
    "        tiny_train = transforms.Compose([\n",
    "        transforms.RandAugment(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.5]*3,\n",
    "            std=[0.2]*3,\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        tiny_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.5]*3,\n",
    "                std=[0.2]*3,\n",
    "            ),\n",
    "        ])\n",
    "        \n",
    "        train_dataset = TinyImageNet_Preload(root=\"../../../../../_Datasets/tiny-imagenet-200\",\n",
    "                                     mode='train', transform=tiny_train)\n",
    "        test_dataset = TinyImageNet_Preload(root=\"../../../../../_Datasets/tiny-imagenet-200\",\n",
    "                                     mode='val', transform=tiny_test)\n",
    "        \n",
    "    elif dataset == 'cifar10':\n",
    "        NC = 10\n",
    "        BS = 32\n",
    "        EPOCHS = 200\n",
    "        \n",
    "        cifar_train = transforms.Compose([\n",
    "            transforms.RandomCrop(size=32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "                std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        cifar_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "                std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        train_dataset = datasets.CIFAR10(root=\"../../../../../_Datasets/cifar10/\", train=True, download=True, transform=cifar_train)\n",
    "        test_dataset = datasets.CIFAR10(root=\"../../../../../_Datasets/cifar10/\", train=False, download=True, transform=cifar_test)\n",
    "\n",
    "    elif dataset == 'cifar100':\n",
    "        NC = 100\n",
    "        BS = 128\n",
    "        EPOCHS = 300\n",
    "\n",
    "        cifar_train = transforms.Compose([\n",
    "            transforms.RandomCrop(size=32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.5071, 0.4865, 0.4409],\n",
    "                std=[0.2009, 0.1984, 0.2023],\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        cifar_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.5071, 0.4865, 0.4409],\n",
    "                std=[0.2009, 0.1984, 0.2023],\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        train_dataset = datasets.CIFAR100(root=\"../../../../../_Datasets/cifar100/\", train=True, download=True, transform=cifar_train)\n",
    "        test_dataset = datasets.CIFAR100(root=\"../../../../../_Datasets/cifar100/\", train=False, download=True, transform=cifar_test)\n",
    "        \n",
    "    ##### Now create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BS, shuffle=True, num_workers=4)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BS, shuffle=False, num_workers=4)\n",
    "    \n",
    "    ### Now create models\n",
    "    \n",
    "    seq_len = (imsize[-1]*imsize[-2])//(patch_size*patch_size)\n",
    "    mlp_dim = expansion\n",
    "    print(seq_len, mlp_dim)\n",
    "    \n",
    "    if sparse_att:\n",
    "        seq_len = int(2**np.ceil(np.log2(np.sqrt(seq_len))))\n",
    "    if sparse_mlp:\n",
    "        mlp_dim = int(2**np.ceil(np.log2(np.sqrt(expansion))))\n",
    "    \n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    model = Mixer_ViT_Classifier(imsize, \n",
    "                                 patch_size=[patch_size]*2, \n",
    "                                 hidden_channel=expansion, \n",
    "                                 num_blocks=num_layers_, \n",
    "                                 num_classes=NC, \n",
    "                                 block_seq_size=seq_len, \n",
    "                                 block_mlp_size=mlp_dim,\n",
    "                                 pos_emb=pos_emb).to(device)\n",
    "    \n",
    "#     _x = torch.randn(BS, *imsize).to(device)\n",
    "#     print(\"Output: \",vit_mixer(_x).shape)\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"number of params: \", num_params)\n",
    "    \n",
    "    _a, _b, _c = 'att', 'mlp', 'nPE'\n",
    "    if sparse_att: _a = 'sAtt'\n",
    "    if sparse_mlp: _b = 'sMlp'\n",
    "    if pos_emb: _c = 'PE'\n",
    "    model_name = f'01.3_ViT_{_c}_{dataset}_patch{patch_size}_l{num_layers}_{_a}_{_b}_s{SEED}'\n",
    "    print(f\"Model Name: {model_name}\")\n",
    "    \n",
    "    if experiment_skip(model_name, EPOCHS):\n",
    "        print(f'EXPERIMENT DONE... SKIPPING : {model_name}')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    STAT ={'train_stat':[], 'test_stat':[], 'params':num_params, }\n",
    "\n",
    "    ## Following is copied from \n",
    "    ### https://github.com/kuangliu/pytorch-cifar/blob/master/main.py\n",
    "\n",
    "    # Training\n",
    "    def train(epoch):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "#             break\n",
    "\n",
    "        STAT['train_stat'].append((epoch, train_loss/(batch_idx+1), 100.*correct/total)) ### (Epochs, Loss, Acc)\n",
    "        print(f\"[Train] {epoch} Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "        return\n",
    "\n",
    "    global best_acc\n",
    "    best_acc = -1\n",
    "    def test(epoch):\n",
    "        global best_acc\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        time_taken = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader)):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                start = time.time()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                start = time.time()-start\n",
    "                time_taken.append(start)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        STAT['test_stat'].append((epoch, test_loss/(batch_idx+1), 100.*correct/total, np.mean(time_taken))) ### (Epochs, Loss, Acc, time)\n",
    "        print(f\"[Test] {epoch} Loss: {test_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "\n",
    "        # Save checkpoint.\n",
    "        acc = 100.*correct/total\n",
    "        if acc > best_acc:\n",
    "            print('Saving..')\n",
    "            state = {\n",
    "                'model': model.state_dict(),\n",
    "                'acc': acc,\n",
    "                'epoch': epoch\n",
    "            }\n",
    "            if not os.path.isdir('models'):\n",
    "                os.mkdir('models')\n",
    "            torch.save(state, f'./models/benchmark/{model_name}.pth')\n",
    "            best_acc = acc\n",
    "\n",
    "        with open(f\"./output/benchmark/{model_name}_data.json\", 'w') as f:\n",
    "            json.dump(STAT, f, indent=0)\n",
    "\n",
    "    ### Train the whole damn thing\n",
    "#     EPOCHS = 1\n",
    "    for epoch in range(0, EPOCHS):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "        scheduler.step()\n",
    "        \n",
    "    \n",
    "    train_stat = np.array(STAT['train_stat'])\n",
    "    test_stat = np.array(STAT['test_stat'])\n",
    "\n",
    "    plt.plot(train_stat[:,1], label='train')\n",
    "    plt.plot(test_stat[:,1], label='test')\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"./output/benchmark/plots/{model_name}_loss.svg\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_stat[:,2], label='train')\n",
    "    plt.plot(test_stat[:,2], label='test')\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"./output/benchmark/plots/{model_name}_accs.svg\")\n",
    "    plt.show()\n",
    "    \n",
    "    del model, optimizer\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0d03c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark(dataset='tiny', \n",
    "#           patch_size=4, num_layers=10, SEED=123, sparse_att=True, sparse_mlp=True, cuda=0\n",
    "#          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "717e715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Automate the benchmark\n",
    "# ###### for tiny\n",
    "# cuda_idx = 0\n",
    "# # for seed in [147, 258, 369]:\n",
    "# for seed in [147]:\n",
    "#     for patch_size in [16, 8, 4]:\n",
    "#         for sparse_attention in [False, True]:\n",
    "#             for sparse_mlp in [False, True]:\n",
    "# #                 for nlayers in [6, 10, 14]:\n",
    "#                 for nlayers in [6]:\n",
    "#                     print(f'''\n",
    "#                         Experimenting on Tiny Dataset \n",
    "#                         patch:{patch_size},\n",
    "#                         sparse_att: {sparse_attention},\n",
    "#                         sparse_mlp: {sparse_mlp},\n",
    "#                         num_layers : {nlayers},\n",
    "#                         seed: {seed}\n",
    "#                     ''')\n",
    "            \n",
    "#                     benchmark(dataset='tiny', \n",
    "#                               patch_size=patch_size, \n",
    "#                               num_layers=nlayers, \n",
    "#                               SEED=seed, \n",
    "#                               sparse_att=sparse_attention, sparse_mlp=sparse_mlp, \n",
    "#                               cuda=cuda_idx\n",
    "#                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af01a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Automate the benchmark\n",
    "# ###### for tiny\n",
    "\n",
    "# not_working = [\n",
    "#     (4, False, True, 6),\n",
    "#     (4, False, False, 10),\n",
    "#     (4, False, False, 14),\n",
    "#     (4, False, True, 10),\n",
    "#     (4, False, True, 14),\n",
    "#     (4, True, True, 14),\n",
    "# ]\n",
    "\n",
    "# cuda_idx = 0\n",
    "# # for seed in [147, 258, 369]:\n",
    "# for seed in [147]:\n",
    "#     for patch_size in [16, 8, 4]:\n",
    "#         for sparse_attention in [False, True]:\n",
    "#             for sparse_mlp in [False, True]:\n",
    "#                 for nlayers in [6, 10, 14]:\n",
    "\n",
    "#                     print(f'''\n",
    "#                         Experimenting on Tiny Dataset \n",
    "#                         patch:{patch_size},\n",
    "#                         sparse_att: {sparse_attention},\n",
    "#                         sparse_mlp: {sparse_mlp},\n",
    "#                         num_layers : {nlayers},\n",
    "#                         seed: {seed}\n",
    "#                     ''')\n",
    "            \n",
    "#                 ### check if config is in not_working case\n",
    "#                     exit = False\n",
    "#                     for nw in not_working:\n",
    "#                         if patch_size==nw[0] and \\\n",
    "#                             sparse_attention==nw[1] and \\\n",
    "#                             sparse_mlp==nw[2] and\\\n",
    "#                             nlayers==nw[3]:\n",
    "                            \n",
    "#                             exit=True\n",
    "#                             break\n",
    "#                     if exit:\n",
    "#                         print(f'Exiting as the config is in NOT WORKING')\n",
    "#                         continue\n",
    "\n",
    "\n",
    "#                     benchmark(dataset='tiny', \n",
    "#                               patch_size=patch_size, \n",
    "#                               num_layers=nlayers, \n",
    "#                               SEED=seed, \n",
    "#                               sparse_att=sparse_attention, sparse_mlp=sparse_mlp, \n",
    "#                               cuda=cuda_idx\n",
    "#                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa4e109c-de63-40cf-b46e-084834eacc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Automate the benchmark\n",
    "# ###### for c10\n",
    "\n",
    "# not_working = [\n",
    "# ]\n",
    "\n",
    "# cuda_idx = 0\n",
    "# for dataset in ['cifar10']:\n",
    "#     for seed in [147]:\n",
    "#         for nlayers in [12]:\n",
    "#             for patch_size in [2]:\n",
    "#                 for sparse_attention in [True]:\n",
    "#                     for sparse_mlp in [False]:\n",
    "#                         for PE in [False]:\n",
    "\n",
    "#                             print(f'''\n",
    "#                                 Experimenting on {dataset} Dataset \n",
    "#                                 patch:{patch_size},\n",
    "#                                 sparse_att: {sparse_attention},\n",
    "#                                 sparse_mlp: {sparse_mlp},\n",
    "#                                 num_layers: {nlayers},\n",
    "#                                 pos_embed: {PE},\n",
    "#                                 seed: {seed}\n",
    "#                             ''')\n",
    "\n",
    "#                         ### check if config is in not_working case\n",
    "#                             exit = False\n",
    "#                             for nw in not_working:\n",
    "#                                 if patch_size==nw[0] and \\\n",
    "#                                     sparse_attention==nw[1] and \\\n",
    "#                                     sparse_mlp==nw[2] and\\\n",
    "#                                     nlayers==nw[3]:\n",
    "\n",
    "#                                     exit=True\n",
    "#                                     break\n",
    "#                             if exit:\n",
    "#                                 print(f'Exiting as the config is in NOT WORKING')\n",
    "#                                 continue\n",
    "\n",
    "\n",
    "#                             benchmark(dataset=dataset, \n",
    "#                                       patch_size=patch_size, \n",
    "#                                       num_layers=nlayers, \n",
    "#                                       SEED=seed, \n",
    "#                                       sparse_att=sparse_attention, sparse_mlp=sparse_mlp, \n",
    "#                                       pos_emb=PE,\n",
    "#                                       cuda=cuda_idx\n",
    "#                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59373f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asfdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b3d9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Automate the benchmark\n",
    "# ###### for c100\n",
    "\n",
    "# not_working = [\n",
    "#     (1, False, False, 4),\n",
    "#     (1, False, False, 8),\n",
    "#     (1, False, False, 12),\n",
    "#     (1, False, True, 4),\n",
    "#     (1, False, True, 8),\n",
    "#     (1, False, True, 12),\n",
    "# ]\n",
    "\n",
    "# cuda_idx = 0\n",
    "# dataset = 'cifar100'\n",
    "# # for seed in [147, 258, 369]:\n",
    "# for seed in [147]:\n",
    "#     for patch_size in [1, 2, 4, 8]:\n",
    "#         for sparse_attention in [False, True]:\n",
    "#             for sparse_mlp in [False, True]:\n",
    "#                 for nlayers in [4, 8, 12]:\n",
    "\n",
    "#                     print(f'''\n",
    "#                         Experimenting on {dataset} Dataset \n",
    "#                         patch:{patch_size},\n",
    "#                         sparse_att: {sparse_attention},\n",
    "#                         sparse_mlp: {sparse_mlp},\n",
    "#                         num_layers : {nlayers},\n",
    "#                         seed: {seed}\n",
    "#                     ''')\n",
    "            \n",
    "#                 ### check if config is in not_working case\n",
    "#                     exit = False\n",
    "#                     for nw in not_working:\n",
    "#                         if patch_size==nw[0] and \\\n",
    "#                             sparse_attention==nw[1] and \\\n",
    "#                             sparse_mlp==nw[2] and\\\n",
    "#                             nlayers==nw[3]:\n",
    "                            \n",
    "#                             exit=True\n",
    "#                             break\n",
    "#                     if exit:\n",
    "#                         print(f'Exiting as the config is in NOT WORKING')\n",
    "#                         continue\n",
    "\n",
    "\n",
    "#                     benchmark(dataset=dataset, \n",
    "#                               patch_size=patch_size, \n",
    "#                               num_layers=nlayers, \n",
    "#                               SEED=seed, \n",
    "#                               sparse_att=sparse_attention, sparse_mlp=sparse_mlp, \n",
    "#                               cuda=cuda_idx\n",
    "#                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e9c0465-3702-4a9b-b3eb-3b642926dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## c10 works on all patch sizes on 3090 using 64 batch size\n",
    "## c100 doesn't work on patch 1 for dense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63adc707-1309-4719-9764-eca72cdd1f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca20622e",
   "metadata": {},
   "source": [
    "## Benchmark Memory and Time CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1faebb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvidia_smi\n",
    "\n",
    "MB = 1024*1024\n",
    "def get_memory_used():\n",
    "    nvidia_smi.nvmlInit()\n",
    "    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(1)\n",
    "    info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "    val = info.used/MB\n",
    "    nvidia_smi.nvmlShutdown()\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1812db91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313.625"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_used()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb7fa91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "def get_memory_used():\n",
    "    command = \"nvidia-smi --query-gpu=memory.used --format=csv\"\n",
    "    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    return memory_free_values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30adc9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_used()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47d9cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asdfsadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7429fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_memory(dataset:str, patch_size:int, num_layers:int, SEED:int, sparse_att:bool=False, sparse_mlp:bool=False, pos_emb:bool=False, cuda:int=0):\n",
    "    device = torch.device(f\"cuda:{cuda}\")\n",
    "    \n",
    "    if sparse_att:\n",
    "        assert num_layers%2 == 0, 'number of blocks on sparse transformer is (x2)/2 hence it must be even'\n",
    "        num_layers_ = num_layers//2\n",
    "    else:\n",
    "        num_layers_ = num_layers\n",
    "    \n",
    "    BS = 32\n",
    "    NC = -1\n",
    "    EPOCHS = 1\n",
    "    imsize = (3, 32, 32)\n",
    "    expansion_dict = {16:1024, 8:256, 4:128, 2:64, 1:64}\n",
    "    expansion = expansion_dict[patch_size]\n",
    "\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    ##### Data Transforms\n",
    "        \n",
    "    if dataset == 'cifar10':\n",
    "        NC = 10\n",
    "    elif dataset == 'cifar100':\n",
    "        NC = 100\n",
    "\n",
    "    ### Now create models\n",
    "    \n",
    "    seq_len = (imsize[-1]*imsize[-2])//(patch_size*patch_size)\n",
    "    mlp_dim = expansion\n",
    "    print(seq_len, mlp_dim)\n",
    "    \n",
    "    if sparse_att:\n",
    "        seq_len = int(2**np.ceil(np.log2(np.sqrt(seq_len))))\n",
    "    if sparse_mlp:\n",
    "        mlp_dim = int(2**np.ceil(np.log2(np.sqrt(expansion))))\n",
    "    \n",
    "    mem_begin = get_memory_used()\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    model = Mixer_ViT_Classifier(imsize, \n",
    "                                 patch_size=[patch_size]*2, \n",
    "                                 hidden_channel=expansion, \n",
    "                                 num_blocks=num_layers_, \n",
    "                                 num_classes=NC, \n",
    "                                 block_seq_size=seq_len, \n",
    "                                 block_mlp_size=mlp_dim,\n",
    "                                 pos_emb=pos_emb).to(device)\n",
    "    \n",
    "    _x = torch.randn(BS, *imsize)#.to(device)\n",
    "    _y = torch.randint(10, (BS,))\n",
    "#     print(\"Output: \",vit_mixer(_x).shape)\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"number of params: \", num_params)\n",
    "    \n",
    "    _a, _b, _c = 'att', 'mlp', 'nPE'\n",
    "    if sparse_att: _a = 'sAtt'\n",
    "    if sparse_mlp: _b = 'sMlp'\n",
    "    if pos_emb: _c = 'PE'\n",
    "    model_name = f'01.3_ViT_{_c}_{dataset}_patch{patch_size}_l{num_layers}_{_a}_{_b}_s{SEED}'\n",
    "    print(f\"Model Name: {model_name}\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    inputs, targets = _x.to(device), _y.to(device)\n",
    "    ### test time taken for multiple iterations\n",
    "    time_taken = []\n",
    "    for i in range(50):\n",
    "        start = time.time()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        start = time.time()-start\n",
    "        time_taken.append(start)\n",
    "    train_time = np.mean(time_taken)\n",
    "        \n",
    "    mem_end = get_memory_used()\n",
    "    print(f\"mem begin: {mem_begin}  end: {mem_end}\")\n",
    "\n",
    "    model.eval()\n",
    "    time_taken = []\n",
    "    for i in range(50):\n",
    "        with torch.no_grad():\n",
    "            start = time.time()\n",
    "            outputs = model(inputs)\n",
    "            start = time.time()-start\n",
    "            time_taken.append(start)\n",
    "            \n",
    "    test_time = np.mean(time_taken)\n",
    "    \n",
    "    filename = f\"./output/benchmark_memory_data.json\"\n",
    "    if not os.path.exists(filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump({}, f, indent=0)\n",
    "            \n",
    "    with open(filename, 'r+') as f:\n",
    "#         with open(filename,'r+') as f:\n",
    "        file_data = json.load(f)\n",
    "        file_data[f\"{model_name}\"] = {'memory':mem_end-mem_begin, 'time_train':train_time, 'time_test':test_time}\n",
    "        f.seek(0)\n",
    "        json.dump(file_data, f, indent = 0)\n",
    "\n",
    "    del model, optimizer\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e8dbad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Automate the benchmark\n",
    "# ###### for c10\n",
    "\n",
    "# not_working = [\n",
    "# ]\n",
    "\n",
    "# cuda_idx = 0\n",
    "# seed = 147\n",
    "# PE = False\n",
    "# for dataset in ['cifar10']:\n",
    "#     for nlayers in [8, 4]:\n",
    "#         for patch_size in [2, 4]:\n",
    "#             for sparse_attention in [False, True]:\n",
    "#                 for sparse_mlp in [False]:\n",
    "\n",
    "#                     print(f'''\n",
    "#                         Experimenting on {dataset} Dataset \n",
    "#                         patch:{patch_size},\n",
    "#                         sparse_att: {sparse_attention},\n",
    "#                         sparse_mlp: {sparse_mlp},\n",
    "#                         num_layers: {nlayers},\n",
    "#                         pos_embed: {PE},\n",
    "#                         seed: {seed}\n",
    "#                     ''')\n",
    "\n",
    "#                     benchmark_memory(dataset=dataset, \n",
    "#                               patch_size=patch_size, \n",
    "#                               num_layers=nlayers, \n",
    "#                               SEED=seed, \n",
    "#                               sparse_att=sparse_attention, sparse_mlp=sparse_mlp, \n",
    "#                               pos_emb=PE,\n",
    "#                               cuda=cuda_idx\n",
    "#                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c0e54bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 64\n",
      "ViT Mixer : Channels per patch -> Initial:12 Final:64\n",
      "[2, 4, 8, 16, 32, 64]\n",
      "Sequence len: 256 ; Block size: 16\n",
      "Channel dim: 64 num heads: 8\n",
      "MLP dim: 64 ; Block size: 64\n",
      "number of params:  1773220\n",
      "Model Name: 01.3_ViT_nPE_cifar100_patch2_l4_sAtt_mlp_s147\n",
      "mem begin: 5  end: 565\n"
     ]
    }
   ],
   "source": [
    "benchmark_memory(dataset='cifar100', \n",
    "                  patch_size=2, \n",
    "                  num_layers=4, \n",
    "                  SEED=147, \n",
    "                  sparse_att=True, \n",
    "                  sparse_mlp=False, \n",
    "                  pos_emb=False,\n",
    "                  cuda=0\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675693c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
