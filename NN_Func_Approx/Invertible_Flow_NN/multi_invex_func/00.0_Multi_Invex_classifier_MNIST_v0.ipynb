{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "import random, os, pathlib, time\n",
    "from tqdm import tqdm\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nflib\n",
    "from nflib.flows import SequentialFlow, NormalizingFlow, ActNorm, AffineConstantFlow\n",
    "import nflib.coupling_flows as icf\n",
    "import nflib.inn_flow as inn\n",
    "import nflib.res_flow as irf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import distributions\n",
    "from torch.distributions import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mylibrary.datasets as datasets\n",
    "import mylibrary.nnlib as tnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "Download complete.\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "mnist = datasets.FashionMNIST()\n",
    "# mnist.download_mnist()\n",
    "# mnist.save_mnist()\n",
    "train_data, train_label_, test_data, test_label_ = mnist.load()\n",
    "\n",
    "train_data = train_data / 255.\n",
    "test_data = test_data / 255.\n",
    "\n",
    "# train_label = tnn.Logits.index_to_logit(train_label_)\n",
    "train_size = len(train_label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting data to pytorch format\n",
    "train_data = torch.Tensor(train_data)\n",
    "test_data = torch.Tensor(test_data)\n",
    "train_label = torch.LongTensor(train_label_)\n",
    "test_label = torch.LongTensor(test_label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Dataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        \n",
    "#         self.label = mask.type(torch.float32).reshape(-1,1)\n",
    "        self._shuffle_data_()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _shuffle_data_(self):\n",
    "        randidx = random.sample(range(len(self.data)), k=len(self.data))\n",
    "        self.data = self.data[randidx]\n",
    "        self.label = self.label[randidx]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, lbl = self.data[idx], self.label[idx]\n",
    "        return img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST_Dataset(train_data, train_label)\n",
    "test_dataset = MNIST_Dataset(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectedClassifier_Softmax(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim, num_sets, output_dim, inv_temp=1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_sets = num_sets\n",
    "        self.inv_temp = nn.Parameter(torch.ones(1)*inv_temp)\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, num_sets)\n",
    "        self.linear.bias.data *= 0\n",
    "        self.linear.weight.data *= 0.1\n",
    "        self.cls_weight = nn.Parameter(torch.ones(num_sets, output_dim)/output_dim)\n",
    "        self.cls_confidence = None\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hard=True):\n",
    "        x = self.linear(x)\n",
    "        if hard:\n",
    "            x = torch.softmax(-x*1e5, dim=1)\n",
    "        else:\n",
    "            x = torch.softmax(-x*self.inv_temp, dim=1)\n",
    "        self.cls_confidence = x\n",
    "        c = torch.softmax(self.cls_weight, dim=1)\n",
    "#         c = self.cls_weight\n",
    "        return x@c ## since both are normalized, it is also normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConnectedClassifier_SoftKMeans(nn.Module):\n",
    "    \n",
    "#     def __init__(self,input_dim, num_sets, output_dim, inv_temp=1):\n",
    "#         super().__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.output_dim = output_dim\n",
    "#         self.num_sets = num_sets\n",
    "#         self.inv_temp = nn.Parameter(torch.ones(1)*inv_temp)\n",
    "        \n",
    "#         self.centers = nn.Parameter(torch.rand(num_sets, input_dim)*2-1)\n",
    "        \n",
    "# #         self.cls_weight = nn.Parameter(torch.ones(num_sets, output_dim)/output_dim)\n",
    "\n",
    "#         init_val = torch.randn(num_sets, output_dim)*0.01\n",
    "#         for ns in range(num_sets):\n",
    "#             init_val[ns, ns%output_dim] = 10.\n",
    "#         self.cls_weight = nn.Parameter(init_val)\n",
    "\n",
    "#         self.cls_confidence = None\n",
    "        \n",
    "        \n",
    "#     def forward(self, x, hard=False):\n",
    "#         x = x[:, :self.input_dim]\n",
    "#         dists = torch.cdist(x, self.centers)\n",
    "#         dists = dists/np.sqrt(self.input_dim) ### correction to make diagonal of unit square 1 in nD space\n",
    "        \n",
    "#         if hard:\n",
    "#             x = torch.softmax(-dists*1e5, dim=1)\n",
    "#         else:\n",
    "#             x = torch.softmax(-dists*self.inv_temp, dim=1)\n",
    "#         self.cls_confidence = x\n",
    "#         c = torch.softmax(self.cls_weight, dim=1)\n",
    "# #         c = self.cls_weight\n",
    "#         return x@c ## since both are normalized, it is also normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectedClassifier_SoftKMeans(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim, num_sets, output_dim, inv_temp=1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_sets = num_sets\n",
    "        self.inv_temp = nn.Parameter(torch.ones(1)*inv_temp)\n",
    "        \n",
    "        self.centers = nn.Parameter(torch.rand(num_sets, input_dim)*2-1)\n",
    "        \n",
    "#         self.cls_weight = nn.Parameter(torch.ones(num_sets, output_dim)/output_dim)\n",
    "\n",
    "        init_val = torch.randn(num_sets, output_dim)*0.01\n",
    "        for ns in range(num_sets):\n",
    "            init_val[ns, ns%output_dim] = 10.\n",
    "        self.cls_weight = nn.Parameter(init_val)\n",
    "\n",
    "        self.cls_confidence = None\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hard=False):\n",
    "        self.cls_weight.data = torch.abs(self.cls_weight.data/self.cls_weight.data.sum(dim=1, keepdim=True))\n",
    "        \n",
    "        x = x[:, :self.input_dim]\n",
    "        dists = torch.cdist(x, self.centers)\n",
    "        dists = dists/np.sqrt(self.input_dim) ### correction to make diagonal of unit square 1 in nD space\n",
    "        \n",
    "        if hard:\n",
    "            x = torch.softmax(-dists*1e5, dim=1)\n",
    "        else:\n",
    "            x = torch.softmax(-dists*self.inv_temp, dim=1)\n",
    "        self.cls_confidence = x\n",
    "#         c = torch.softmax(self.cls_weight, dim=1)\n",
    "        c = self.cls_weight\n",
    "        return x@c ## since both are normalized, it is also normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.cls_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "actf = irf.Swish\n",
    "flows = [\n",
    "    ActNorm(784),\n",
    "    irf.ResidualFlow(784, [784], activation=actf),\n",
    "    ActNorm(784),\n",
    "    irf.ResidualFlow(784, [784], activation=actf),\n",
    "    ActNorm(784),\n",
    "        ]\n",
    "\n",
    "model = SequentialFlow(flows)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(nn.Linear(784, 784, bias=False),\n",
    "#                       nn.BatchNorm1d(784),\n",
    "#                       nn.SELU(),\n",
    "#                       nn.Linear(784, 784, bias=False),\n",
    "#                       nn.BatchNorm1d(784),\n",
    "#                       nn.SELU(),\n",
    "#                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialFlow(\n",
       "  (flows): ModuleList(\n",
       "    (0): ActNorm()\n",
       "    (1): ResidualFlow(\n",
       "      (resblock): ModuleList(\n",
       "        (0): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (1): Swish()\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ActNorm()\n",
       "    (3): ResidualFlow(\n",
       "      (resblock): ModuleList(\n",
       "        (0): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (1): Swish()\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (4): ActNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-2.4122e-01,  3.9845e-01, -2.6946e+00,  1.7283e+00,  2.5897e-01,\n",
       "          -5.0711e-01, -7.8307e-01,  1.8738e-01, -2.6747e-01, -1.5474e-01,\n",
       "          -8.4955e-01, -4.1385e-01,  4.1120e-01,  6.0509e-01, -1.3333e+00,\n",
       "           2.4900e-01, -5.5555e-01, -8.0115e-02, -2.8354e-01,  2.3042e+00,\n",
       "           4.1138e-01, -1.4528e+00,  8.8822e-01,  7.9267e-01,  5.1717e-03,\n",
       "           1.7742e+00, -8.6386e-01,  1.0566e+00, -7.1203e-01, -7.2001e-01,\n",
       "          -1.4182e+00, -5.0732e-01,  1.9776e-01, -1.3824e+00,  1.1846e+00,\n",
       "          -4.2273e-02, -8.5323e-01,  1.6248e+00,  7.3998e-01, -1.6188e+00,\n",
       "          -3.4078e-01, -9.1075e-01,  1.4554e+00, -1.7190e+00,  9.1147e-01,\n",
       "           5.5793e-01,  4.5162e-01, -4.3244e-01, -2.6180e-02,  6.6832e-02,\n",
       "          -7.7927e-01,  5.3435e-01,  7.0518e-01,  7.1681e-01,  9.1930e-02,\n",
       "           1.1415e-01,  7.9607e-01,  5.6855e-02, -1.3875e-01,  4.1392e+00,\n",
       "          -2.9740e-01, -3.6595e-01,  1.8473e+00,  4.1234e-01, -1.0612e+00,\n",
       "           3.5713e-01, -1.7831e+00, -3.2199e-01, -1.7895e-01,  1.4818e+00,\n",
       "          -3.0406e-02,  8.1047e-02,  1.0146e+00,  9.1526e-01,  1.7461e+00,\n",
       "           4.7048e-01,  7.5221e-02, -5.5942e-01,  6.6888e-01, -2.3850e+00,\n",
       "          -1.1702e+00, -2.1268e+00,  1.4312e-01, -1.3451e+00,  1.4233e+00,\n",
       "          -5.8786e-02,  2.8273e-01,  5.2633e-01, -4.3248e-01, -1.4663e+00,\n",
       "           1.9029e+00,  2.0901e-01, -1.8878e+00,  6.2443e-01,  1.3331e+00,\n",
       "           1.1858e-01, -2.3419e+00, -8.3038e-01, -1.0468e+00,  1.0360e+00,\n",
       "           1.9015e+00, -2.6552e-01,  6.0358e-01,  4.2541e-01,  8.6163e-01,\n",
       "           8.3626e-01,  9.7030e-01, -1.0053e+00, -7.3511e-01, -4.5054e-01,\n",
       "          -7.3617e-01, -5.2988e-01,  1.7304e+00, -7.2151e-01, -1.0681e-02,\n",
       "           1.2861e+00, -7.7117e-02, -6.9450e-02,  1.1647e+00, -1.6719e+00,\n",
       "          -8.2081e-01, -1.6659e-01, -1.2845e+00, -2.2682e+00, -6.6848e-01,\n",
       "           2.5872e-01,  7.8180e-01, -1.4848e+00, -2.0634e-01, -1.1000e-01,\n",
       "          -1.0138e+00, -1.3306e+00,  2.1915e-01,  5.6634e-02,  9.0260e-01,\n",
       "          -2.9203e-02, -1.3273e+00, -1.2308e-01, -1.4300e+00,  9.2836e-02,\n",
       "          -1.8901e+00,  4.0433e-01, -7.3821e-01,  4.1644e-01, -8.8364e-02,\n",
       "           1.7288e+00, -1.2080e+00, -7.2514e-01, -2.0746e+00,  6.3745e-01,\n",
       "          -6.1210e-01,  9.8189e-01, -2.6955e+00, -3.4679e-01,  1.7045e-01,\n",
       "           4.8227e-01,  1.2948e-01,  8.5223e-01,  2.5470e-01, -1.2437e+00,\n",
       "           7.5922e-01, -7.2979e-01,  9.2931e-01, -3.0985e-01, -9.5265e-01,\n",
       "          -5.8519e-01,  7.2948e-01, -9.1539e-01,  1.0370e+00, -1.3785e-01,\n",
       "           3.5908e-02, -6.3127e-01, -7.4998e-01, -5.4528e-01, -4.1798e-01,\n",
       "          -9.6720e-01, -6.2513e-01,  7.9029e-01,  1.5906e+00,  7.7046e-01,\n",
       "           3.4534e-01,  3.0019e-01,  3.9818e-01, -2.1325e-01, -1.2782e+00,\n",
       "          -1.0256e+00,  6.1595e-01, -1.9161e-01, -7.0002e-01,  6.7002e-02,\n",
       "           1.5783e+00, -1.2162e+00,  1.1404e+00, -2.8037e-01, -4.7299e-02,\n",
       "          -1.3974e+00,  5.9317e-01,  1.8191e+00,  1.2433e+00,  4.1153e-01,\n",
       "          -5.2612e-02, -7.3896e-01, -8.3744e-01,  8.9885e-01, -4.5233e-01,\n",
       "          -3.4346e-01,  2.7559e+00,  6.9838e-01, -1.1578e+00, -1.5759e-01,\n",
       "          -1.2066e+00, -1.0886e+00,  5.7170e-01,  1.0097e+00, -3.8095e-01,\n",
       "          -6.5786e-01,  3.3016e-01, -2.4168e-01, -7.7920e-01, -1.0407e-01,\n",
       "           1.1212e+00,  4.1407e-01,  1.2198e+00,  7.0536e-02,  6.2931e-01,\n",
       "           4.8611e-02, -1.7375e+00, -2.0283e-01,  1.0214e+00, -6.6821e-01,\n",
       "           2.0259e+00,  7.0626e-01, -4.1322e-01, -2.8906e-01, -2.8230e-02,\n",
       "           1.1129e+00,  1.2349e-01, -1.7781e+00, -1.0419e+00,  5.3161e-01,\n",
       "           1.7297e-01,  7.5990e-02,  3.4881e-02, -1.8913e+00, -2.4715e-01,\n",
       "           1.4209e+00, -6.1123e-01,  2.7703e-01,  1.0811e+00,  1.9127e-04,\n",
       "           7.9460e-01, -7.8714e-01, -3.2564e-01,  1.4615e-01,  1.8731e+00,\n",
       "           1.0908e+00,  4.2461e-01, -5.5578e-01,  3.0360e-01,  2.7184e-01,\n",
       "          -3.1582e-01,  4.6621e-01,  2.5182e-01,  4.5595e-01, -1.0843e+00,\n",
       "           2.1773e+00,  3.1207e-01, -5.2931e-01,  5.3178e-01, -4.4882e-01,\n",
       "          -9.6525e-01,  1.2338e+00, -7.7449e-01,  2.1198e+00,  3.8558e-01,\n",
       "          -8.0496e-01, -1.2531e+00, -1.1830e+00,  1.6511e+00, -9.2658e-01,\n",
       "          -6.8295e-01, -4.5304e-01,  6.4750e-02, -8.3418e-02,  1.2382e+00,\n",
       "          -1.3489e-01, -1.5023e+00,  1.5910e+00, -1.3810e+00,  8.2659e-01,\n",
       "          -6.3406e-02,  3.3839e-01,  1.2708e+00, -1.0447e+00,  1.6433e+00,\n",
       "          -1.7566e+00,  1.0525e+00, -8.3132e-01, -2.1357e-01,  9.9157e-01,\n",
       "          -1.2138e+00,  7.8154e-01, -3.9174e-01, -4.4143e-01,  3.0442e-01,\n",
       "          -1.7073e+00, -1.3180e+00,  7.6913e-01, -1.0263e+00, -6.9735e-04,\n",
       "           1.0900e+00,  1.2666e+00,  4.0003e-01,  1.1482e-01, -1.2584e+00,\n",
       "          -4.9102e-01,  5.1544e-02, -1.4090e+00,  1.4411e+00,  8.6286e-01,\n",
       "           4.7895e-01, -2.2631e+00, -3.1939e-01,  1.3568e+00,  2.0570e+00,\n",
       "           1.0145e+00, -6.9446e-01,  6.8832e-01, -9.7316e-01,  3.1073e-01,\n",
       "           1.0224e-01, -1.1064e+00, -1.3052e+00, -5.7865e-01, -6.7557e-01,\n",
       "          -1.0985e+00, -1.1038e+00, -5.6762e-01,  7.9282e-01,  1.8326e-01,\n",
       "          -4.3439e-01, -1.3437e+00,  1.9517e+00,  1.0308e+00, -8.4421e-01,\n",
       "           1.7251e+00, -3.0733e-01,  1.1301e+00, -2.2613e-01,  1.0223e+00,\n",
       "           1.7898e-01,  1.8917e-01,  5.7720e-01,  1.4849e-01,  7.9200e-01,\n",
       "          -5.2813e-01, -9.3202e-02, -6.3243e-01, -8.2138e-01, -1.3567e-01,\n",
       "          -4.1460e-01, -5.1283e-01, -1.8647e-01,  1.1348e-01,  1.6844e+00,\n",
       "           1.5406e+00,  3.1931e-02,  6.0784e-01,  1.4038e+00,  4.0606e-01,\n",
       "           1.8452e+00, -1.5693e+00, -1.1807e+00,  1.0402e+00, -6.9265e-01,\n",
       "           9.8968e-01, -7.7346e-01, -8.9360e-01, -4.2643e-01, -1.5447e+00,\n",
       "          -9.0408e-01,  7.6150e-01,  7.3222e-01,  9.9348e-01, -1.3359e+00,\n",
       "           1.7800e+00, -1.2132e+00,  3.7920e-01,  5.6181e-01,  2.4743e-01,\n",
       "          -8.8955e-01,  1.4274e+00,  1.7838e+00,  1.2015e+00, -9.2212e-01,\n",
       "          -4.7642e-01,  6.6022e-01,  7.2726e-01, -4.8360e-01,  2.5162e+00,\n",
       "           6.2765e-01, -6.2524e-01,  1.8300e+00,  9.9474e-01,  6.4326e-01,\n",
       "          -4.7803e-01, -3.2125e-01,  2.0294e+00, -1.1687e+00,  1.1885e-01,\n",
       "          -6.7440e-01, -1.7686e-01, -1.9299e+00,  1.2635e+00,  1.3312e+00,\n",
       "           6.6291e-01, -1.5597e+00, -6.1888e-01,  7.2594e-01,  2.5118e+00,\n",
       "           4.6169e-01, -1.4364e+00, -1.1326e+00, -4.6266e-01,  1.6312e-01,\n",
       "          -2.0867e+00,  7.3086e-01,  1.8480e-01,  2.5223e-02,  1.3783e+00,\n",
       "          -1.0389e+00,  2.9667e-01, -6.3116e-01, -8.0644e-01, -2.2247e-01,\n",
       "          -3.0772e-01, -1.4676e+00, -4.0149e-01,  9.4233e-01, -8.8967e-01,\n",
       "           3.8721e-01, -2.0674e+00, -2.5378e+00,  1.4801e+00,  4.1853e-01,\n",
       "          -1.5570e+00,  2.5723e-01,  1.2988e+00,  2.4966e-01, -4.5822e-01,\n",
       "          -1.3647e+00, -1.3383e+00, -5.9910e-01, -1.1962e+00,  5.3254e-01,\n",
       "           2.5306e-01,  9.1080e-01, -8.3342e-01,  9.4455e-01, -5.2233e-01,\n",
       "          -4.7503e-01,  2.6879e-01, -2.6386e-01,  1.2447e+00,  1.7277e+00,\n",
       "           3.6441e-01,  4.0947e-01, -3.6570e-02,  1.4963e-01,  1.1072e+00,\n",
       "          -1.2368e+00,  1.0938e-01,  1.7353e+00,  1.4046e+00,  3.4156e-01,\n",
       "           3.2508e-01, -7.9678e-02,  4.9555e-01, -5.7523e-01, -1.1535e+00,\n",
       "           4.8810e-02, -4.9804e-01,  3.9065e-01, -4.8330e-02,  3.8950e-01,\n",
       "          -6.3775e-01,  1.8085e+00,  2.6041e-01, -3.9398e-01,  1.0685e-01,\n",
       "           2.5718e+00,  8.7720e-01,  4.8400e-01,  5.8062e-01, -6.1526e-02,\n",
       "           8.3233e-01, -1.0231e+00, -2.1455e+00, -7.6029e-01, -2.5103e+00,\n",
       "          -1.5810e+00, -1.6306e+00, -1.1432e+00, -1.0620e+00,  3.2884e-01,\n",
       "          -8.4344e-01, -7.8637e-01, -1.5175e+00, -4.7215e-01, -2.3106e-01,\n",
       "           8.9276e-01,  4.1846e-01, -1.6523e-01, -9.5962e-01,  4.9958e-01,\n",
       "           1.6459e+00, -3.1547e-01,  2.2230e+00,  1.3913e+00, -5.6278e-01,\n",
       "          -4.9479e-01, -3.0114e-01, -3.6157e-01, -1.4687e-01,  1.8295e+00,\n",
       "           1.0845e+00, -1.3340e-01,  4.7647e-01, -3.6934e-02, -6.3173e-01,\n",
       "          -8.9946e-01, -8.7670e-01,  1.8306e+00, -6.4941e-01,  4.3141e-01,\n",
       "           6.0184e-01, -1.1712e+00,  6.3164e-01,  5.3274e-01,  1.6679e+00,\n",
       "          -1.9848e-01, -1.0719e-02, -5.3399e-01, -1.1852e+00,  3.0385e-01,\n",
       "          -2.9667e-01,  3.2802e-02, -2.5290e-01,  9.4205e-01,  2.5217e-02,\n",
       "           6.7081e-01,  6.6108e-01, -1.4289e+00,  3.3050e-01,  4.4825e-01,\n",
       "           1.0004e+00,  6.0574e-01,  4.2361e-01,  1.3202e+00,  2.0392e+00,\n",
       "          -1.0828e-01, -7.4304e-02,  1.9191e+00,  2.1103e-01, -1.6727e+00,\n",
       "          -1.3631e-01,  1.3886e+00, -8.1297e-01,  9.3386e-01, -6.2602e-01,\n",
       "          -4.0780e-01, -2.8106e-02, -2.6011e-01,  9.6697e-01, -1.1827e+00,\n",
       "          -3.7041e-01,  7.7267e-02,  2.6553e-03, -6.8293e-01, -3.4172e-01,\n",
       "          -9.6876e-01, -4.1960e-01,  2.4313e-01, -2.5642e+00, -1.0058e-01,\n",
       "          -1.6905e+00,  8.4594e-01,  1.9365e-01, -2.0760e+00,  1.4529e+00,\n",
       "           4.2771e-01,  1.2276e+00, -9.7325e-01,  1.6240e+00, -1.8009e+00,\n",
       "          -7.2753e-01, -1.3590e+00,  1.2461e-01,  1.9444e-01, -6.7277e-01,\n",
       "           2.3827e+00, -6.1695e-01, -1.1710e+00, -1.3441e-01,  1.7254e+00,\n",
       "          -6.0676e-01,  1.3960e+00,  8.2584e-01, -3.8398e-01, -1.1849e+00,\n",
       "          -9.2382e-02,  2.2773e+00,  1.5750e+00, -6.9610e-02,  4.0627e-01,\n",
       "          -1.3597e-01,  1.4179e+00,  1.6398e+00,  1.3198e+00, -5.9521e-01,\n",
       "          -5.3060e-01, -5.9153e-01, -9.7652e-01, -3.3172e-01,  6.1231e-02,\n",
       "          -6.8336e-01,  1.9420e-01,  7.8180e-01,  2.0835e+00,  1.2135e+00,\n",
       "           2.6674e+00,  3.2737e-01,  5.9710e-02, -5.9882e-01,  1.5069e-01,\n",
       "           5.9504e-01, -2.3816e-01,  1.0275e+00, -8.1200e-02, -5.5974e-01,\n",
       "           7.9585e-02, -7.4682e-01,  3.9015e-02, -5.8829e-02, -4.8208e-01,\n",
       "           5.5964e-01,  7.8600e-01,  7.8963e-01, -1.5833e+00, -1.0330e+00,\n",
       "           4.3119e-01, -2.3065e-03, -8.7033e-02,  1.6517e+00,  1.4143e+00,\n",
       "          -8.7767e-01, -8.6175e-01,  9.7180e-01, -1.3332e+00,  9.6782e-01,\n",
       "           9.8921e-01, -2.2691e-01,  7.4918e-01, -2.0033e-01,  3.5765e-01,\n",
       "           8.8831e-01, -9.0991e-01,  4.8470e-01, -2.3047e-01,  3.3863e-01,\n",
       "          -5.7902e-01, -1.0310e+00,  8.7648e-01, -1.1022e-01,  1.2030e+00,\n",
       "          -1.4069e+00, -1.7438e+00,  8.4866e-01,  2.1941e-01,  1.6968e-01,\n",
       "           4.0923e-01,  5.8992e-01,  5.3898e-02,  1.7780e-01,  2.0026e-01,\n",
       "          -1.6761e-01,  1.0105e+00, -2.8130e-01, -5.2737e-01,  4.8139e-01,\n",
       "           5.2457e-01, -9.1406e-01,  2.1410e+00,  1.2994e+00,  4.8491e-01,\n",
       "           1.6375e-01,  1.8142e+00, -3.2207e-02,  1.3043e-01, -4.5434e-01,\n",
       "           1.0100e+00,  6.7284e-01, -9.5003e-01, -1.0474e+00,  3.0002e-01,\n",
       "           8.8571e-01,  7.1810e-01, -7.0647e-01, -5.7942e-01,  9.0998e-01,\n",
       "           1.4989e+00, -1.1331e+00,  9.1932e-01, -4.3906e-01, -3.1661e-01,\n",
       "          -6.3937e-01,  1.7926e-01,  5.4560e-01, -7.4315e-01,  6.6669e-01,\n",
       "           1.1823e+00,  2.7299e+00, -1.7412e-01, -2.2255e+00,  1.0573e+00,\n",
       "           6.6244e-01,  3.7007e-01, -4.9247e-01, -5.9452e-01,  8.8086e-01,\n",
       "           4.9912e-01, -1.7880e+00, -3.1606e-01, -1.1001e+00,  1.4888e+00,\n",
       "           3.8832e-01,  1.0960e+00, -1.3731e+00, -4.2796e-01, -7.5536e-01,\n",
       "          -1.0127e-01, -6.3871e-01,  9.3959e-02, -2.0481e+00,  5.2870e-01,\n",
       "          -6.6350e-01, -1.5276e-01, -1.5401e+00, -1.1167e+00, -6.6744e-01,\n",
       "          -6.1521e-01, -5.8495e-02, -5.3752e-01,  1.0437e+00, -8.5703e-01,\n",
       "           2.1784e-01,  1.0623e+00, -3.9826e-01,  8.7607e-01, -7.8088e-02,\n",
       "          -1.5552e+00, -3.5037e-01, -5.2723e-01,  1.0835e+00, -1.8009e+00,\n",
       "           8.4265e-01,  2.0079e+00,  2.7262e-01, -2.3589e-01,  2.8243e-01,\n",
       "           9.8653e-01, -1.7637e+00, -1.5284e+00,  4.8513e-01, -1.5107e+00,\n",
       "          -1.0167e+00,  7.4551e-02,  1.9563e+00, -2.9083e-01,  9.4419e-01,\n",
       "           3.9037e-01, -4.2744e-01,  2.0496e-01,  2.0448e+00]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-3.3848e-01, -3.0949e-01, -2.5308e-02, -1.1487e+00, -1.0017e+00,\n",
       "          -4.9300e-01,  8.3338e-01, -7.3080e-05, -1.1949e+00,  9.1381e-01,\n",
       "           5.0534e-01, -1.1752e+00,  1.2437e+00, -6.5928e-01, -5.2835e-01,\n",
       "          -5.6768e-01, -3.6051e-01,  1.1773e+00, -3.1818e-01,  3.7600e-01,\n",
       "          -9.5787e-01, -7.5193e-01,  6.0341e-01,  1.0460e+00,  4.2657e-01,\n",
       "           1.6445e-01, -1.3566e-01, -2.5482e-01,  5.8032e-01, -8.3463e-01,\n",
       "           3.9071e-01, -5.4263e-01,  6.2461e-02,  5.2215e-01,  4.3802e-01,\n",
       "          -5.6223e-01,  4.2897e-01,  2.9093e-01,  2.9711e-01,  8.3253e-01,\n",
       "           6.4335e-01, -1.9465e-01,  1.8331e+00, -4.0439e-01,  5.9699e-01,\n",
       "          -8.2673e-02, -2.0333e+00, -5.5153e-01,  9.4625e-02,  6.0598e-01,\n",
       "          -4.2749e-01,  3.9438e-01,  1.6547e-01,  4.5049e-01, -1.6128e+00,\n",
       "           7.5949e-01,  3.2054e+00,  6.9521e-01, -3.6755e-03, -3.1139e-01,\n",
       "          -9.6237e-01,  1.9237e+00,  1.5516e-01,  2.8544e-01, -1.1634e+00,\n",
       "          -2.2465e+00,  1.7970e+00,  1.4288e+00,  1.0715e+00, -2.4856e+00,\n",
       "          -6.5815e-01,  1.7831e+00,  1.0901e+00,  6.5046e-01, -1.9580e+00,\n",
       "           9.8480e-02, -1.4565e+00,  5.5664e-01, -1.3418e-01, -4.3086e-01,\n",
       "          -1.9762e-01, -2.7075e-01, -2.3153e+00,  1.3930e+00, -1.6029e+00,\n",
       "          -5.0338e-01,  1.0102e+00, -2.6664e-01,  1.0587e+00,  1.0573e+00,\n",
       "           1.1813e-01, -2.4726e-01, -5.0874e-02, -8.7518e-01, -3.0483e-01,\n",
       "          -1.6683e+00,  1.6447e+00, -1.8285e-01,  6.7882e-01, -2.3256e+00,\n",
       "           1.2866e-01,  7.6710e-01, -1.2998e+00, -1.2017e-01, -5.9714e-01,\n",
       "           2.6267e-01, -4.5974e-01,  5.3048e-02,  1.7480e+00,  7.2360e-01,\n",
       "           3.4934e-01, -9.3225e-01,  1.5020e+00,  1.3978e+00, -7.5347e-01,\n",
       "          -3.7257e-01,  3.0765e-01, -4.1770e-01,  1.7045e+00, -4.2259e-01,\n",
       "          -4.9999e-01,  9.8918e-01, -1.0367e+00,  1.9955e+00,  1.2160e-01,\n",
       "          -4.1488e-01,  2.5258e-01,  1.7937e+00,  6.0949e-01, -1.2424e+00,\n",
       "          -4.7370e-01,  6.0222e-01, -1.1515e+00, -5.2247e-01,  2.8320e-01,\n",
       "          -4.4800e-03,  1.0652e+00,  4.7533e-01,  1.0113e+00,  1.0755e+00,\n",
       "           8.2486e-01, -2.2705e+00,  9.3930e-01,  1.0877e+00, -5.0528e-01,\n",
       "          -6.7588e-01,  4.7067e-01,  6.0313e-03, -3.7122e-01,  5.4936e-01,\n",
       "          -6.5102e-01,  1.0052e+00,  6.9882e-01,  4.9837e-01,  1.7659e-01,\n",
       "           2.0535e-01, -1.1706e+00, -1.3522e+00, -1.7249e+00,  4.4552e-02,\n",
       "          -1.4316e+00, -9.9740e-01, -1.1068e+00, -5.2097e-01, -1.7124e-01,\n",
       "           5.4207e-01, -1.3121e+00,  1.0588e+00,  1.1180e+00, -1.3550e+00,\n",
       "           9.1974e-01,  1.1720e+00,  8.4046e-01, -3.4168e-01, -3.5511e-01,\n",
       "          -8.5745e-01, -5.9778e-01,  5.3403e-01, -3.6564e-01, -1.2949e+00,\n",
       "          -2.4623e-01, -1.1121e+00,  4.2878e-01, -1.0016e+00, -4.8412e-01,\n",
       "          -5.3408e-01,  1.9372e-02, -1.0710e+00, -1.3617e+00, -4.1998e-01,\n",
       "          -9.9200e-02,  4.3909e-01,  1.3263e+00, -6.7528e-01,  2.2618e-01,\n",
       "          -2.1527e+00, -1.1455e-01,  1.3778e+00, -8.2405e-01,  1.6579e+00,\n",
       "           2.3782e+00,  1.5503e+00, -7.3299e-01,  1.0628e+00,  2.6696e+00,\n",
       "           8.1446e-01,  6.1121e-01,  1.8613e-02, -1.6056e+00, -1.4291e-01,\n",
       "          -9.3100e-01,  5.9684e-01, -5.7540e-01,  1.3638e-01,  1.3630e+00,\n",
       "           3.4590e-01,  1.2694e-01, -3.4579e-01,  7.2032e-01,  4.2293e-01,\n",
       "           2.3773e-01,  2.7801e+00,  1.1045e+00,  5.5750e-01,  3.6713e-01,\n",
       "          -6.5147e-01, -9.4058e-02, -5.3808e-02,  1.8308e+00, -6.3179e-03,\n",
       "           1.0032e+00, -2.7768e-01,  1.9382e+00,  2.2678e+00, -1.1681e+00,\n",
       "           1.2199e+00, -1.1208e+00, -7.1960e-01,  6.0220e-01, -8.6276e-01,\n",
       "          -2.5307e-01,  6.3487e-01, -1.4493e+00, -7.9548e-01, -1.2720e-01,\n",
       "          -1.3689e+00,  8.5663e-02,  4.1447e-01, -3.7497e-01,  1.4544e+00,\n",
       "          -8.3084e-01,  1.1799e+00,  2.7877e-01, -1.1470e+00,  2.4869e-02,\n",
       "          -8.7881e-01,  1.0966e+00, -1.6073e+00, -2.5081e-01, -3.5128e-01,\n",
       "          -2.3330e-01, -2.7460e-01, -6.0891e-02, -1.2691e+00, -2.1194e+00,\n",
       "          -6.7670e-01,  9.9457e-02, -2.0245e-01, -2.0186e+00,  4.4858e-02,\n",
       "          -6.5520e-02, -2.9591e-01, -6.1067e-02,  1.3016e+00, -1.5210e+00,\n",
       "           1.6582e+00,  3.0651e-01,  8.6126e-01,  8.3874e-01, -4.3927e-01,\n",
       "           7.4953e-01, -9.5191e-01, -4.7248e-01,  4.8719e-01,  4.2070e-01,\n",
       "          -4.0550e-01, -4.0027e-01,  2.1458e+00,  2.3965e+00, -8.6971e-01,\n",
       "          -1.0427e+00,  5.2570e-01,  1.6082e+00, -2.2595e-01, -1.6530e+00,\n",
       "          -2.5914e-01, -1.6927e+00,  2.5567e+00,  1.3593e+00,  1.8207e+00,\n",
       "          -9.2116e-01, -5.9096e-01,  4.6403e-01, -1.2707e+00,  1.3305e+00,\n",
       "           5.2374e-01, -3.1882e-01,  9.2319e-01, -1.4017e+00,  4.0136e-01,\n",
       "           8.1584e-01, -1.4037e+00,  7.8467e-01, -1.2150e+00,  1.0706e+00,\n",
       "          -7.7982e-01, -1.2842e+00,  5.4549e-01, -7.7833e-01, -1.4865e-01,\n",
       "          -2.2397e-01,  9.3384e-01,  1.6658e-01,  9.3442e-01, -1.0193e+00,\n",
       "           2.3353e+00,  1.0503e+00,  1.9634e-01,  3.9234e-01, -5.5394e-01,\n",
       "          -2.5057e-01, -5.5461e-01,  1.4475e+00,  9.8182e-01, -8.1871e-02,\n",
       "          -1.0266e+00, -4.6403e-01, -2.4135e-01,  2.9894e-01, -9.8612e-01,\n",
       "          -6.0982e-01,  3.0556e-01, -5.5434e-01,  4.7361e-01, -3.3541e-01,\n",
       "          -2.3312e-01,  6.1171e-01, -5.1253e-01,  1.4708e+00, -3.2686e-01,\n",
       "          -1.8387e-01, -1.1436e+00,  1.7589e-01, -1.6546e+00, -1.1681e+00,\n",
       "          -8.8582e-02,  3.1414e-01, -2.0047e+00,  7.3634e-01,  6.5467e-01,\n",
       "          -6.7971e-01,  1.2279e+00, -3.6627e-01,  6.0246e-01,  9.7241e-01,\n",
       "           1.5172e+00,  1.3056e+00,  1.5568e+00, -1.7539e+00, -1.1584e-01,\n",
       "           5.5438e-01,  1.1371e+00,  1.9313e+00,  3.3107e-01,  8.2661e-01,\n",
       "          -4.5965e-02, -5.2653e-01, -1.0529e+00, -7.1168e-01,  3.1823e-01,\n",
       "           3.5872e-01,  1.4835e+00,  6.3507e-01,  1.7960e+00, -6.3725e-01,\n",
       "          -1.0632e+00,  1.6744e-01, -1.0211e+00, -5.1477e-01, -2.9819e-03,\n",
       "          -2.3747e+00, -1.0102e-01,  1.1362e+00,  2.0148e+00, -2.7833e-01,\n",
       "           3.3530e-01,  3.5028e-01, -4.2496e-01, -9.1960e-01, -2.6273e-01,\n",
       "          -1.0399e-01, -2.1465e+00, -1.5339e+00,  4.9914e-01, -2.4824e-02,\n",
       "          -8.4195e-01, -1.0805e+00, -6.1587e-01,  1.9674e+00, -9.9551e-01,\n",
       "          -1.4685e+00, -9.3807e-01,  9.5388e-01, -1.2430e+00,  1.4952e+00,\n",
       "          -2.0482e-01,  5.4912e-01, -1.1905e+00, -1.1556e+00, -4.1774e-01,\n",
       "          -1.9232e-01,  4.0486e-01,  8.6323e-01,  5.5798e-01,  1.0861e+00,\n",
       "          -2.9428e-01,  2.1105e+00,  1.6431e+00, -2.0879e-01, -5.6366e-02,\n",
       "           5.9010e-01, -8.6115e-01,  3.9173e-01,  1.5473e+00,  2.2235e+00,\n",
       "          -1.6616e+00, -7.6652e-01,  3.3724e-01,  1.2161e+00, -6.8769e-01,\n",
       "           9.9528e-01, -1.1251e-01, -4.4106e-01, -1.6809e+00,  6.5972e-01,\n",
       "           8.0607e-01,  8.6477e-02,  1.2138e+00, -1.8243e-02, -2.8324e-01,\n",
       "           9.9927e-01,  1.1439e-01,  9.6616e-01,  7.0261e-01, -6.5335e-01,\n",
       "           2.7386e-01,  1.5345e-01,  7.3391e-01, -9.3646e-01, -2.5980e-01,\n",
       "          -6.2863e-02,  1.0814e+00, -1.9681e-01,  6.1592e-01,  3.7940e+00,\n",
       "          -7.7179e-01, -9.8049e-01, -4.7394e-01,  8.0306e-02, -3.8981e-01,\n",
       "           3.3878e-02, -7.2456e-01, -3.3176e-01,  4.7224e-01, -9.0718e-01,\n",
       "           4.1391e-01, -4.0817e-01,  1.2209e+00,  7.7021e-01,  9.2750e-01,\n",
       "           9.2892e-01, -1.0767e+00,  5.2718e-01,  1.1804e+00,  6.4930e-01,\n",
       "           7.7743e-01, -2.5246e+00, -6.7314e-01,  7.4098e-01,  4.9817e-01,\n",
       "           3.7552e-01,  2.8238e-02, -1.4368e-01, -6.9955e-01,  1.8584e+00,\n",
       "           7.9768e-02, -1.2748e-01,  1.3692e+00, -1.7860e+00, -4.4236e-01,\n",
       "          -1.5910e+00,  2.1331e+00,  1.2115e+00, -7.6089e-02,  7.3735e-01,\n",
       "          -2.0895e+00,  1.0276e+00, -3.2676e-01, -1.6953e-01,  2.6600e+00,\n",
       "          -7.3488e-01,  4.4929e-01, -8.7492e-01, -3.2344e-01,  5.8932e-01,\n",
       "           1.2645e-01, -1.3993e-01,  2.4048e+00, -7.7698e-01, -5.9232e-01,\n",
       "          -7.9426e-02, -5.6381e-01, -1.1103e+00, -3.7704e-01,  1.2632e+00,\n",
       "           1.0780e+00, -1.5870e-01,  7.3012e-01, -3.0541e-01,  6.7981e-02,\n",
       "           7.1360e-01, -8.4645e-01,  1.5978e+00, -7.5782e-01, -6.7128e-01,\n",
       "           1.2486e+00,  9.7713e-01, -4.4633e-01,  5.9970e-01, -2.5083e-01,\n",
       "          -1.2463e+00,  1.0230e+00, -4.0229e-01, -2.2893e+00, -9.3657e-02,\n",
       "           1.3830e+00,  1.0241e+00,  1.4151e+00,  1.2702e+00, -3.4671e-01,\n",
       "           1.0133e+00, -7.7683e-01, -2.0988e-01, -2.4787e+00, -1.2310e+00,\n",
       "           1.5258e+00,  4.2922e-02, -7.2262e-01, -7.2811e-01,  4.6601e-01,\n",
       "          -1.5001e+00, -3.9281e+00,  9.1214e-01,  1.2584e-01, -2.8880e-01,\n",
       "           2.7324e-01, -1.2091e+00,  1.5033e-01, -4.6092e-01, -1.0233e+00,\n",
       "           2.7620e+00,  1.3326e+00, -1.3464e+00,  1.2634e+00,  1.5313e-01,\n",
       "          -1.5680e-01,  1.3762e+00, -1.1913e+00, -2.0534e-01,  3.0774e-01,\n",
       "           7.6543e-01,  3.3801e-01, -9.7049e-01, -3.2641e-01, -2.4065e-01,\n",
       "           1.4071e+00, -7.6425e-01,  7.0121e-02,  8.3292e-01,  7.8984e-01,\n",
       "           9.2292e-01, -2.5852e-01,  5.5594e-01,  3.0317e-01,  5.8084e-01,\n",
       "           2.6322e-01, -1.5283e-01,  9.3086e-02,  2.3660e-01,  1.1537e+00,\n",
       "          -9.0522e-03, -1.2058e+00,  1.3858e+00,  5.2119e-01,  1.7633e+00,\n",
       "          -2.4870e-01,  1.6065e+00,  7.2133e-01,  2.1028e+00, -8.0645e-01,\n",
       "           1.1721e+00,  2.8383e+00,  1.0474e+00,  8.0673e-01, -1.2752e+00,\n",
       "          -8.7514e-03,  5.4869e-01, -5.1453e-01,  6.8282e-01, -1.8362e+00,\n",
       "          -1.2128e+00, -2.0132e+00,  2.4806e+00, -2.2246e+00,  6.4208e-01,\n",
       "          -2.1809e+00,  9.3617e-01, -3.3312e-02,  1.2259e+00,  2.0889e+00,\n",
       "           3.1862e-01, -9.1485e-01,  3.8835e-01, -1.4855e-01,  1.1214e+00,\n",
       "           2.0053e+00,  5.9722e-01,  2.4793e-01, -1.4447e+00,  6.3778e-01,\n",
       "           8.4525e-01, -3.5615e-01,  9.5097e-01, -2.0546e+00, -1.1534e+00,\n",
       "           8.9473e-01, -6.9054e-01,  3.2055e-01,  1.2716e+00,  1.3703e+00,\n",
       "          -6.7268e-02,  1.2344e+00,  9.6289e-02,  9.1361e-01, -5.5067e-01,\n",
       "           6.6537e-01, -4.6033e-01,  7.5257e-01, -1.5401e+00, -3.9638e-01,\n",
       "          -6.7732e-01,  5.4164e-01,  1.1385e-01, -9.9723e-01, -1.6878e+00,\n",
       "          -8.3626e-01,  8.0754e-01, -2.7932e-02, -5.3454e-01,  2.3276e+00,\n",
       "          -1.2282e+00, -6.9698e-01, -1.0850e+00,  6.1709e-01,  1.4819e+00,\n",
       "           1.5993e+00, -9.2403e-01, -1.1363e+00, -9.4340e-01, -1.0694e+00,\n",
       "          -7.8871e-01, -2.9174e-02,  1.2034e+00, -3.9206e-02,  1.1215e+00,\n",
       "          -7.6586e-01,  8.1140e-01,  1.0565e+00, -9.2279e-01,  5.1319e-01,\n",
       "           2.3862e-01,  2.1326e-01,  4.3737e-01, -4.5646e-01, -1.2864e-01,\n",
       "          -1.7514e+00, -2.1378e-01,  1.3118e+00,  1.6189e+00, -9.1991e-01,\n",
       "           8.5774e-01, -7.2673e-01,  2.7352e-01, -2.3503e-01, -1.4485e+00,\n",
       "           1.3425e+00, -1.2124e-01,  1.0738e+00, -2.5866e+00,  1.3706e-01,\n",
       "           1.6013e+00, -1.0949e-01,  1.0532e-01, -2.6630e-01,  4.2476e-01,\n",
       "          -1.4174e+00, -4.1901e-01, -2.9806e-01, -7.0155e-01,  2.2385e-01,\n",
       "           1.3348e-01,  7.1562e-01,  1.0862e+00,  7.0084e-01,  6.8413e-02,\n",
       "          -4.6857e-01, -1.6952e-01,  5.3579e-01, -8.1600e-01, -7.5882e-01,\n",
       "           7.8197e-01,  5.8430e-02,  1.2286e-01, -8.0133e-01, -1.1796e+00,\n",
       "          -1.0741e-01,  7.1494e-01, -8.3795e-01, -2.2226e+00, -7.7833e-01,\n",
       "          -6.3267e-01,  3.5248e-01,  1.4169e+00, -1.6070e+00, -4.4834e-01,\n",
       "           3.4641e-01,  1.5510e-01,  1.1586e+00,  2.6097e-01,  7.2178e-01,\n",
       "          -6.6829e-01, -1.5806e+00,  2.7447e-01,  1.0915e-01,  1.0037e+00,\n",
       "           6.2576e-01, -7.6889e-01, -6.4040e-01, -3.4478e-01,  9.7660e-02,\n",
       "          -1.1884e+00, -2.7496e-01,  1.0601e-01,  8.4273e-03, -4.7182e-01,\n",
       "           5.1135e-01,  2.4690e-02, -2.5405e-01, -6.9453e-01,  1.0509e+00,\n",
       "           2.7249e-01,  1.7931e+00, -9.3866e-01, -1.2062e+00,  1.2763e+00,\n",
       "           4.6712e-01, -1.6580e+00, -6.9714e-01,  4.2010e-01, -8.5965e-01,\n",
       "           2.3430e-02, -2.2901e+00,  5.7923e-01,  1.2115e-01]], device='cuda:0',\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.flows[0].parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ConnectedClassifier_SoftKMeans(784, 100, 10)\n",
    "# classifier = ConnectedClassifier_Softmax(784, 10, 10)\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0003\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(dataset=train_dataset, num_workers=4, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, num_workers=4, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  2466466\n"
     ]
    }
   ],
   "source": [
    "# criterion = nn.NLLLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(list(model.parameters())+list(classifier.parameters()),\n",
    "                       lr=learning_rate, weight_decay=1e-15) # todo tune WD\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "print(\"number of params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(torch.isnan(p).type(torch.float32).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4540, -1.0148, -0.4887,  ..., -0.7704,  0.0872, -1.5229],\n",
       "        [-0.1529,  0.4253,  0.3910,  ...,  1.0342,  0.1840,  0.7612],\n",
       "        [ 0.0140, -0.4461,  0.8271,  ..., -0.9388,  0.4237, -1.1483],\n",
       "        ...,\n",
       "        [-1.4833, -0.1052, -2.0154,  ...,  1.1987,  2.0257,  0.7927],\n",
       "        [ 0.3202,  0.7024,  0.6155,  ..., -1.3535, -0.4620, -0.1215],\n",
       "        [-1.6686, -1.1265, -1.2277,  ...,  1.5840, -1.5207, -0.1042]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(10, 784).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 784])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = iter(test_loader).next()[0]\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:07<00:00, 165.58it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0:0,  Loss:1.5591812133789062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 298.78it/s]\n",
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:89.72%, Test Acc:87.05%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:07<00:00, 167.10it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1:0,  Loss:1.4683300256729126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 287.81it/s]\n",
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:90.07%, Test Acc:87.50%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:07<00:00, 168.09it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2:0,  Loss:1.5660227537155151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 297.02it/s]\n",
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:90.22%, Test Acc:88.28%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:07<00:00, 168.11it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3:0,  Loss:1.524125576019287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 302.18it/s]\n",
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:90.48%, Test Acc:87.15%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:07<00:00, 167.90it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4:0,  Loss:1.5458433628082275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 303.09it/s]\n",
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:90.70%, Test Acc:88.30%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:07<00:00, 165.60it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5:0,  Loss:1.5767687559127808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 288.29it/s]\n",
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:90.83%, Test Acc:88.17%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:07<00:00, 166.64it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6:0,  Loss:1.5994981527328491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 276.07it/s]\n",
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:91.02%, Test Acc:88.28%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:07<00:00, 166.81it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7:0,  Loss:1.5361965894699097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 320.30it/s]\n",
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:91.19%, Test Acc:88.47%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:07<00:00, 167.32it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8:0,  Loss:1.5533859729766846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 314.72it/s]\n",
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:91.42%, Test Acc:88.55%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:07<00:00, 167.86it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9:0,  Loss:1.5348869562149048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 289.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:91.59%, Test Acc:88.36%\n",
      "\n",
      "\t-> Train Acc 91.595 ; Test Acc 88.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "EPOCHS = 10\n",
    "\n",
    "index = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_acc = 0\n",
    "    train_count = 0\n",
    "    for xx, yy in tqdm(train_loader):\n",
    "        xx, yy = xx.to(device), yy.to(device)\n",
    "#     for xx, yy in tqdm(test_loader):\n",
    "\n",
    "        yout = model(xx)\n",
    "#         print(yout)\n",
    "        yout = classifier(yout)    \n",
    "#         print(yout)\n",
    "        loss = criterion(yout, yy)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(float(loss))\n",
    "\n",
    "        outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "        correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "        train_acc += correct\n",
    "        train_count += len(outputs)\n",
    "#         break\n",
    "\n",
    "    train_accs.append(float(train_acc)/train_count*100)\n",
    "    train_acc = 0\n",
    "    train_count = 0\n",
    "\n",
    "    print(f'Epoch: {epoch}:{index},  Loss:{float(loss)}')\n",
    "    test_count = 0\n",
    "    test_acc = 0\n",
    "    for xx, yy in tqdm(test_loader):\n",
    "        xx, yy = xx.to(device), yy.to(device)\n",
    "        with torch.no_grad():\n",
    "            yout = classifier(model(xx))    \n",
    "        outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "        correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "        test_acc += correct\n",
    "        test_count += len(xx)\n",
    "    test_accs.append(float(test_acc)/test_count*100)\n",
    "    print(f'Train Acc:{train_accs[-1]:.2f}%, Test Acc:{test_accs[-1]:.2f}%')\n",
    "    print()\n",
    "\n",
    "### after each class index is finished training\n",
    "print(f'\\t-> Train Acc {max(train_accs)} ; Test Acc {max(test_accs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.cls_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard test accuracy with count per classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 311.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Test Acc:88.36%\n",
      "[74, 0, 63, 1, 650, 0, 68, 10, 11, 11, 344, 6, 111, 16, 40, 35, 150, 14, 5, 1, 6, 0, 31, 52, 4, 301, 2, 92, 185, 507, 38, 0, 174, 24, 11, 3, 0, 15, 5, 74, 65, 329, 511, 114, 31, 4, 446, 1, 340, 2, 15, 230, 26, 154, 1, 78, 219, 102, 9, 214, 19, 9, 17, 52, 13, 40, 18, 747, 62, 5, 3, 336, 15, 17, 57, 22, 3, 10, 3, 159, 273, 1, 6, 269, 48, 6, 73, 14, 366, 21, 3, 68, 17, 332, 280, 480, 57, 11, 15, 28]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_count = 0\n",
    "test_acc = 0\n",
    "set_count = torch.zeros(classifier.num_sets).to(device)\n",
    "for xx, yy in tqdm(test_loader):\n",
    "    xx, yy = xx.to(device), yy.to(device)\n",
    "    with torch.no_grad():\n",
    "        yout = classifier(model(xx), hard=True)\n",
    "        set_indx, count = torch.unique(torch.argmax(classifier.cls_confidence, dim=1), return_counts=True) \n",
    "        set_count[set_indx] += count\n",
    "    outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "    correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "    test_acc += correct\n",
    "    test_count += len(xx)\n",
    "\n",
    "print(f'Hard Test Acc:{float(test_acc)/test_count*100:.2f}%')\n",
    "print(set_count.type(torch.long).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard train accuracy with count per classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:03<00:00, 359.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Train Acc:91.65%\n",
      "[409, 0, 318, 18, 3561, 2, 368, 86, 44, 60, 2173, 17, 588, 65, 242, 238, 788, 66, 19, 0, 59, 0, 213, 250, 27, 1823, 11, 595, 1173, 3158, 265, 0, 1260, 125, 72, 23, 7, 49, 11, 397, 457, 2083, 2972, 701, 157, 31, 2750, 5, 1994, 5, 116, 1421, 196, 891, 3, 507, 1342, 707, 47, 1212, 130, 76, 88, 338, 91, 222, 68, 4301, 373, 39, 0, 1870, 117, 79, 292, 126, 3, 108, 18, 997, 1614, 0, 8, 1717, 243, 20, 389, 59, 2211, 96, 14, 452, 84, 2009, 2009, 2892, 381, 47, 109, 133]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_count = 0\n",
    "test_acc = 0\n",
    "set_count = torch.zeros(classifier.num_sets).to(device)\n",
    "for xx, yy in tqdm(train_loader):\n",
    "    xx, yy = xx.to(device), yy.to(device)\n",
    "    with torch.no_grad():\n",
    "        yout = classifier(model(xx), hard=True)\n",
    "        set_indx, count = torch.unique(torch.argmax(classifier.cls_confidence, dim=1), return_counts=True) \n",
    "        set_count[set_indx] += count\n",
    "    outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "    correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "    test_acc += correct\n",
    "    test_count += len(xx)\n",
    "\n",
    "print(f'Hard Train Acc:{float(test_acc)/test_count*100:.2f}%')\n",
    "print(set_count.type(torch.long).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(94, device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Classifiers that enclose any data\n",
    "torch.count_nonzero(set_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3,\n",
       "        4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7,\n",
       "        8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,\n",
       "        2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5,\n",
       "        6, 7, 8, 9], device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### classifier with class representation\n",
    "torch.argmax(classifier.cls_weight, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class labels are same as that of initialized\n",
    "# tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3,\n",
    "#         4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7,\n",
    "#         8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,\n",
    "#         2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5,\n",
    "#         6, 7, 8, 9], device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[14.5033, -2.3390, -2.3409, -2.2740, -2.3565, -2.3720, -2.1097, -2.3840,\n",
       "         -2.3706, -2.3809],\n",
       "        [-2.3772, 14.4716, -2.3956, -2.3275, -2.3879, -2.3858, -2.3834, -2.3961,\n",
       "         -2.4096, -2.4031],\n",
       "        [-2.3144, -2.3321, 14.5638, -2.3289, -2.1009, -2.3546, -2.1033, -2.3621,\n",
       "         -2.3181, -2.3527],\n",
       "        [-2.2622, -2.2628, -2.3509, 14.4797, -2.3197, -2.3655, -2.3083, -2.3762,\n",
       "         -2.3589, -2.3743],\n",
       "        [-2.4062, -2.4268, -2.1114, -2.3799, 14.5947, -2.4376, -2.1948, -2.4477,\n",
       "         -2.3933, -2.4408],\n",
       "        [-2.3914, -2.3885, -2.3987, -2.3950, -2.4002, 14.5619, -2.3902, -2.2499,\n",
       "         -2.3637, -2.3111],\n",
       "        [-2.0379, -2.2405, -1.9522, -2.1715, -1.9584, -2.2554, 14.4938, -2.2768,\n",
       "         -2.2024, -2.2734],\n",
       "        [-2.4377, -2.4237, -2.4369, -2.4340, -2.4470, -2.2871, -2.4310, 14.6102,\n",
       "         -2.4123, -2.3418],\n",
       "        [-2.4166, -2.4554, -2.4106, -2.4334, -2.4156, -2.4106, -2.3998, -2.4082,\n",
       "         14.5393, -2.4187],\n",
       "        [-2.4469, -2.4581, -2.4528, -2.4438, -2.4467, -2.3900, -2.4535, -2.3702,\n",
       "         -2.4254, 14.5676],\n",
       "        [14.5766, -2.4126, -2.4018, -2.3672, -2.4212, -2.4484, -2.1930, -2.4511,\n",
       "         -2.4380, -2.4530],\n",
       "        [-2.4461, 14.5942, -2.4693, -2.4125, -2.4606, -2.4656, -2.4632, -2.4777,\n",
       "         -2.4743, -2.4752],\n",
       "        [-2.3210, -2.3574, 14.5542, -2.3470, -2.1084, -2.3586, -2.1174, -2.3685,\n",
       "         -2.3288, -2.3571],\n",
       "        [-2.3095, -2.3021, -2.3972, 14.5566, -2.3533, -2.4150, -2.3574, -2.4180,\n",
       "         -2.4066, -2.4194],\n",
       "        [-2.3510, -2.3707, -2.0640, -2.3366, 14.5434, -2.3865, -2.1573, -2.3875,\n",
       "         -2.3472, -2.3690],\n",
       "        [-2.4329, -2.4485, -2.4334, -2.4391, -2.4435, 14.6500, -2.4411, -2.2928,\n",
       "         -2.4005, -2.3545],\n",
       "        [-2.0620, -2.2636, -1.9974, -2.2088, -2.0036, -2.2835, 14.5137, -2.3031,\n",
       "         -2.2350, -2.2977],\n",
       "        [-2.3989, -2.3888, -2.3922, -2.3984, -2.4051, -2.2437, -2.3967, 14.5476,\n",
       "         -2.3704, -2.2914],\n",
       "        [-2.4025, -2.4227, -2.3964, -2.4122, -2.3904, -2.3960, -2.3845, -2.3923,\n",
       "         14.5339, -2.3923],\n",
       "        [-2.3507, -2.3534, -2.3456, -2.3543, -2.3471, -2.2604, -2.3510, -2.2537,\n",
       "         -2.3339, 14.4762],\n",
       "        [14.5296, -2.3719, -2.3739, -2.3159, -2.3877, -2.4054, -2.1550, -2.4165,\n",
       "         -2.4044, -2.4104],\n",
       "        [-2.3796, 14.4992, -2.4026, -2.3413, -2.4004, -2.3983, -2.3927, -2.4012,\n",
       "         -2.4206, -2.4071],\n",
       "        [-2.3303, -2.3664, 14.5460, -2.3577, -2.1263, -2.3669, -2.1403, -2.3756,\n",
       "         -2.3354, -2.3696],\n",
       "        [-2.3322, -2.3261, -2.4181, 14.5667, -2.3797, -2.4528, -2.3768, -2.4533,\n",
       "         -2.4447, -2.4437],\n",
       "        [-2.3149, -2.3296, -2.0452, -2.2880, 14.4786, -2.3682, -2.1279, -2.3611,\n",
       "         -2.3176, -2.3621],\n",
       "        [-2.4764, -2.4816, -2.4740, -2.4775, -2.4810, 14.6950, -2.4702, -2.3083,\n",
       "         -2.4508, -2.4014],\n",
       "        [-1.8848, -2.0978, -1.8106, -2.0391, -1.8179, -2.1058, 14.2939, -2.1305,\n",
       "         -2.0354, -2.1155],\n",
       "        [-2.4528, -2.4466, -2.4418, -2.4593, -2.4550, -2.2974, -2.4436, 14.6194,\n",
       "         -2.4207, -2.3720],\n",
       "        [-2.4854, -2.5100, -2.4772, -2.5042, -2.4786, -2.4842, -2.4717, -2.4782,\n",
       "         14.6181, -2.4817],\n",
       "        [-2.4965, -2.4924, -2.4904, -2.4920, -2.4852, -2.4192, -2.4910, -2.4068,\n",
       "         -2.4716, 14.6400],\n",
       "        [14.5415, -2.3508, -2.3549, -2.3035, -2.3709, -2.3928, -2.1489, -2.4027,\n",
       "         -2.3956, -2.4025],\n",
       "        [-2.3886, 14.4800, -2.4023, -2.3400, -2.3989, -2.4032, -2.4068, -2.4188,\n",
       "         -2.4354, -2.4183],\n",
       "        [-2.3374, -2.3723, 14.5774, -2.3641, -2.1106, -2.3839, -2.1344, -2.3914,\n",
       "         -2.3493, -2.3799],\n",
       "        [-2.2975, -2.2962, -2.3862, 14.5273, -2.3536, -2.4035, -2.3393, -2.4033,\n",
       "         -2.3935, -2.3990],\n",
       "        [-2.3454, -2.3612, -2.0455, -2.3185, 14.5273, -2.3622, -2.1348, -2.3799,\n",
       "         -2.3374, -2.3665],\n",
       "        [-2.4339, -2.4274, -2.4286, -2.4330, -2.4461, 14.6178, -2.4375, -2.2734,\n",
       "         -2.4041, -2.3502],\n",
       "        [-2.0059, -2.1919, -1.8962, -2.1311, -1.9004, -2.1859, 14.4045, -2.2104,\n",
       "         -2.1367, -2.1974],\n",
       "        [-2.4376, -2.4244, -2.4336, -2.4428, -2.4389, -2.2802, -2.4268, 14.5759,\n",
       "         -2.4053, -2.3391],\n",
       "        [-2.3751, -2.3982, -2.3506, -2.3817, -2.3608, -2.3509, -2.3434, -2.3606,\n",
       "         14.4962, -2.3504],\n",
       "        [-2.4516, -2.4512, -2.4523, -2.4476, -2.4517, -2.3801, -2.4471, -2.3584,\n",
       "         -2.4193, 14.5929],\n",
       "        [14.5519, -2.3955, -2.3991, -2.3358, -2.4108, -2.4406, -2.1777, -2.4469,\n",
       "         -2.4267, -2.4440],\n",
       "        [-2.4937, 14.6647, -2.5227, -2.4590, -2.5125, -2.5149, -2.5085, -2.5233,\n",
       "         -2.5245, -2.5288],\n",
       "        [-2.3553, -2.3982, 14.6072, -2.3871, -2.1606, -2.4076, -2.1619, -2.4155,\n",
       "         -2.3892, -2.4179],\n",
       "        [-2.3319, -2.3166, -2.4188, 14.5670, -2.3798, -2.4184, -2.3777, -2.4315,\n",
       "         -2.4223, -2.4239],\n",
       "        [-2.3413, -2.3543, -2.0604, -2.3177, 14.5209, -2.3753, -2.1303, -2.3759,\n",
       "         -2.3352, -2.3755],\n",
       "        [-2.3952, -2.3907, -2.3907, -2.3917, -2.3973, 14.5710, -2.3924, -2.2285,\n",
       "         -2.3587, -2.3165],\n",
       "        [-2.1170, -2.2771, -2.0100, -2.2367, -1.9951, -2.3033, 14.5616, -2.3181,\n",
       "         -2.2466, -2.3134],\n",
       "        [-2.4266, -2.4031, -2.4250, -2.4234, -2.4361, -2.2753, -2.4186, 14.5742,\n",
       "         -2.3922, -2.3399],\n",
       "        [-2.4717, -2.4837, -2.4454, -2.4709, -2.4568, -2.4450, -2.4504, -2.4444,\n",
       "         14.6156, -2.4503],\n",
       "        [-2.4217, -2.4254, -2.4107, -2.4165, -2.4139, -2.3409, -2.4117, -2.3250,\n",
       "         -2.3919, 14.5440],\n",
       "        [14.4715, -2.3349, -2.3315, -2.2712, -2.3406, -2.3784, -2.1026, -2.3803,\n",
       "         -2.3692, -2.3838],\n",
       "        [-2.4630, 14.6433, -2.4787, -2.4246, -2.4741, -2.4875, -2.4734, -2.4897,\n",
       "         -2.4876, -2.4806],\n",
       "        [-2.3167, -2.3573, 14.5493, -2.3474, -2.1006, -2.3597, -2.1230, -2.3730,\n",
       "         -2.3196, -2.3622],\n",
       "        [-2.3485, -2.3291, -2.4384, 14.5785, -2.4065, -2.4526, -2.3927, -2.4577,\n",
       "         -2.4409, -2.4591],\n",
       "        [-2.3057, -2.3212, -2.0268, -2.2787, 14.4821, -2.3356, -2.1083, -2.3506,\n",
       "         -2.2928, -2.3359],\n",
       "        [-2.4330, -2.4243, -2.4178, -2.4281, -2.4352, 14.6672, -2.4395, -2.2850,\n",
       "         -2.3981, -2.3551],\n",
       "        [-2.0846, -2.2473, -1.9914, -2.2005, -1.9815, -2.2787, 14.5436, -2.2919,\n",
       "         -2.2337, -2.2884],\n",
       "        [-2.4549, -2.4565, -2.4550, -2.4553, -2.4591, -2.3082, -2.4587, 14.6221,\n",
       "         -2.4313, -2.3745],\n",
       "        [-2.4281, -2.4526, -2.4163, -2.4448, -2.4245, -2.4047, -2.4084, -2.3920,\n",
       "         14.5474, -2.3988],\n",
       "        [-2.4941, -2.5055, -2.4882, -2.5039, -2.4971, -2.4335, -2.5025, -2.4171,\n",
       "         -2.4803, 14.6240],\n",
       "        [14.5001, -2.3353, -2.3315, -2.2812, -2.3408, -2.3660, -2.0976, -2.3767,\n",
       "         -2.3565, -2.3697],\n",
       "        [-2.4550, 14.5984, -2.4581, -2.3980, -2.4573, -2.4574, -2.4557, -2.4751,\n",
       "         -2.4748, -2.4644],\n",
       "        [-2.3042, -2.3389, 14.5511, -2.3364, -2.0957, -2.3528, -2.0988, -2.3652,\n",
       "         -2.3218, -2.3589],\n",
       "        [-2.3292, -2.3062, -2.4120, 14.5757, -2.3812, -2.4323, -2.3684, -2.4344,\n",
       "         -2.4263, -2.4350],\n",
       "        [-2.3127, -2.3257, -2.0327, -2.2728, 14.5020, -2.3502, -2.1100, -2.3521,\n",
       "         -2.3157, -2.3418],\n",
       "        [-2.4532, -2.4487, -2.4563, -2.4530, -2.4577, 14.6664, -2.4511, -2.3096,\n",
       "         -2.4231, -2.3850],\n",
       "        [-1.9991, -2.1968, -1.9266, -2.1418, -1.9624, -2.2042, 14.4408, -2.2322,\n",
       "         -2.1712, -2.2150],\n",
       "        [-2.4896, -2.4842, -2.4922, -2.4936, -2.4931, -2.3469, -2.4831, 14.6560,\n",
       "         -2.4672, -2.4021],\n",
       "        [-2.4698, -2.4895, -2.4536, -2.4841, -2.4566, -2.4622, -2.4432, -2.4569,\n",
       "         14.6035, -2.4495],\n",
       "        [-2.4485, -2.4373, -2.4347, -2.4545, -2.4426, -2.3696, -2.4315, -2.3569,\n",
       "         -2.4186, 14.5698],\n",
       "        [14.4746, -2.3198, -2.3295, -2.2560, -2.3422, -2.3667, -2.1087, -2.3813,\n",
       "         -2.3553, -2.3685],\n",
       "        [-2.4757, 14.6365, -2.5036, -2.4422, -2.4936, -2.5058, -2.4911, -2.5101,\n",
       "         -2.5143, -2.5100],\n",
       "        [-2.3072, -2.3396, 14.5470, -2.3316, -2.0918, -2.3423, -2.1036, -2.3584,\n",
       "         -2.3195, -2.3461],\n",
       "        [-2.2840, -2.2525, -2.3592, 14.5209, -2.3175, -2.3788, -2.3170, -2.3861,\n",
       "         -2.3692, -2.3857],\n",
       "        [-2.3486, -2.3576, -2.0699, -2.3180, 14.5256, -2.3889, -2.1377, -2.3856,\n",
       "         -2.3335, -2.3774],\n",
       "        [-2.4465, -2.4494, -2.4474, -2.4486, -2.4591, 14.6589, -2.4454, -2.2923,\n",
       "         -2.4156, -2.3664],\n",
       "        [-1.9892, -2.1670, -1.8658, -2.1116, -1.8750, -2.1887, 14.3938, -2.2008,\n",
       "         -2.1338, -2.1907],\n",
       "        [-2.4344, -2.4343, -2.4389, -2.4376, -2.4409, -2.2955, -2.4281, 14.5613,\n",
       "         -2.4018, -2.3460],\n",
       "        [-2.4459, -2.4827, -2.4483, -2.4569, -2.4419, -2.4349, -2.4307, -2.4260,\n",
       "         14.5314, -2.4283],\n",
       "        [-2.4783, -2.4831, -2.4773, -2.4706, -2.4653, -2.4156, -2.4774, -2.4006,\n",
       "         -2.4510, 14.6073],\n",
       "        [14.5565, -2.3996, -2.3966, -2.3432, -2.4078, -2.4334, -2.1807, -2.4346,\n",
       "         -2.4193, -2.4353],\n",
       "        [-2.3963, 14.5455, -2.4224, -2.3481, -2.4138, -2.4183, -2.4088, -2.4232,\n",
       "         -2.4364, -2.4360],\n",
       "        [-2.2266, -2.2783, 14.4820, -2.2485, -2.0074, -2.2885, -2.0126, -2.2955,\n",
       "         -2.2511, -2.2870],\n",
       "        [-2.3198, -2.3155, -2.4124, 14.5810, -2.3715, -2.4264, -2.3673, -2.4300,\n",
       "         -2.4129, -2.4268],\n",
       "        [-2.3411, -2.3638, -2.0570, -2.3106, 14.5319, -2.3835, -2.1356, -2.3823,\n",
       "         -2.3357, -2.3726],\n",
       "        [-2.4161, -2.4082, -2.4119, -2.4156, -2.4206, 14.5984, -2.4173, -2.2752,\n",
       "         -2.3863, -2.3338],\n",
       "        [-2.0933, -2.2739, -2.0152, -2.2259, -2.0239, -2.2802, 14.5167, -2.2945,\n",
       "         -2.2300, -2.2896],\n",
       "        [-2.4395, -2.4366, -2.4317, -2.4294, -2.4384, -2.3006, -2.4422, 14.5824,\n",
       "         -2.3993, -2.3388],\n",
       "        [-2.4946, -2.5203, -2.4675, -2.5071, -2.4760, -2.4765, -2.4679, -2.4736,\n",
       "         14.6393, -2.4656],\n",
       "        [-2.4549, -2.4499, -2.4404, -2.4496, -2.4465, -2.3789, -2.4362, -2.3592,\n",
       "         -2.4104, 14.5717],\n",
       "        [14.5037, -2.3617, -2.3541, -2.2925, -2.3603, -2.3849, -2.1256, -2.4033,\n",
       "         -2.3764, -2.3942],\n",
       "        [-2.4786, 14.6242, -2.4967, -2.4416, -2.4846, -2.4951, -2.4947, -2.5042,\n",
       "         -2.5029, -2.5058],\n",
       "        [-2.3098, -2.3394, 14.5434, -2.3329, -2.0861, -2.3534, -2.1093, -2.3666,\n",
       "         -2.3229, -2.3485],\n",
       "        [-2.3261, -2.3115, -2.4256, 14.5816, -2.3874, -2.4354, -2.3763, -2.4406,\n",
       "         -2.4389, -2.4394],\n",
       "        [-2.3585, -2.3761, -2.0964, -2.3300, 14.5634, -2.3946, -2.1725, -2.3972,\n",
       "         -2.3686, -2.3970],\n",
       "        [-2.4742, -2.4709, -2.4810, -2.4761, -2.4773, 14.6967, -2.4780, -2.3347,\n",
       "         -2.4526, -2.3952],\n",
       "        [-2.0710, -2.2568, -1.9742, -2.2007, -1.9760, -2.2670, 14.5117, -2.2925,\n",
       "         -2.2258, -2.2925],\n",
       "        [-2.4088, -2.4150, -2.4205, -2.4154, -2.4169, -2.2604, -2.4153, 14.5622,\n",
       "         -2.3793, -2.3144],\n",
       "        [-2.4502, -2.4829, -2.4334, -2.4692, -2.4474, -2.4313, -2.4347, -2.4369,\n",
       "         14.5791, -2.4386],\n",
       "        [-2.4916, -2.4937, -2.4834, -2.4945, -2.4893, -2.4206, -2.4936, -2.4037,\n",
       "         -2.4708, 14.5785]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.cls_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.unique(torch.argmax(classifier.cls_confidence, dim=1), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([7.8020], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.inv_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.8328e-08, 3.8267e-08, 3.8325e-08, 3.8312e-08, 3.8171e-08, 4.4385e-08,\n",
       "        3.8185e-08, 1.0000e+00, 3.9245e-08, 4.1537e-08], device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### example output per classifier\n",
    "yout[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdfsdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-80a679df1217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0masdfsdf\u001b[0m \u001b[0;31m## to break the code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'asdfsdf' is not defined"
     ]
    }
   ],
   "source": [
    "asdfsdf ## to break the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyze per classifier accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:05<00:00, 233.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Train Acc:91.65%\n",
      "[409, 0, 318, 18, 3561, 2, 368, 86, 44, 60, 2173, 17, 588, 65, 242, 238, 788, 66, 19, 0, 59, 0, 213, 250, 27, 1823, 11, 595, 1173, 3158, 265, 0, 1260, 125, 72, 23, 7, 49, 11, 397, 457, 2083, 2972, 701, 157, 31, 2750, 5, 1994, 5, 116, 1421, 196, 891, 3, 507, 1342, 707, 47, 1212, 130, 76, 88, 338, 91, 222, 68, 4301, 373, 39, 0, 1870, 117, 79, 292, 126, 3, 108, 18, 997, 1614, 0, 8, 1717, 243, 20, 389, 59, 2211, 96, 14, 452, 84, 2009, 2009, 2892, 381, 47, 109, 133]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_count = 0\n",
    "test_acc = 0\n",
    "set_count = torch.zeros(classifier.num_sets).to(device)\n",
    "set_acc = torch.zeros(classifier.num_sets).to(device)\n",
    "for xx, yy in tqdm(train_loader):\n",
    "    xx, yy = xx.to(device), yy.to(device)\n",
    "    with torch.no_grad():\n",
    "        yout = classifier(model(xx), hard=True)\n",
    "        \n",
    "    cls_indx = torch.argmax(classifier.cls_confidence, dim=1)\n",
    "    set_indx, count = torch.unique(cls_indx, return_counts=True) \n",
    "    set_count[set_indx] += count\n",
    "    \n",
    "    outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "    correct = (outputs == yy.data.cpu().numpy()).astype(float)\n",
    "    \n",
    "    ### class_index has 100 possible values\n",
    "    for i, c in enumerate(correct):\n",
    "        set_acc[cls_indx[i]] += c\n",
    "    \n",
    "#     print(set_acc.sum(), set_count.sum())\n",
    "#     break\n",
    "    test_acc += correct.sum()\n",
    "    test_count += len(xx)\n",
    "\n",
    "print(f'Hard Train Acc:{float(test_acc)/test_count*100:.2f}%')\n",
    "print(set_count.type(torch.long).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8973,    nan, 0.8711, 0.5000, 0.8110, 1.0000, 0.6168, 0.8953, 0.8864,\n",
       "        0.9000, 0.9609, 0.9412, 0.8197, 0.6769, 0.8430, 0.9874, 0.5723, 0.8333,\n",
       "        0.8421,    nan, 0.8644,    nan, 0.9624, 0.7560, 0.6667, 0.9863, 0.3636,\n",
       "        0.9580, 0.9838, 0.9725, 0.9472,    nan, 0.8754, 0.7040, 0.5000, 1.0000,\n",
       "        0.7143, 0.8980, 1.0000, 0.9673, 0.8709, 0.9981, 0.8782, 0.8488, 0.6624,\n",
       "        0.9355, 0.7967, 1.0000, 0.9940, 0.4000, 0.8707, 0.9887, 0.8418, 0.9484,\n",
       "        0.0000, 0.9961, 0.8860, 0.9760, 0.8298, 0.9645, 0.7769, 0.9737, 0.8068,\n",
       "        0.9379, 0.6264, 0.9820, 0.5882, 0.9772, 0.9786, 0.9231,    nan, 0.9968,\n",
       "        0.7350, 0.6709, 0.7774, 0.9365, 0.6667, 0.8241, 0.8889, 0.9609, 0.8990,\n",
       "           nan, 0.1250, 0.9278, 0.6132, 1.0000, 0.8869, 0.8136, 0.9851, 0.9062,\n",
       "        0.9286, 0.9978, 0.8095, 0.9512, 0.8706, 0.9969, 0.7087, 0.7447, 0.9083,\n",
       "        0.7820], device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_acc/set_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,\t 409,\t 0\t 89.73%\n",
      "2,\t 318,\t 2\t 87.11%\n",
      "3,\t 18,\t 3\t 50.00%\n",
      "4,\t 3561,\t 4\t 81.10%\n",
      "5,\t 2,\t 5\t 100.00%\n",
      "6,\t 368,\t 6\t 61.68%\n",
      "7,\t 86,\t 7\t 89.53%\n",
      "8,\t 44,\t 8\t 88.64%\n",
      "9,\t 60,\t 9\t 90.00%\n",
      "10,\t 2173,\t 0\t 96.09%\n",
      "11,\t 17,\t 1\t 94.12%\n",
      "12,\t 588,\t 2\t 81.97%\n",
      "13,\t 65,\t 3\t 67.69%\n",
      "14,\t 242,\t 4\t 84.30%\n",
      "15,\t 238,\t 5\t 98.74%\n",
      "16,\t 788,\t 6\t 57.23%\n",
      "17,\t 66,\t 7\t 83.33%\n",
      "18,\t 19,\t 8\t 84.21%\n",
      "20,\t 59,\t 0\t 86.44%\n",
      "22,\t 213,\t 2\t 96.24%\n",
      "23,\t 250,\t 3\t 75.60%\n",
      "24,\t 27,\t 4\t 66.67%\n",
      "25,\t 1823,\t 5\t 98.63%\n",
      "26,\t 11,\t 6\t 36.36%\n",
      "27,\t 595,\t 7\t 95.80%\n",
      "28,\t 1173,\t 8\t 98.38%\n",
      "29,\t 3158,\t 9\t 97.25%\n",
      "30,\t 265,\t 0\t 94.72%\n",
      "32,\t 1260,\t 2\t 87.54%\n",
      "33,\t 125,\t 3\t 70.40%\n",
      "34,\t 72,\t 4\t 50.00%\n",
      "35,\t 23,\t 5\t 100.00%\n",
      "36,\t 7,\t 6\t 71.43%\n",
      "37,\t 49,\t 7\t 89.80%\n",
      "38,\t 11,\t 8\t 100.00%\n",
      "39,\t 397,\t 9\t 96.73%\n",
      "40,\t 457,\t 0\t 87.09%\n",
      "41,\t 2083,\t 1\t 99.81%\n",
      "42,\t 2972,\t 2\t 87.82%\n",
      "43,\t 701,\t 3\t 84.88%\n",
      "44,\t 157,\t 4\t 66.24%\n",
      "45,\t 31,\t 5\t 93.55%\n",
      "46,\t 2750,\t 6\t 79.67%\n",
      "47,\t 5,\t 7\t 100.00%\n",
      "48,\t 1994,\t 8\t 99.40%\n",
      "49,\t 5,\t 9\t 40.00%\n",
      "50,\t 116,\t 0\t 87.07%\n",
      "51,\t 1421,\t 1\t 98.87%\n",
      "52,\t 196,\t 2\t 84.18%\n",
      "53,\t 891,\t 3\t 94.84%\n",
      "54,\t 3,\t 4\t 0.00%\n",
      "55,\t 507,\t 5\t 99.61%\n",
      "56,\t 1342,\t 6\t 88.60%\n",
      "57,\t 707,\t 7\t 97.60%\n",
      "58,\t 47,\t 8\t 82.98%\n",
      "59,\t 1212,\t 9\t 96.45%\n",
      "60,\t 130,\t 0\t 77.69%\n",
      "61,\t 76,\t 1\t 97.37%\n",
      "62,\t 88,\t 2\t 80.68%\n",
      "63,\t 338,\t 3\t 93.79%\n",
      "64,\t 91,\t 4\t 62.64%\n",
      "65,\t 222,\t 5\t 98.20%\n",
      "66,\t 68,\t 6\t 58.82%\n",
      "67,\t 4301,\t 7\t 97.72%\n",
      "68,\t 373,\t 8\t 97.86%\n",
      "69,\t 39,\t 9\t 92.31%\n",
      "71,\t 1870,\t 1\t 99.68%\n",
      "72,\t 117,\t 2\t 73.50%\n",
      "73,\t 79,\t 3\t 67.09%\n",
      "74,\t 292,\t 4\t 77.74%\n",
      "75,\t 126,\t 5\t 93.65%\n",
      "76,\t 3,\t 6\t 66.67%\n",
      "77,\t 108,\t 7\t 82.41%\n",
      "78,\t 18,\t 8\t 88.89%\n",
      "79,\t 997,\t 9\t 96.09%\n",
      "80,\t 1614,\t 0\t 89.90%\n",
      "82,\t 8,\t 2\t 12.50%\n",
      "83,\t 1717,\t 3\t 92.78%\n",
      "84,\t 243,\t 4\t 61.32%\n",
      "85,\t 20,\t 5\t 100.00%\n",
      "86,\t 389,\t 6\t 88.69%\n",
      "87,\t 59,\t 7\t 81.36%\n",
      "88,\t 2211,\t 8\t 98.51%\n",
      "89,\t 96,\t 9\t 90.62%\n",
      "90,\t 14,\t 0\t 92.86%\n",
      "91,\t 452,\t 1\t 99.78%\n",
      "92,\t 84,\t 2\t 80.95%\n",
      "93,\t 2009,\t 3\t 95.12%\n",
      "94,\t 2009,\t 4\t 87.06%\n",
      "95,\t 2892,\t 5\t 99.69%\n",
      "96,\t 381,\t 6\t 70.87%\n",
      "97,\t 47,\t 7\t 74.47%\n",
      "98,\t 109,\t 8\t 90.83%\n",
      "99,\t 133,\t 9\t 78.20%\n"
     ]
    }
   ],
   "source": [
    "for i, (cnt, acc, cls) in enumerate(zip(set_count.type(torch.long).tolist(),\n",
    "                                   (set_acc/set_count).tolist(),\n",
    "                                   torch.argmax(classifier.cls_weight, dim=1).tolist())):\n",
    "    if cnt == 0: continue\n",
    "    print(f\"{i},\\t {cnt},\\t {cls}\\t {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
