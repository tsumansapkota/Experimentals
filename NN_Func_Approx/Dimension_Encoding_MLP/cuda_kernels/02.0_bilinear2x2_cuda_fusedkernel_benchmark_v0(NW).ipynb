{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import datasets\n",
    "import random, sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cuda:1\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "test_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"../../../../_Datasets/FMNIST/\", train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.FashionMNIST(root=\"../../../../_Datasets/FMNIST/\", train=False, download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "BS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BS, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BS, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## demo of train loader\n",
    "xx, yy = iter(train_loader).next()\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda -bmm2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bmm2x2_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMM2x2Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "#     @torch.jit.ignore\n",
    "    def forward(ctx, inputs, weights):\n",
    "        outputs = bmm2x2_cuda.forward(inputs, weights)\n",
    "        ctx.save_for_backward(inputs, weights)\n",
    "        return outputs[0]\n",
    "    \n",
    "    @staticmethod\n",
    "#     @torch.jit.ignore\n",
    "    def backward(ctx, grad_output):\n",
    "        inputs, weights = ctx.saved_tensors\n",
    "        del_input, del_weights = bmm2x2_cuda.backward(\n",
    "            inputs, \n",
    "            weights, \n",
    "            grad_output)\n",
    "    \n",
    "        return del_input, del_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairWeight2(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        assert input_dim%2 == 0, \"Input dim must be even number\"\n",
    "        self.weight = torch.eye(2).unsqueeze(0).repeat_interleave(input_dim//2, dim=0)\n",
    "        self.weight = nn.Parameter(self.weight)\n",
    "        self.bmmfunc = BMM2x2Function()\n",
    "        \n",
    "    @torch.jit.ignore\n",
    "    def bmm(self, x, w):\n",
    "        return BMM2x2Function.apply(x, w)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, dim = x.shape[0], x.shape[1]\n",
    "        x = x.view(bs, -1, 2)\n",
    "        x = self.bmm(x, self.weight)\n",
    "        x = x.view(bs, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda - Bilinear2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bilinear2x2_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLinear2x2Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, weights):\n",
    "        outputs = bilinear2x2_cuda.forward(inputs, weights)\n",
    "        ctx.save_for_backward(inputs, weights)\n",
    "        return outputs[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        inputs, weights = ctx.saved_tensors\n",
    "#         del_input, del_weights = bmm2x2_cuda.backward(\n",
    "#             grad_output.contiguous(), \n",
    "#             grad_cell.contiguous(), \n",
    "#             grad_output.contiguous())\n",
    "        del_input, del_weights = bilinear2x2_cuda.backward(\n",
    "            inputs, \n",
    "            weights, \n",
    "            grad_output)\n",
    "    \n",
    "        return del_input, del_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairBilinear2(nn.Module):\n",
    "    def __init__(self, dim, grid_width):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.grid_width = grid_width\n",
    "        \n",
    "        self.num_pairs = self.dim // 2\n",
    "        along_row = torch.linspace(0, 1, self.grid_width).reshape(1, -1).t()\n",
    "        along_col = torch.linspace(0, 1, self.grid_width).reshape(-1, 1).t()\n",
    "        \n",
    "        self.Y = torch.stack([along_row+along_col*0, along_row*0+along_col])\n",
    "        self.Y = torch.repeat_interleave(self.Y.unsqueeze(0), self.num_pairs, dim=0)\n",
    "        self.Y = nn.Parameter(self.Y)\n",
    "        \n",
    "        self.pairW = torch.eye(2).unsqueeze(0).repeat_interleave(self.num_pairs, dim=0)\n",
    "        self.pairW = nn.Parameter(self.pairW)\n",
    "    \n",
    "#     @torch.jit.ignore\n",
    "#     def pairbl2x2(self, x, w):\n",
    "#         return BiLinear2x2Function.apply(x, w)\n",
    "    \n",
    "#     @torch.jit.ignore\n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        \n",
    "############# This block ########################\n",
    "        ### this block is significantly faster\n",
    "    \n",
    "#         x = x.view(bs, -1, 2).transpose(0,1)\n",
    "#         x = torch.bmm(x, self.pairW)\n",
    "#         x = x.transpose(1,0)#.reshape(-1, 2)\n",
    "        \n",
    "############# OR This block ########################\n",
    "        x = x.view(bs, -1, 2)\n",
    "#         x = BMM2x2Function.apply(x, self.pairW)\n",
    "####################################################\n",
    "#         x = x.view(bs, -1, 2)\n",
    "        x = BiLinear2x2Function.apply(x, self.Y)\n",
    "        x = x.view(bs, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fused 2x2 Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fused2x2ops_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedBiLinear2x2Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, weights, grids):\n",
    "        outputs, input_buffer, dbg = fused2x2ops_cuda.bilinear2x2_forward(inputs, weights, grids)\n",
    "#         print(dbg)\n",
    "\n",
    "#         outputs, input_buffer = fused2x2ops_cuda.bilinear2x2_forward(inputs, weights, grids)\n",
    "        ctx.save_for_backward(input_buffer, weights, grids)\n",
    "        return outputs, dbg\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output, a):\n",
    "        input_buffer, weights, grids = ctx.saved_tensors\n",
    "#         del_input, del_weights = bmm2x2_cuda.backward(\n",
    "#             grad_output.contiguous(), \n",
    "#             grad_cell.contiguous(), \n",
    "#             grad_output.contiguous())\n",
    "        del_input, del_weights, del_grids = fused2x2ops_cuda.bilinear2x2_backward(\n",
    "            input_buffer, \n",
    "            weights, \n",
    "            grids,\n",
    "            grad_output)\n",
    "    \n",
    "        return del_input, del_weights, del_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fused2x2BiLinear(nn.Module):\n",
    "    def __init__(self, dim, grid_width, num_layers = None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.grid_width = grid_width\n",
    "        \n",
    "        self.num_layers = int(np.ceil(np.log2(self.dim)))\n",
    "        if num_layers is not None:\n",
    "            self.num_layers = num_layers\n",
    "        \n",
    "        \n",
    "        self.num_pairs = self.dim // 2\n",
    "        \n",
    "#         along_row = torch.linspace(0, 1, self.grid_width).reshape(1, -1)\n",
    "#         along_col = torch.linspace(0, 1, self.grid_width).reshape(-1, 1)\n",
    "        \n",
    "        along_row = torch.linspace(0, 1, self.grid_width).reshape(1, -1).t()\n",
    "        along_col = torch.linspace(0, 1, self.grid_width).reshape(-1, 1).t()\n",
    "        \n",
    "        self.Y = torch.stack([along_row+along_col*0, along_row*0+along_col])\n",
    "        \n",
    "        ### repeat same for num_pairs\n",
    "        self.Y = torch.repeat_interleave(self.Y.unsqueeze(0), self.num_pairs, dim=0)\n",
    "        ### repeat same for num_Layers\n",
    "        self.Y = torch.repeat_interleave(self.Y.unsqueeze(0), self.num_layers, dim=0)\n",
    "        \n",
    "        print(self.Y.shape)\n",
    "        self.Y = nn.Parameter(self.Y)\n",
    "        \n",
    "        ### repeat same for num_pairs\n",
    "        self.pairW = torch.eye(2).unsqueeze(0).repeat_interleave(self.num_pairs, dim=0)\n",
    "        ### repeat same for num_Layers\n",
    "        self.pairW = self.pairW.unsqueeze(0).repeat_interleave(self.num_layers, dim=0)\n",
    "        \n",
    "        print(self.pairW.shape)\n",
    "        self.pairW = nn.Parameter(self.pairW)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, self.dbg = FusedBiLinear2x2Function.apply(x.clone(), self.pairW, self.Y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2, 3, 3])\n",
      "torch.Size([4, 8, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "N = 16\n",
    "fbl = Fused2x2BiLinear(N, 3, num_layers=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, N).to(device)\n",
    "y = fbl(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0.,  1.],\n",
       "          [ 1.,  2.,  3.],\n",
       "          [ 2.,  4.,  5.],\n",
       "          [ 3.,  6.,  7.],\n",
       "          [ 4.,  8.,  9.],\n",
       "          [ 5., 10., 11.],\n",
       "          [ 6., 12., 13.],\n",
       "          [ 7., 14., 15.]]],\n",
       "\n",
       "\n",
       "        [[[ 0.,  0.,  2.],\n",
       "          [ 1.,  1.,  3.],\n",
       "          [ 2.,  4.,  6.],\n",
       "          [ 3.,  5.,  7.],\n",
       "          [ 4.,  8., 10.],\n",
       "          [ 5.,  9., 11.],\n",
       "          [ 6., 12., 14.],\n",
       "          [ 7., 13., 15.]]],\n",
       "\n",
       "\n",
       "        [[[ 0.,  0.,  4.],\n",
       "          [ 1.,  1.,  5.],\n",
       "          [ 2.,  2.,  6.],\n",
       "          [ 3.,  3.,  7.],\n",
       "          [ 4.,  8., 12.],\n",
       "          [ 5.,  9., 13.],\n",
       "          [ 6., 10., 14.],\n",
       "          [ 7., 11., 15.]]],\n",
       "\n",
       "\n",
       "        [[[ 0.,  0.,  8.],\n",
       "          [ 1.,  1.,  9.],\n",
       "          [ 2.,  2., 10.],\n",
       "          [ 3.,  3., 11.],\n",
       "          [ 4.,  4., 12.],\n",
       "          [ 5.,  5., 13.],\n",
       "          [ 6.,  6., 14.],\n",
       "          [ 7.,  7., 15.]]]], device='cuda:0',\n",
       "       grad_fn=<FusedBiLinear2x2FunctionBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.dbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0081, -0.7993,  0.0350,  1.5818,  0.6688, -0.9515,  1.2559,  0.4520,\n",
       "         -2.5717,  0.0262, -1.1490, -0.1717, -0.7478, -1.1236, -1.2404, -0.7634]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0081, -0.7993,  0.0350,  1.5818,  0.6688, -0.9515,  1.2559,  0.4520,\n",
       "         -2.5717,  0.0262, -1.1490, -0.1717, -0.7478, -1.1236, -1.2404, -0.7634]],\n",
       "       device='cuda:0', grad_fn=<FusedBiLinear2x2FunctionBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000],\n",
       "           [0.5000, 0.5000, 0.5000],\n",
       "           [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "          [[0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000],\n",
       "           [0.0000, 0.5000, 1.0000]]]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbl = PairBilinear2(16, 3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.5000],\n",
       "          [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000],\n",
       "          [0.0000, 0.5000, 1.0000]]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbl.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = pbl(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0081, -0.7993,  0.0350,  1.5818,  0.6688, -0.9515,  1.2559,  0.4520,\n",
       "         -2.5717,  0.0262, -1.1490, -0.1717, -0.7478, -1.1236, -1.2404, -0.7634]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0081, -0.7993,  0.0350,  1.5818,  0.6688, -0.9515,  1.2559,  0.4520,\n",
       "         -2.5717,  0.0262, -1.1490, -0.1717, -0.7478, -1.1236, -1.2404, -0.7634]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -0.0676,  0.1257],\n",
       "         [ 0.0000, -0.0051,  0.0095],\n",
       "         [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000, -0.0676,  0.1257],\n",
       "         [ 0.0000, -0.0051,  0.0095],\n",
       "         [ 0.0000,  0.0000,  0.0000]]], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.Y.grad[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -0.0676,  0.1257],\n",
       "         [ 0.0000, -0.0051,  0.0095],\n",
       "         [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000, -0.0676,  0.1257],\n",
       "         [ 0.0000, -0.0051,  0.0095],\n",
       "         [ 0.0000,  0.0000,  0.0000]]], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbl.Y.grad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fbl.Y.grad[0] - pbl.Y.grad.transpose(-1, -2)\n",
    "fbl.Y.grad[0] - pbl.Y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-1082f901338f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "pbl.pairW.grad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.pairW.grad[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbl.pairW[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.pairW[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedPairBilinearBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, grid_width):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_layers = int(np.ceil(np.log2(input_dim)))\n",
    "        \n",
    "        extra =  2**self.num_layers - input_dim\n",
    "        torch.manual_seed(123)\n",
    "        self.selector = torch.randperm(self.input_dim)[:extra]\n",
    "        \n",
    "        self.fused_pair_bilinear = Fused2x2BiLinear(2**self.num_layers, grid_width=grid_width, )#num_layers=5)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x should have dimension -> bs, M\n",
    "        '''\n",
    "        x = torch.cat((x, x[:, self.selector]), dim=1)\n",
    "        return self.fused_pair_bilinear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 2, 3, 3])\n",
      "torch.Size([10, 512, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "fpbl = FusedPairBilinearBlock(784, 3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1024])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = fpbl(torch.randn(10, 784).to(device))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules and Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasLayer(nn.Module):\n",
    "    def __init__(self, dim, init_val=0):\n",
    "        super().__init__()\n",
    "        self.bias = nn.Parameter(torch.ones(dim)*init_val)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x+self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizedPairBilinearSpline(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, grid_width):\n",
    "        super().__init__()\n",
    "        assert input_dim%2 == 0, \"Input dim must be even number\"\n",
    "        self.input_dim = input_dim\n",
    "        self.num_layers = int(np.ceil(np.log2(input_dim)))\n",
    "            \n",
    "        self.facto_nets = []\n",
    "        self.idx_revidx = []\n",
    "        for i in range(self.num_layers):\n",
    "            idrid = self.get_pair(self.input_dim, i+1)\n",
    "            net = PairBilinear2(self.input_dim, grid_width)\n",
    "            self.facto_nets.append(net)\n",
    "            self.idx_revidx.append(idrid)\n",
    "        self.facto_nets = nn.ModuleList(self.facto_nets)\n",
    "            \n",
    "#     @torch.jit.ignore\n",
    "    def get_pair(self, inp_dim, step=1):\n",
    "        dim = 2**int(np.ceil(np.log2(inp_dim)))\n",
    "        assert isinstance(step, int), \"Step must be integer\"\n",
    "\n",
    "        blocks = (2**step)\n",
    "        range_ = dim//blocks\n",
    "        adder_ = torch.arange(0, range_)*blocks\n",
    "\n",
    "        pairs_ = torch.Tensor([0, blocks//2])\n",
    "        repeat_ = torch.arange(0, blocks//2).reshape(-1,1)\n",
    "        block_map = (pairs_+repeat_).reshape(-1)\n",
    "\n",
    "        reorder_for_pair = (block_map+adder_.reshape(-1,1)).reshape(-1)\n",
    "        indx = reorder_for_pair.type(torch.long)\n",
    "        indx = indx[indx<inp_dim]\n",
    "\n",
    "        rev_indx = torch.argsort(indx)\n",
    "        return indx, rev_indx\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## swap first and then forward and reverse-swap\n",
    "        y = x\n",
    "#         for i in range(len(self.facto_nets)):\n",
    "        for i, fn in enumerate(self.facto_nets):\n",
    "            idx, revidx = self.idx_revidx[i]\n",
    "            y = y[:, idx]\n",
    "            y = fn(y) \n",
    "            y = y[:, revidx]\n",
    "#         y = x + y ## this is residual addition... remove if only want feed forward\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairBilinearBlock(FactorizedPairBilinearSpline):\n",
    "    \n",
    "    def __init__(self, input_dim, grid_width):\n",
    "        num_layers = int(np.ceil(np.log2(input_dim)))\n",
    "        extra =  2**num_layers - input_dim\n",
    "        torch.manual_seed(123)\n",
    "        self.selector = torch.randperm(input_dim)[:extra]\n",
    "        \n",
    "        super().__init__(2**num_layers, grid_width)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x should have dimension -> bs, M\n",
    "        '''\n",
    "        x = torch.cat((x, x[:, self.selector]), dim=1)\n",
    "        return super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizedPairBilinearSpline_2(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, grid_width):\n",
    "        super().__init__()\n",
    "        assert input_dim%2 == 0, \"Input dim must be even number\"\n",
    "        self.input_dim = input_dim\n",
    "        self.num_layers = int(np.ceil(np.log2(input_dim)))\n",
    "            \n",
    "        self.facto_nets = []\n",
    "        for i in range(self.num_layers):\n",
    "            net = PairBilinear2(self.input_dim, grid_width)\n",
    "            self.facto_nets.append(net)\n",
    "        self.facto_nets = nn.ModuleList(self.facto_nets)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        ## swap first and then forward and reverse-swap\n",
    "        bs = x.shape[0]\n",
    "        y = x\n",
    "#         for i in range(len(self.facto_nets)):\n",
    "        for i, fn in enumerate(self.facto_nets):\n",
    "#             y = y.view(-1,2,2**(i+1)).permute(0, 2,1).contiguous().view(bs, -1)\n",
    "            y = y.view(-1,2,2**i).permute(0, 2,1).contiguous().view(bs, -1)\n",
    "            y = fn(y) \n",
    "#             y = y.view(-1,2**(i+1),2).permute(0, 2,1).contiguous()\n",
    "            y = y.view(-1,2**i,2).permute(0, 2,1).contiguous()\n",
    "#         y = x + y ## this is residual addition... remove if only want feed forward\n",
    "        return y.view(bs, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairBilinearBlock_2(FactorizedPairBilinearSpline_2):\n",
    "    \n",
    "    def __init__(self, input_dim, grid_width):\n",
    "        num_layers = int(np.ceil(np.log2(input_dim)))\n",
    "        extra =  2**num_layers - input_dim\n",
    "        torch.manual_seed(123)\n",
    "        self.selector = torch.randperm(input_dim)[:extra]\n",
    "        \n",
    "        super().__init__(2**num_layers, grid_width)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x should have dimension -> bs, M\n",
    "        '''\n",
    "        x = torch.cat((x, x[:, self.selector]), dim=1)\n",
    "        return super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfL = FactorizedPairBilinearSpline(784, 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5040,  2.9259, -0.2752,  ...,  0.8611, -1.0041,  0.2877],\n",
       "        [ 0.1645, -0.7405, -0.9497,  ...,  0.4982,  0.1717,  0.2237],\n",
       "        [-1.8787, -0.4780, -1.1443,  ...,  1.0046,  0.4900,  0.2338],\n",
       "        ...,\n",
       "        [-1.2427,  0.2971,  0.4793,  ..., -1.9474, -0.2935,  1.4595],\n",
       "        [ 0.4139, -0.8466, -0.0513,  ..., -0.5742, -0.8309,  0.6456],\n",
       "        [-1.3414,  0.4671, -1.7051,  ...,  0.7226,  0.0492,  0.7175]],\n",
       "       device='cuda:0', grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfL(torch.randn(100, 784).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "_a = torch.randn(100, 784).to(device)\n",
    "\n",
    "# %timeit pfL(_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): PairBilinear2()\n",
       "  (1): PairBilinear2()\n",
       "  (2): PairBilinear2()\n",
       "  (3): PairBilinear2()\n",
       "  (4): PairBilinear2()\n",
       "  (5): PairBilinear2()\n",
       "  (6): PairBilinear2()\n",
       "  (7): PairBilinear2()\n",
       "  (8): PairBilinear2()\n",
       "  (9): PairBilinear2()\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfL.facto_nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799680"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_count = sum([torch.numel(p) for p in pfL.parameters()])\n",
    "param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bias = BiasLayer(784)\n",
    "        self.la1 = FactorizedPairBilinearSpline(784, grid_width=3)\n",
    "        self.bn1 = nn.BatchNorm1d(784)\n",
    "        self.fc = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bias(x)\n",
    "        x = self.la1(x)\n",
    "#         x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        H = 16\n",
    "        self.bias = nn.Linear(784, H)\n",
    "        self.bn1 = nn.BatchNorm1d(H)\n",
    "        self.fc = nn.Linear(H, 10)\n",
    "        self.la1 = FusedPairBilinearBlock(H, grid_width=3)\n",
    "#         self.la1 = PairBilinearBlock_2(H, grid_width=3)\n",
    "\n",
    "#         self.bias = BiasLayer(784)\n",
    "#         self.la1 = FusedPairBilinearBlock(784, grid_width=3)\n",
    "# #         self.la1 = PairBilinearBlock(784, grid_width=3)\n",
    "#         self.bn1 = nn.BatchNorm1d(1024)\n",
    "#         self.fc = nn.Linear(1024, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bias(x)\n",
    "        x = self.la1(x)\n",
    "#         x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class FactorNet2_debug(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        H = 16\n",
    "        self.bias = nn.Linear(784, H)\n",
    "        self.bn1 = nn.BatchNorm1d(H)\n",
    "        self.fc = nn.Linear(H, 10)\n",
    "        self.la1 = PairBilinearBlock_2(H, grid_width=3)\n",
    "\n",
    "#         self.bias = BiasLayer(784)\n",
    "# #         self.la1 = FusedPairBilinearBlock(784, grid_width=3)\n",
    "#         self.la1 = PairBilinearBlock_2(784, grid_width=3)\n",
    "#         self.bn1 = nn.BatchNorm1d(1024)\n",
    "#         self.fc = nn.Linear(1024, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bias(x)\n",
    "        x = self.la1(x)\n",
    "#         x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinaryNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.la1 = nn.Linear(784, 784, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(784)\n",
    "        self.la2 = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.la1(x))\n",
    "        x = torch.relu(x)\n",
    "        x = self.la2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2, 3, 3])\n",
      "torch.Size([4, 8, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13466"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FactorNet2()\n",
    "param_count = sum([torch.numel(p) for p in model.parameters()])\n",
    "param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.la1.fused_pair_bilinear.Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(624074, 46.34442299123719)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OrdinaryNet()\n",
    "param_count1 = sum([torch.numel(p) for p in model.parameters()])\n",
    "param_count1, param_count1/param_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2, 3, 3])\n",
      "torch.Size([4, 8, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FactorNet2(\n",
       "  (bias): Linear(in_features=784, out_features=16, bias=True)\n",
       "  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc): Linear(in_features=16, out_features=10, bias=True)\n",
       "  (la1): FusedPairBilinearBlock(\n",
       "    (fused_pair_bilinear): Fused2x2BiLinear()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = FactorNet2().to(device)\n",
    "# model = OrdinaryNet().to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorNet2_debug(\n",
       "  (bias): Linear(in_features=784, out_features=16, bias=True)\n",
       "  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc): Linear(in_features=16, out_features=10, bias=True)\n",
       "  (la1): PairBilinearBlock_2(\n",
       "    (facto_nets): ModuleList(\n",
       "      (0): PairBilinear2()\n",
       "      (1): PairBilinear2()\n",
       "      (2): PairBilinear2()\n",
       "      (3): PairBilinear2()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model_deb = FactorNet2_debug().to(device)\n",
    "model_deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_deb = torch.optim.Adam(model_deb.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  13466\n"
     ]
    }
   ],
   "source": [
    "print(\"number of params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 784])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([10, 16])\n",
      "torch.Size([10])\n",
      "torch.Size([4, 8, 2, 3, 3])\n",
      "torch.Size([4, 8, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  13466\n"
     ]
    }
   ],
   "source": [
    "print(\"number of params: \", sum(p.numel() for p in model_deb.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 784])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([10, 16])\n",
      "torch.Size([10])\n",
      "torch.Size([8, 2, 3, 3])\n",
      "torch.Size([8, 2, 2])\n",
      "torch.Size([8, 2, 3, 3])\n",
      "torch.Size([8, 2, 2])\n",
      "torch.Size([8, 2, 3, 3])\n",
      "torch.Size([8, 2, 2])\n",
      "torch.Size([8, 2, 3, 3])\n",
      "torch.Size([8, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "for p in model_deb.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.bias.weight.data - model_deb.bias.weight.data).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.bias.bias.data - model_deb.bias.bias.data).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.bn1.weight.data - model_deb.bn1.weight.data).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.bn1.bias.data - model_deb.bn1.bias.data).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.fc.weight.data - model_deb.fc.weight.data).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.fc.bias.data - model_deb.fc.bias.data).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 8/300 [00:00<00:06, 42.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Diff: 0.0\n",
      "1: Diff: 0.0\n",
      "2: Diff: 1.2451782938072142e-09\n",
      "3: Diff: 2.392567699516235e-09\n",
      "4: Diff: 4.7832728888863585e-09\n",
      "5: Diff: 5.233101951773733e-09\n",
      "6: Diff: 6.3853806686609005e-09\n",
      "7: Diff: 6.981892841650961e-09\n",
      "8: Diff: 8.173286936141722e-09\n",
      "9: Diff: 8.870848056119485e-09\n",
      "10: Diff: 8.894130765213504e-09\n",
      "11: Diff: 1.0334188615956919e-08\n",
      "12: Diff: 1.1385418829945593e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 22/300 [00:00<00:04, 55.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13: Diff: 1.2611039323928708e-08\n",
      "14: Diff: 1.292815454689844e-08\n",
      "15: Diff: 1.3666228149133985e-08\n",
      "16: Diff: 1.4203600962048313e-08\n",
      "17: Diff: 1.7596642365447224e-08\n",
      "18: Diff: 1.652189673961857e-08\n",
      "19: Diff: 1.871678989573411e-08\n",
      "20: Diff: 2.0723790683518928e-08\n",
      "21: Diff: 2.0030887171174072e-08\n",
      "22: Diff: 2.1864195787202334e-08\n",
      "23: Diff: 2.4599025039151456e-08\n",
      "24: Diff: 2.319878156242794e-08\n",
      "25: Diff: 2.590334169383368e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 38/300 [00:00<00:04, 64.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26: Diff: 2.742931393129311e-08\n",
      "27: Diff: 2.8549926867071918e-08\n",
      "28: Diff: 2.7035364169591958e-08\n",
      "29: Diff: 3.270851678394138e-08\n",
      "30: Diff: 3.241282087174113e-08\n",
      "31: Diff: 3.1503383013387065e-08\n",
      "32: Diff: 3.513880386663004e-08\n",
      "33: Diff: 3.33590435275255e-08\n",
      "34: Diff: 3.792089486864825e-08\n",
      "35: Diff: 4.3436422458853485e-08\n",
      "36: Diff: 4.1459223609763285e-08\n",
      "37: Diff: 4.6123751928917045e-08\n",
      "38: Diff: 4.665297836936588e-08\n",
      "39: Diff: 5.143881054436861e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 52/300 [00:00<00:03, 65.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40: Diff: 5.1418322044582965e-08\n",
      "41: Diff: 5.696528049270455e-08\n",
      "42: Diff: 6.182119705044897e-08\n",
      "43: Diff: 6.904267024765431e-08\n",
      "44: Diff: 6.956420861570223e-08\n",
      "45: Diff: 7.118936906636009e-08\n",
      "46: Diff: 8.006813345673436e-08\n",
      "47: Diff: 8.258177075504136e-08\n",
      "48: Diff: 1.0021916097002759e-07\n",
      "49: Diff: 9.863451566616277e-08\n",
      "50: Diff: 1.0347552859002462e-07\n",
      "51: Diff: 1.0620477297607067e-07\n",
      "52: Diff: 1.3457332670441247e-07\n",
      "53: Diff: 1.4613476650993107e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 66/300 [00:01<00:03, 63.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54: Diff: 1.6478171005473996e-07\n",
      "55: Diff: 1.8965826598105195e-07\n",
      "56: Diff: 2.2539497024354205e-07\n",
      "57: Diff: 2.688025233510416e-07\n",
      "58: Diff: 2.7721515039047517e-07\n",
      "59: Diff: 3.163497979130625e-07\n",
      "60: Diff: 3.340910268434527e-07\n",
      "61: Diff: 3.3704100133036263e-07\n",
      "62: Diff: 3.367140948284941e-07\n",
      "63: Diff: 3.7695701848861063e-07\n",
      "64: Diff: 3.8332353824444e-07\n",
      "65: Diff: 4.0258188960251573e-07\n",
      "66: Diff: 4.623546487891872e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 80/300 [00:01<00:03, 63.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67: Diff: 4.460611648937629e-07\n",
      "68: Diff: 4.431917659530882e-07\n",
      "69: Diff: 5.708146204597142e-07\n",
      "70: Diff: 5.321242042555241e-07\n",
      "71: Diff: 6.219912620508694e-07\n",
      "72: Diff: 6.085001018618641e-07\n",
      "73: Diff: 6.348551551127457e-07\n",
      "74: Diff: 6.33542924788344e-07\n",
      "75: Diff: 7.215524533421558e-07\n",
      "76: Diff: 7.80806885813945e-07\n",
      "77: Diff: 6.846651103842305e-07\n",
      "78: Diff: 6.740010007888486e-07\n",
      "79: Diff: 7.414836886709963e-07\n",
      "80: Diff: 8.448935773230914e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 94/300 [00:01<00:03, 62.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81: Diff: 7.063681550789624e-07\n",
      "82: Diff: 9.134775496022485e-07\n",
      "83: Diff: 9.107887990467134e-07\n",
      "84: Diff: 7.783780233694415e-07\n",
      "85: Diff: 8.459687705908436e-07\n",
      "86: Diff: 1.0565156571828993e-06\n",
      "87: Diff: 9.718090723254136e-07\n",
      "88: Diff: 9.842761983236414e-07\n",
      "89: Diff: 1.1517340681166388e-06\n",
      "90: Diff: 1.1747778216886218e-06\n",
      "91: Diff: 1.241836798726581e-06\n",
      "92: Diff: 1.4881474044159404e-06\n",
      "93: Diff: 1.3483172551786993e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▎      | 101/300 [00:01<00:03, 61.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94: Diff: 1.5437240108440164e-06\n",
      "95: Diff: 1.4906592014085618e-06\n",
      "96: Diff: 1.5745815744594438e-06\n",
      "97: Diff: 1.2384556384859025e-06\n",
      "98: Diff: 1.3709920949622756e-06\n",
      "99: Diff: 1.2577017969306326e-06\n",
      "100: Diff: 1.2021577049381449e-06\n",
      "101: Diff: 1.3562711274062167e-06\n",
      "102: Diff: 1.1947472557949368e-06\n",
      "103: Diff: 1.323953370047093e-06\n",
      "104: Diff: 1.4028270243215957e-06\n",
      "105: Diff: 1.1558645383047406e-06\n",
      "106: Diff: 1.3995469316796516e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 113/300 [00:01<00:03, 58.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107: Diff: 1.5992840189937851e-06\n",
      "108: Diff: 1.751044351294695e-06\n",
      "109: Diff: 1.472750795983302e-06\n",
      "110: Diff: 1.6953973727140692e-06\n",
      "111: Diff: 1.6320394706781371e-06\n",
      "112: Diff: 1.7367387954436708e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-210510d989c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train_acc = 0\n",
    "    train_count = 0\n",
    "    i = -1\n",
    "    for xx, yy in tqdm(train_loader):\n",
    "        i += 1 \n",
    "        xx = xx.view(xx.shape[0], -1)\n",
    "        xx, yy = xx.to(device), yy.to(device)\n",
    "\n",
    "        yout = model(xx)\n",
    "        loss = criterion(yout, yy)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        yout_ = model_deb(xx)\n",
    "        loss_ = criterion(yout_, yy)\n",
    "        optimizer_deb.zero_grad()\n",
    "        loss_.backward()\n",
    "        optimizer_deb.step()\n",
    "\n",
    "        losses.append(float(loss))\n",
    "\n",
    "        outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "        correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "        train_acc += correct\n",
    "        train_count += len(outputs)\n",
    "        \n",
    "        ########3 Find Similarities and Differences @@@@@@@@@@\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            diff = yout - yout_\n",
    "            print(f\"{i}: Diff: {diff.abs().mean()}\")\n",
    "        \n",
    "        if torch.any(torch.isnan(model.la1.fused_pair_bilinear.Y)):\n",
    "            print(f\"{i}, NAN Grid back; loss {float(loss)}\", flush=True)\n",
    "        if torch.any(torch.isnan(model.la1.fused_pair_bilinear.pairW)):\n",
    "            print(f\"{i}, NAN Weight back; loss {float(loss)}\", flush=True)\n",
    "#         sys.stdout.flush()\n",
    "        \n",
    "#         break\n",
    "#     break\n",
    "\n",
    "    train_accs.append(float(train_acc)/train_count*100)\n",
    "    train_acc = 0\n",
    "    train_count = 0\n",
    "\n",
    "    print(f'Epoch: {epoch},  Loss:{float(loss)}')\n",
    "    test_count = 0\n",
    "    test_acc = 0\n",
    "#     i = 1\n",
    "    for xx, yy in tqdm(test_loader):\n",
    "#         i = -1\n",
    "        xx = xx.view(xx.shape[0], -1)\n",
    "        xx, yy = xx.to(device), yy.to(device)\n",
    "        with torch.no_grad():\n",
    "            yout = model(xx)\n",
    "            yout_ = model_deb(xx)\n",
    "        outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "        correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "        test_acc += correct\n",
    "        test_count += len(xx)\n",
    "    test_accs.append(float(test_acc)/test_count*100)\n",
    "    print(f'Train Acc:{train_accs[-1]:.2f}%, Test Acc:{test_accs[-1]:.2f}%')\n",
    "    print()\n",
    "\n",
    "### after each class index is finished training\n",
    "print(f'\\t-> Train Acc {max(train_accs)} ; Test Acc {max(test_accs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.la1.fused_pair_bilinear.Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.la1.fused_pair_bilinear.pairW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.la1.fused_pair_bilinear.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.la1.fused_pair_bilinear.pairW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.any(torch.isnan(model.la1.fused_pair_bilinear.Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The two models with same computation differ in gradients..\n",
    "Below code analyses models for the differences in the output.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = FactorNet2().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.03)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model_deb = FactorNet2_debug().to(device)\n",
    "optimizer_deb = torch.optim.SGD(model_deb.parameters(), lr=0.03)\n",
    "\n",
    "print(\"number of params: \", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "print(\"number of params: \", sum(p.numel() for p in model_deb.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.la1.fused_pair_bilinear.pairW[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deb.la1.facto_nets[0].pairW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.la1.fused_pair_bilinear.Y[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deb.la1.facto_nets[0].Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xx, yy in tqdm(train_loader):\n",
    "    xx = xx.view(xx.shape[0], -1)\n",
    "    xx, yy = xx.to(device), yy.to(device)\n",
    "\n",
    "    yout = model(xx)\n",
    "    loss = criterion(yout.clone(), yy.clone())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    yout_ = model_deb(xx)\n",
    "    loss_ = criterion(yout_.clone(), yy.clone())\n",
    "    optimizer_deb.zero_grad()\n",
    "    loss_.backward()\n",
    "    optimizer_deb.step()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    diff = yout - yout_\n",
    "    print(f\"Diff: {diff.abs().mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.la1.fused_pair_bilinear.Y.grad[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deb.la1.facto_nets[-1].Y.grad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    diff = model.la1.fused_pair_bilinear.Y.grad[i] - model_deb.la1.facto_nets[i].Y.grad\n",
    "    print(f\"Y grad diff {torch.abs(diff).sum()}\")\n",
    "    \n",
    "    diff = model.la1.fused_pair_bilinear.pairW.grad[i] - model_deb.la1.facto_nets[i].pairW.grad\n",
    "    print(f\"W grad diff {torch.abs(diff).sum()}\")\n",
    "    \n",
    "    diff = model.la1.fused_pair_bilinear.Y[i] - model_deb.la1.facto_nets[i].Y\n",
    "    print(f\"Y diff {torch.abs(diff).mean()}\")\n",
    "    \n",
    "    diff = model.la1.fused_pair_bilinear.pairW[i] - model_deb.la1.facto_nets[i].pairW\n",
    "    print(f\"W diff {torch.abs(diff).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.la1.fused_pair_bilinear.Y[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deb.la1.facto_nets[0].Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bias.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deb.bias.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
