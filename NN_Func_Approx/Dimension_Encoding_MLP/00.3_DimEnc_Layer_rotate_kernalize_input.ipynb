{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os, time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mylibrary.datasets as datasets\n",
    "import mylibrary.nnlib as tnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = datasets.FashionMNIST()\n",
    "train_data, train_label_, test_data, test_label_ = mnist.load()\n",
    "\n",
    "train_data = train_data / 255.\n",
    "test_data = test_data / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "\n",
    "learning_rate = 0.0001\n",
    "batch_size = 50\n",
    "\n",
    "train_label = tnn.Logits.index_to_logit(train_label_)\n",
    "train_size = len(train_label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting data to pytorch format\n",
    "train_data = torch.Tensor(train_data)\n",
    "test_data = torch.Tensor(test_data)\n",
    "train_label = torch.LongTensor(train_label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### does binary like encoding but continuous sin and cos\n",
    "def generate_dimension_encoding(dim, binarize=True):\n",
    "    num_d = int(np.ceil(np.log2(dim)))\n",
    "    scale = np.arange(0, num_d, 1).reshape(1,-1)\n",
    "    scale = (1/2)**scale\n",
    "    \n",
    "    index = np.arange(0, dim, 1).reshape(-1,1)\n",
    "    mat = index*scale\n",
    "    \n",
    "    if binarize:\n",
    "        mat = mat*np.pi/2\n",
    "    sin_mat = np.sin(mat)**2\n",
    "    cos_mat = np.cos(mat)**2\n",
    "    pos_mat = np.concatenate((sin_mat, cos_mat), axis=1)\n",
    "    return torch.Tensor(pos_mat)\n",
    "\n",
    "### does exact binary encoding of position\n",
    "def generate_dimension_encoding2(dim):\n",
    "    num_d = int(np.ceil(np.log2(dim)))\n",
    "    pos_mat = np.empty((dim, num_d))\n",
    "    for i in range(dim):\n",
    "        binary = np.binary_repr(i, width=num_d)\n",
    "        pos_mat[i] = [float(a) for a in binary]\n",
    "    \n",
    "    pos_mat = np.concatenate((pos_mat, 1-pos_mat), axis=1)\n",
    "    return torch.Tensor(pos_mat)\n",
    "\n",
    "\n",
    "### does sin and cosine of position (meant to be used as addition)\n",
    "def generate_dimension_encoding3(dim, binarize=False):\n",
    "    num_d = int(np.ceil(np.log2(dim)))\n",
    "    scale = np.arange(0, num_d, 1).reshape(1,-1)\n",
    "    scale = (1/2)**scale\n",
    "    \n",
    "    index = np.arange(0, dim, 1).reshape(-1,1)\n",
    "    mat = index*scale\n",
    "    if binarize:\n",
    "        mat = mat*np.pi/2\n",
    "    sin_mat = np.sin(mat)\n",
    "    cos_mat = np.cos(mat)\n",
    "    pos_mat = np.concatenate((sin_mat, cos_mat), axis=1)\n",
    "    return torch.Tensor(pos_mat)\n",
    "\n",
    "#### as in positional encoding of attention transformer (as addition)\n",
    "def generate_dimension_encoding4(dim, binarize=False):\n",
    "    num_d = int(np.ceil(np.log2(dim)))\n",
    "    scale = np.arange(0, num_d, 1).reshape(1,-1)\n",
    "    scale = (1/10000)**(scale*2/dim)\n",
    "    \n",
    "    index = np.arange(0, dim, 1).reshape(-1,1)\n",
    "    mat = index*scale\n",
    "    if binarize:\n",
    "        mat = mat*np.pi/2\n",
    "    sin_mat = np.sin(mat)\n",
    "    cos_mat = np.cos(mat)\n",
    "    pos_mat = np.concatenate((sin_mat, cos_mat), axis=1)\n",
    "    return torch.Tensor(pos_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sequential_mlp(layer_dims, activation):\n",
    "    layers = []\n",
    "    for i in range(len(layer_dims)-1):\n",
    "        _a = nn.Linear(layer_dims[i], layer_dims[i+1])\n",
    "        layers += [_a, activation]\n",
    "    layers = layers[:-1]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# class DimMix_Layer_add(nn.Module):\n",
    "    \n",
    "#     def __init__(self,dim, pair, out_pair=None, hidden_ratio = [2], activation=nn.ReLU()):\n",
    "#         super().__init__()\n",
    "#         self.dim = dim\n",
    "#         self.pair = pair\n",
    "#         self.out_pair = out_pair\n",
    "#         if out_pair is None:\n",
    "#             self.out_pair = pair\n",
    "#         if dim%pair != 0 :\n",
    "#             raise ValueError(f\"Dim: {dim} should be exactly divisible by Pair: {pair}\")\n",
    "\n",
    "#         self.pos_mat = generate_dimension_encoding4(dim//pair)\n",
    "#         self.pos_mat = nn.Parameter(self.pos_mat)\n",
    "        \n",
    "#         self.linear = nn.Linear(pair, self.pos_mat.shape[1])\n",
    "        \n",
    "#         inp_dim = self.pos_mat.shape[1]\n",
    "#         la_dims = [inp_dim] + [int(hr*inp_dim) for hr in hidden_ratio] + [self.out_pair]\n",
    "#         self.net = construct_sequential_mlp(la_dims, activation)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         xs = x.shape\n",
    "#         _x = self.linear(x.reshape(-1, self.pair))\n",
    "#         _x = _x.reshape(-1, *self.pos_mat.shape)+self.pos_mat.unsqueeze(dim=0)\n",
    "#         _x = self.net(_x.reshape(-1, self.pos_mat.shape[1]))\n",
    "#         _x = _x.reshape(xs[0], -1)\n",
    "#         return _x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 0.],\n",
       "        [0., 1., 0., 1., 0., 1.],\n",
       "        [0., 1., 1., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 1., 1.],\n",
       "        [1., 0., 1., 0., 1., 0.],\n",
       "        [1., 1., 0., 0., 0., 1.],\n",
       "        [1., 1., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 16\n",
    "pair = 2\n",
    "\n",
    "X = torch.randn(2,dim)\n",
    "if dim%pair != 0 :\n",
    "    raise ValueError(f\"Dim: {dim} should be exactly divisible by Pair: {pair}\")\n",
    "pos_mat = generate_dimension_encoding2(dim//pair)\n",
    "pos_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DimMix_Layer_add(dim, pair)(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_dim = 4\n",
    "\n",
    "centers = torch.rand(kernel_dim, dim)*1.5-0.5\n",
    "scale = (torch.rand(1, kernel_dim)+0.25)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 16])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = (X.unsqueeze(1)-centers.unsqueeze(0))\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3907, -0.6059,  1.6753,  0.5986, -0.4268, -1.2034,  0.2296,\n",
       "          -1.9732,  0.9881,  0.7617,  1.6232,  1.0629,  1.2625, -0.6373,\n",
       "          -1.7661, -0.4456],\n",
       "         [ 0.8025, -0.2108,  1.1790,  0.7272,  0.5218, -0.8926, -0.3339,\n",
       "          -1.9174,  0.1320,  0.6218,  0.4348, -0.0187,  1.2707,  0.1887,\n",
       "          -1.6097,  0.1498],\n",
       "         [ 0.6720, -0.4486,  0.4953,  1.0849,  0.3548, -2.2187, -0.7718,\n",
       "          -1.4467,  0.8851,  1.1420,  0.4641,  0.2796,  1.2614, -0.5348,\n",
       "          -1.2307, -1.1026],\n",
       "         [ 0.4467, -0.4439,  0.4500,  1.4391,  0.1766, -0.7839,  0.2730,\n",
       "          -1.6284,  0.7510,  0.0916,  0.9207,  0.3206,  0.2467, -0.4942,\n",
       "          -0.4930, -0.8069]],\n",
       "\n",
       "        [[ 0.2996, -0.4902, -0.3616,  0.4359, -0.9002,  0.9319,  0.5062,\n",
       "           0.5698,  1.7613,  0.3194,  0.4969,  0.0485,  0.9640, -1.5235,\n",
       "          -1.1817, -0.8883],\n",
       "         [ 0.7114, -0.0951, -0.8580,  0.5646,  0.0483,  1.2427, -0.0573,\n",
       "           0.6256,  0.9052,  0.1795, -0.6916, -1.0331,  0.9722, -0.6974,\n",
       "          -1.0252, -0.2929],\n",
       "         [ 0.5809, -0.3328, -1.5416,  0.9223, -0.1186, -0.0834, -0.4951,\n",
       "           1.0963,  1.6583,  0.6997, -0.6622, -0.7348,  0.9629, -1.4209,\n",
       "          -0.6462, -1.5453],\n",
       "         [ 0.3556, -0.3282, -1.5870,  1.2765, -0.2969,  1.3514,  0.5497,\n",
       "           0.9146,  1.5243, -0.3508, -0.2056, -0.6937, -0.0518, -1.3804,\n",
       "           0.0914, -1.2496]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[19.9002, 12.3214, 16.8008,  8.7644],\n",
       "         [11.8000,  8.4770, 15.1133, 13.8831]]),\n",
       " tensor([[19.9002, 12.3214, 16.8008,  8.7644],\n",
       "         [11.8000,  8.4770, 15.1133, 13.8831]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(K, dim=2)**2, torch.sum(K**2, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.exp(-(scale**2)*torch.sum(K**2, dim=2))\n",
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.9765e-07, 2.3265e-01, 1.3477e-01, 3.8366e-01],\n",
       "        [2.2363e-04, 3.6669e-01, 1.6483e-01, 2.1925e-01]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.cat([X, k], dim=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn(3, 2,2)\n",
    "X = torch.randn(4, 3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0461, -2.9304],\n",
       "         [ 0.4146,  2.1248],\n",
       "         [ 0.0241, -0.0172]],\n",
       "\n",
       "        [[-0.3109,  0.8908],\n",
       "         [-0.7844, -1.5582],\n",
       "         [ 0.3740, -0.7978]],\n",
       "\n",
       "        [[ 1.6886, -2.9869],\n",
       "         [-1.2455, -2.0309],\n",
       "         [-0.1662, -0.6536]],\n",
       "\n",
       "        [[-0.3452,  0.2842],\n",
       "         [-1.2715, -1.6654],\n",
       "         [-0.0447,  0.1354]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(X.transpose(0,1), W).transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadialKernelize(nn.Module):\n",
    "    def __init__(self, dim, kernel_dim):\n",
    "        super().__init__()\n",
    "        assert kernel_dim >= 0 and dim >=0\n",
    "        self.kernel_dim = kernel_dim\n",
    "        self.dim = dim\n",
    "        if kernel_dim != 0:\n",
    "            centers = torch.rand(kernel_dim, dim)*1.5-0.5\n",
    "            scale = (torch.rand(1, kernel_dim)+0.25)*1\n",
    "            self.centers = nn.Parameter(centers)\n",
    "            self.scale = nn.Parameter(scale)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.kernel_dim == 0: return x\n",
    "        \n",
    "#         print(x.shape, self.centers.shape)\n",
    "        \n",
    "        K = (x.unsqueeze(1)-self.centers.unsqueeze(0))\n",
    "        K = torch.exp(-(self.scale**2)*torch.sum(K**2, dim=2))\n",
    "        \n",
    "        return torch.cat([x, K], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernalize_Rotate_BN_DimMix(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim, pair, out_pair=None, hidden_ratio = [2], activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.pair = pair\n",
    "        self.out_pair = out_pair\n",
    "        if out_pair is None:\n",
    "            self.out_pair = pair\n",
    "        if dim%pair != 0 :\n",
    "            raise ValueError(f\"Dim: {dim} should be exactly divisible by Pair: {pair}\")\n",
    "\n",
    "        self.pos_mat = generate_dimension_encoding4(dim//pair)\n",
    "        num_pos, inp_dim = self.pos_mat.shape\n",
    "        ### there will be num_pos=dim//pair different inputs\n",
    "        ### inp_dim will be the dimension of inputs\n",
    "        if inp_dim < pair:\n",
    "            self.pos_mat = torch.cat([self.pos_mat, torch.zeros(num_pos,pair-inp_dim)], dim=1)\n",
    "            inp_dim = self.pos_mat.shape[1]\n",
    "        \n",
    "        self.pos_mat = nn.Parameter(self.pos_mat)\n",
    "        \n",
    "        self.kernalizer = RadialKernelize(pair, inp_dim-pair)\n",
    "        \n",
    "        self.rotate = torch.randn(num_pos, inp_dim, inp_dim)\n",
    "        self.rotate /= torch.norm(self.rotate, dim=2, keepdim=True)\n",
    "        self.rotate = nn.Parameter(self.rotate)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(num_pos*inp_dim, affine=False)\n",
    "        \n",
    "        la_dims = [inp_dim] + [int(hr*inp_dim) for hr in hidden_ratio] + [self.out_pair]\n",
    "        self.net = construct_sequential_mlp(la_dims, activation)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        xs = x.shape\n",
    "        \n",
    "        ### kernalizer takes batch of pair inputs and adds kernel to make shape=b*num_enc, input_dim\n",
    "        _x = self.kernalizer(x.reshape(-1, self.pair))\n",
    "        \n",
    "        ### rotator rotates the data per dimension output shape=(b, num_enc, input_dim)\n",
    "        _x = _x.reshape(-1, *self.pos_mat.shape)\n",
    "        _x = torch.bmm(_x.transpose(0,1), self.rotate).transpose(0,1)\n",
    "        \n",
    "        ### Normalize the batch without encoding, ...can also normalize after encoding...\n",
    "        _x = self.bn(_x.reshape(-1, self.pos_mat.shape[0]*self.pos_mat.shape[1]))\n",
    "        \n",
    "        ### similar to shift transform\n",
    "        _x = _x.reshape(-1, *self.pos_mat.shape)+self.pos_mat.unsqueeze(dim=0)\n",
    "        ### pass encoded input through the shared network\n",
    "        _x = self.net(_x.reshape(-1, self.pos_mat.shape[1]))\n",
    "        _x = _x.reshape(xs[0], -1)\n",
    "        return _x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Kernalize_Rotate_BN_DimMix(\n",
       "  (kernalizer): RadialKernelize()\n",
       "  (bn): BatchNorm1d(3136, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(2,784)\n",
    "Kernalize_Rotate_BN_DimMix(784, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "196*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DimMix_Layer_cat(nn.Module):\n",
    "    \n",
    "#     def __init__(self,dim, pair, out_pair=None, hidden_ratio = [2], activation=nn.ReLU()):\n",
    "#         super().__init__()\n",
    "#         self.dim = dim\n",
    "#         self.pair = pair\n",
    "#         self.out_pair = out_pair\n",
    "#         if out_pair is None:\n",
    "#             self.out_pair = pair\n",
    "#         if dim%pair != 0 :\n",
    "#             raise ValueError(f\"Dim: {dim} should be exactly divisible by Pair: {pair}\")\n",
    "\n",
    "#         self.pos_mat = generate_dimension_encoding4(dim//pair)\n",
    "#         self.pos_mat = nn.Parameter(self.pos_mat)\n",
    "        \n",
    "#         inp_dim = self.pos_mat.shape[1]+pair\n",
    "#         la_dims = [inp_dim] + [int(hr*inp_dim) for hr in hidden_ratio] + [self.out_pair]\n",
    "#         self.net = construct_sequential_mlp(la_dims, activation)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         xs = x.shape\n",
    "#         _x = x.reshape(-1, self.dim//self.pair, self.pair)\n",
    "#         _pm = self.pos_mat.expand(xs[0], *self.pos_mat.shape)\n",
    "#         _x = torch.cat([_x, _pm], dim=2)\n",
    "#         _x = _x.reshape(-1, self.pos_mat.shape[1]+self.pair)\n",
    "#         _x = self.net(_x).reshape(xs[0], -1)\n",
    "#         return _x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DimMix_Layer_cat(dim, pair)(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(784, 50)\n",
    "        self.l2 = nn.LeakyReLU()\n",
    "        self.l3 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "    \n",
    "class MLP_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(784, 300)\n",
    "        self.l3 = nn.Linear(300, 10)\n",
    "        self.l2 = Kernalize_Rotate_BN_DimMix(300, 5, hidden_ratio=[5,5], activation=nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "# class MLP_2(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.randindx = torch.randperm(784)\n",
    "#         hr = [5*2,5*2]\n",
    "#         self.l1 = DimMix_Layer_add(784, 8, 4, hidden_ratio=hr, activation=nn.ReLU()) #784/2*1 = 392\n",
    "#         self.l2 = DimMix_Layer_add(392, 8, 2, hidden_ratio=hr, activation=nn.ReLU()) #392/4*1 = 98\n",
    "#         self.l3 = DimMix_Layer_add(98, 7, 3, hidden_ratio=hr, activation=nn.ReLU()) #98/7*3 = 42\n",
    "#         self.l4 = nn.Linear(42, 10)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = x[:, self.randindx]\n",
    "#         x = self.l1(x)\n",
    "#         x = self.l2(x)\n",
    "#         x = self.l3(x)\n",
    "#         x = self.l4(x)\n",
    "#         return x\n",
    "    \n",
    "# class MLP_3(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.randindx = torch.randperm(784)\n",
    "#         hr = [5*2,5*2]\n",
    "#         self.l1 = DimMix_Layer_add(784, 8, 8, hidden_ratio=hr, activation=nn.ReLU()) #784/2*1 = 392\n",
    "#         self.l2 = DimMix_Layer_add(784, 8, 8, hidden_ratio=hr, activation=nn.ReLU()) #392/4*1 = 98\n",
    "#         self.l3 = DimMix_Layer_add(784, 8, 8, hidden_ratio=hr, activation=nn.ReLU()) #98/7*3 = 42\n",
    "#         self.l4 = nn.Linear(784, 10)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = x[:, self.randindx]\n",
    "#         x = self.l1(x)\n",
    "#         x = self.l2(x)\n",
    "#         x = self.l3(x)\n",
    "#         x = self.l4(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(2,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_1()(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP_1(\n",
       "  (l1): Linear(in_features=784, out_features=300, bias=True)\n",
       "  (l3): Linear(in_features=300, out_features=10, bias=True)\n",
       "  (l2): Kernalize_Rotate_BN_DimMix(\n",
       "    (kernalizer): RadialKernelize()\n",
       "    (bn): BatchNorm1d(720, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=12, out_features=60, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=60, out_features=60, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=60, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = MLP_1().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 784])\n",
      "torch.Size([300])\n",
      "torch.Size([10, 300])\n",
      "torch.Size([10])\n",
      "torch.Size([60, 12])\n",
      "torch.Size([60, 12, 12])\n",
      "torch.Size([7, 5])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([60, 12])\n",
      "torch.Size([60])\n",
      "torch.Size([60, 60])\n",
      "torch.Size([60])\n",
      "torch.Size([5, 60])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  252657\n"
     ]
    }
   ],
   "source": [
    "## MLP2: number of params:  38390 ; add\n",
    "## MLP0: number of params:  39760\n",
    "## MLP2: number of params:  27363 ; add\n",
    "\n",
    "print(\"number of params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 499/1200 [10:15<16:48,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, batch: 499, step: 500, loss: 0.627275288105011\n",
      "\tTrain acc: 75.444%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 500/1200 [11:27<4:26:57, 22.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest acc: 82.46%, correct: 8246/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 999/1200 [21:36<04:43,  1.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, batch: 999, step: 1000, loss: 0.7603819370269775\n",
      "\tTrain acc: 84.428%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 1000/1200 [22:51<1:18:05, 23.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest acc: 84.47%, correct: 8447/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [26:53<00:00,  1.34s/it]  \n",
      " 25%|██▍       | 299/1200 [06:02<18:25,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1, batch: 299, step: 1500, loss: 0.4564221501350403\n",
      "\tTrain acc: 86.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 300/1200 [07:18<5:55:34, 23.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest acc: 84.86%, correct: 8486/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 446/1200 [10:25<17:36,  1.40s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-b346691ccb4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0myout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-d1ee482689f1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-ca4cd0cbed3d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0m_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m### pass encoded input through the shared network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0m_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0m_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 15*3\n",
    "steps_ = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_acc = 0\n",
    "    train_count = 0\n",
    "    for index in tqdm(range(train_size // batch_size)):\n",
    "        steps_ += 1\n",
    "\n",
    "        train_x = train_data[index * batch_size:(index + 1) * batch_size].to(device)\n",
    "        train_y = train_label[index * batch_size:(index + 1) * batch_size].to(device)\n",
    "\n",
    "        yout = model(train_x)\n",
    "        loss = criterion(yout, train_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "        train_acc += (outputs == train_y.data.cpu().numpy()).sum()\n",
    "        train_count += len(outputs)\n",
    "\n",
    "        if steps_%500==0:\n",
    "            train_accuracy = train_acc/train_count\n",
    "            train_acc, train_count = 0, 0\n",
    "            \n",
    "            print(f'\\nEpoch: {epoch}, batch: {index}, step: {steps_}, loss: {float(loss)}')\n",
    "            print(f'\\tTrain acc: {train_accuracy*100}%')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                test_acc, test_count = 0, 0\n",
    "                model.eval()\n",
    "                for _ti in range(len(test_data) // batch_size):\n",
    "                    test_x = test_data[_ti * batch_size:(_ti + 1) * batch_size].to(device)\n",
    "                    test_y_ = test_label_[_ti * batch_size:(_ti + 1) * batch_size]\n",
    "                    yout = model(test_x)\n",
    "                    outputs = tnn.Logits.logit_to_index(yout.cpu().numpy())\n",
    "                    correct = (outputs == test_y_).sum()\n",
    "                    test_acc += correct\n",
    "                    test_count += len(test_x)\n",
    "                model.train()\n",
    "\n",
    "            print(f'\\tTest acc: {test_acc/test_count*100}%, correct: {test_acc}/{test_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLP2\n",
    "# Train acc: 87.96000000000001%\n",
    "# \tTest acc: 86.27%, correct: 8627/10000\n",
    "## MLP0\n",
    "# Train acc: 89.05999999999999%\n",
    "# \tTest acc: 87.3%, correct: 8730/10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize first activations --paired embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### inputs from -1, 1 for every dimension in a batch\n",
    "X = torch.linspace(-1, 1, steps=20).expand(300, -1).transpose(0,1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "ys = model.l2(X.to(device)).data.cpu()\n",
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(model.l2.pos_mat.cpu().data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_show = 10\n",
    "for i in range(300):\n",
    "    plt.plot(X[:,i], ys[:,i])#+X[:,i])\n",
    "    if (i+1)%num_show == 0:\n",
    "#         plt.axis(\"equal\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MLP_1()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# print(\"number of params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 15\n",
    "# steps_ = 0\n",
    "# for epoch in range(EPOCHS):\n",
    "#     train_acc = 0\n",
    "#     train_count = 0\n",
    "#     for index in range(train_size // batch_size):\n",
    "#         steps_ += 1\n",
    "\n",
    "#         train_x = train_data[index * batch_size:(index + 1) * batch_size]\n",
    "#         train_y = train_label[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "#         yout = model(train_x)\n",
    "#         loss = criterion(yout, train_y)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if steps_%500==0:\n",
    "#             print('\\nTRAIN',epoch, steps_, '-> ', float(loss))\n",
    "# #             yout = model.forward(train_data)\n",
    "# #             outputs = tnn.Logits.logit_to_index(yout)\n",
    "# #             correct = (outputs == np.array(train_label_)).sum()\n",
    "\n",
    "# #             accuracy = correct / len(train_label_) * 100.\n",
    "# #             print('EPOCH = ','accuracy = ', accuracy)\n",
    "# #             print(correct, '/', len(train_label_))\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 yout = model(test_data)\n",
    "#             outputs = tnn.Logits.logit_to_index(yout.cpu().numpy())\n",
    "#             correct = (outputs == np.array(test_label_)).sum()\n",
    "\n",
    "#             accuracy = correct / len(test_label_) * 100.\n",
    "#             print('   TEST  ','accuracy = ', accuracy)\n",
    "#             print(correct, '/', len(test_label_))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.l2.pos_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log2(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
