{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-b22782e32495>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE parameters\n",
    "num_label = 10\n",
    "label_hid1 = 5\n",
    "label_hid2 = 3\n",
    "label_hid3 = 2\n",
    "label_hid4 = 1\n",
    "\n",
    "num_image = 784 # 28*28\n",
    "image_hid1 = 392\n",
    "image_hid2 = 196\n",
    "image_hid3 = 98\n",
    "image_hid4 = 49\n",
    "image_hid5 = 25\n",
    "image_hid6 = 12\n",
    "image_hid7 = 6\n",
    "image_hid8 = 3\n",
    "image_hid9 = 2\n",
    "image_hid10 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tie_weight = tf.transpose\n",
    "act_func = lambda X:X\n",
    "initializer = tf.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLACEHOLDER DEFINATION\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_image])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, num_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Encoded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMAGE COMPRESSION\n",
    "wi1 = tf.Variable(np.load('../save/weights/w1.npy'))\n",
    "wi2 = tf.Variable(np.load('../save/weights/w2.npy'))\n",
    "wi3 = tf.Variable(np.load('../save/weights/w3.npy'))\n",
    "wi4 = tf.Variable(np.load('../save/weights/w4.npy'))\n",
    "wi5 = tf.Variable(np.load('../save/weights/w5.npy'))\n",
    "wi6 = tf.Variable(np.load('../save/weights/w6.npy'))\n",
    "wi7 = tf.Variable(np.load('../save/weights/w7.npy'))\n",
    "wi8 = tf.Variable(np.load('../save/weights/w8.npy'))\n",
    "wi9 = tf.Variable(np.load('../save/weights/w9.npy'))\n",
    "wi10 = tf.Variable(np.load('../save/weights/w10.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_layer1 = act_func(tf.matmul(X, wi1))\n",
    "img_layer2 = act_func(tf.matmul(img_layer1, wi2))\n",
    "img_layer3 = act_func(tf.matmul(img_layer2, wi3))\n",
    "img_layer4 = act_func(tf.matmul(img_layer3, wi4))\n",
    "img_layer5 = act_func(tf.matmul(img_layer4, wi5))\n",
    "img_layer6 = act_func(tf.matmul(img_layer5, wi6))\n",
    "img_layer7 = act_func(tf.matmul(img_layer6, wi7))\n",
    "img_layer8 = act_func(tf.matmul(img_layer7, wi8))\n",
    "img_layer9 = act_func(tf.matmul(img_layer8, wi9))\n",
    "img_layer10 = act_func(tf.matmul(img_layer9, wi10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LABEL DECOMPRESSION\n",
    "wl1 = tf.Variable(np.load('../save/weights/lw1.npy'))\n",
    "wl2 = tf.Variable(np.load('../save/weights/lw2.npy'))\n",
    "wl3 = tf.Variable(np.load('../save/weights/lw3.npy'))\n",
    "wl4 = tf.Variable(np.load('../save/weights/lw4.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_layer1 = act_func(tf.matmul(Y, wl1))\n",
    "lbl_layer2 = act_func(tf.matmul(lbl_layer1, wl2))\n",
    "lbl_layer3 = act_func(tf.matmul(lbl_layer2, wl3))\n",
    "lbl_layer4 = act_func(tf.matmul(lbl_layer3, wl4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Parameters for activation learning\n",
    "initializer = tf.random_uniform_initializer()\n",
    "\n",
    "input_no = image_hid9\n",
    "output_no = label_hid3\n",
    "\n",
    "inpt = img_layer9\n",
    "yval = lbl_layer3\n",
    "\n",
    "i=2\n",
    "j=1\n",
    "\n",
    "W1 = tf.Variable(np.load('../save/weights/slearn_parms0_{}-{}.npy'.format(i, j)))\n",
    "# W1 = tf.Variable(tf.matmul(wi10,pW1))\n",
    "W2 = tf.Variable(np.load('../save/weights/slearn_parms1_{}-{}.npy'.format(i, j)))\n",
    "pW3 = tf.Variable(np.load('../save/weights/slearn_parms2_{}-{}.npy'.format(i, j)))\n",
    "W3 = tf.Variable(tf.matmul(pW3, tf.transpose(wl4)))\n",
    "\n",
    "b1 = tf.Variable(np.load('../save/weights/slearn_parms3_{}-{}.npy'.format(i, j)))\n",
    "b2 = tf.Variable(np.load('../save/weights/slearn_parms4_{}-{}.npy'.format(i, j)))\n",
    "pb3 = tf.Variable(np.load('../save/weights/slearn_parms5_{}-{}.npy'.format(i, j)).reshape(-1,1))\n",
    "b3 = tf.Variable(tf.matmul(pb3,tf.transpose(wl4)))\n",
    "\n",
    "a1 = tf.nn.relu(tf.matmul(inpt, W1) + b1)\n",
    "a2 = tf.nn.relu(tf.matmul(a1, W2) + b2)\n",
    "a13 = tf.matmul(a2, W3) + b3\n",
    "\n",
    "output = a13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_28:0' shape=(1, 1) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_29:0' shape=(1, 2) dtype=float32_ref>\n",
      "Tensor(\"transpose_6:0\", shape=(1, 2), dtype=float32)\n",
      "Tensor(\"add_5:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(pb3)\n",
    "print(b3)\n",
    "print(tf.transpose(wl4))\n",
    "print(a13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Encode-Decode Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_layer1 = tf.matmul(output,tf.transpose(wl4))\n",
    "out_layer2 = tf.matmul(output,tf.transpose(wl3))\n",
    "out_layer3 = tf.matmul(out_layer2,tf.transpose(wl2))\n",
    "out_layer4 = tf.matmul(out_layer3,tf.transpose(wl1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTIONS\n",
    "loss = tf.reduce_mean(tf.square(output - yval))\n",
    "# loss = tf.reduce_mean(tf.abs(output_layer - X))\n",
    "# cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y,logits=yout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "learning_rate = 0.0001\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss, var_list=[W1, W2, W3, b1, b2, b3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARATION\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Complete. Training Loss: 22.00576400756836\n",
      "TEST ACCURACY: \n",
      "0.1921\n",
      "TRAIN ACCURACY: \n",
      "0.19183636\n",
      "\n",
      "Epoch 1 Complete. Training Loss: 23.29576873779297\n",
      "TEST ACCURACY: \n",
      "0.1918\n",
      "\n",
      "Epoch 2 Complete. Training Loss: 23.602035522460938\n",
      "TEST ACCURACY: \n",
      "0.1893\n",
      "\n",
      "Epoch 3 Complete. Training Loss: 22.860994338989258\n",
      "TEST ACCURACY: \n",
      "0.1838\n",
      "\n",
      "Epoch 4 Complete. Training Loss: 22.439111709594727\n",
      "TEST ACCURACY: \n",
      "0.2208\n",
      "\n",
      "Epoch 5 Complete. Training Loss: 22.605636596679688\n",
      "TEST ACCURACY: \n",
      "0.2326\n",
      "TRAIN ACCURACY: \n",
      "0.23434545\n",
      "\n",
      "Epoch 6 Complete. Training Loss: 21.183687210083008\n",
      "TEST ACCURACY: \n",
      "0.2327\n",
      "\n",
      "Epoch 7 Complete. Training Loss: 22.15029525756836\n",
      "TEST ACCURACY: \n",
      "0.233\n",
      "\n",
      "Epoch 8 Complete. Training Loss: 17.163867950439453\n",
      "TEST ACCURACY: \n",
      "0.2319\n",
      "\n",
      "Epoch 9 Complete. Training Loss: 22.680448532104492\n",
      "TEST ACCURACY: \n",
      "0.2336\n",
      "\n",
      "Epoch 10 Complete. Training Loss: 20.922012329101562\n",
      "TEST ACCURACY: \n",
      "0.2312\n",
      "TRAIN ACCURACY: \n",
      "0.23376364\n",
      "\n",
      "Epoch 11 Complete. Training Loss: 19.826723098754883\n",
      "TEST ACCURACY: \n",
      "0.2329\n",
      "\n",
      "Epoch 12 Complete. Training Loss: 20.858070373535156\n",
      "TEST ACCURACY: \n",
      "0.2325\n",
      "\n",
      "Epoch 13 Complete. Training Loss: 19.994354248046875\n",
      "TEST ACCURACY: \n",
      "0.234\n",
      "\n",
      "Epoch 14 Complete. Training Loss: 17.889148712158203\n",
      "TEST ACCURACY: \n",
      "0.2336\n",
      "\n",
      "Epoch 15 Complete. Training Loss: 20.985248565673828\n",
      "TEST ACCURACY: \n",
      "0.2326\n",
      "TRAIN ACCURACY: \n",
      "0.23707272\n",
      "\n",
      "Epoch 16 Complete. Training Loss: 18.63827896118164\n",
      "TEST ACCURACY: \n",
      "0.234\n",
      "\n",
      "Epoch 17 Complete. Training Loss: 21.16724395751953\n",
      "TEST ACCURACY: \n",
      "0.2342\n",
      "\n",
      "Epoch 18 Complete. Training Loss: 23.637950897216797\n",
      "TEST ACCURACY: \n",
      "0.2327\n",
      "\n",
      "Epoch 19 Complete. Training Loss: 22.883167266845703\n",
      "TEST ACCURACY: \n",
      "0.233\n",
      "\n",
      "Epoch 20 Complete. Training Loss: 22.186599731445312\n",
      "TEST ACCURACY: \n",
      "0.2344\n",
      "TRAIN ACCURACY: \n",
      "0.23616363\n",
      "\n",
      "Epoch 21 Complete. Training Loss: 20.658946990966797\n",
      "TEST ACCURACY: \n",
      "0.2329\n",
      "\n",
      "Epoch 22 Complete. Training Loss: 20.059066772460938\n",
      "TEST ACCURACY: \n",
      "0.2336\n",
      "\n",
      "Epoch 23 Complete. Training Loss: 20.813215255737305\n",
      "TEST ACCURACY: \n",
      "0.2335\n",
      "\n",
      "Epoch 24 Complete. Training Loss: 18.897708892822266\n",
      "TEST ACCURACY: \n",
      "0.2312\n",
      "\n",
      "Epoch 25 Complete. Training Loss: 18.4476375579834\n",
      "TEST ACCURACY: \n",
      "0.2341\n",
      "TRAIN ACCURACY: \n",
      "0.237\n",
      "\n",
      "Epoch 26 Complete. Training Loss: 19.420146942138672\n",
      "TEST ACCURACY: \n",
      "0.2332\n",
      "\n",
      "Epoch 27 Complete. Training Loss: 18.636377334594727\n",
      "TEST ACCURACY: \n",
      "0.2348\n",
      "\n",
      "Epoch 28 Complete. Training Loss: 24.22811508178711\n",
      "TEST ACCURACY: \n",
      "0.2335\n",
      "\n",
      "Epoch 29 Complete. Training Loss: 23.367698669433594\n",
      "TEST ACCURACY: \n",
      "0.2297\n",
      "\n",
      "Epoch 30 Complete. Training Loss: 20.820009231567383\n",
      "TEST ACCURACY: \n",
      "0.2346\n",
      "TRAIN ACCURACY: \n",
      "0.23632728\n",
      "\n",
      "Epoch 31 Complete. Training Loss: 23.396039962768555\n",
      "TEST ACCURACY: \n",
      "0.2329\n",
      "\n",
      "Epoch 32 Complete. Training Loss: 21.36075210571289\n",
      "TEST ACCURACY: \n",
      "0.232\n",
      "\n",
      "Epoch 33 Complete. Training Loss: 19.733877182006836\n",
      "TEST ACCURACY: \n",
      "0.2322\n",
      "\n",
      "Epoch 34 Complete. Training Loss: 22.780864715576172\n",
      "TEST ACCURACY: \n",
      "0.2332\n",
      "\n",
      "Epoch 35 Complete. Training Loss: 18.091197967529297\n",
      "TEST ACCURACY: \n",
      "0.2311\n",
      "TRAIN ACCURACY: \n",
      "0.23356363\n",
      "\n",
      "Epoch 36 Complete. Training Loss: 18.828311920166016\n",
      "TEST ACCURACY: \n",
      "0.2312\n",
      "\n",
      "Epoch 37 Complete. Training Loss: 19.254131317138672\n",
      "TEST ACCURACY: \n",
      "0.2317\n",
      "\n",
      "Epoch 38 Complete. Training Loss: 19.757413864135742\n",
      "TEST ACCURACY: \n",
      "0.2313\n",
      "\n",
      "Epoch 39 Complete. Training Loss: 24.318557739257812\n",
      "TEST ACCURACY: \n",
      "0.2312\n",
      "\n",
      "Epoch 40 Complete. Training Loss: 20.904552459716797\n",
      "TEST ACCURACY: \n",
      "0.231\n",
      "TRAIN ACCURACY: \n",
      "0.2312909\n",
      "\n",
      "Epoch 41 Complete. Training Loss: 20.78272247314453\n",
      "TEST ACCURACY: \n",
      "0.2307\n",
      "\n",
      "Epoch 42 Complete. Training Loss: 21.51959800720215\n",
      "TEST ACCURACY: \n",
      "0.2315\n",
      "\n",
      "Epoch 43 Complete. Training Loss: 20.243698120117188\n",
      "TEST ACCURACY: \n",
      "0.2315\n",
      "\n",
      "Epoch 44 Complete. Training Loss: 21.23221206665039\n",
      "TEST ACCURACY: \n",
      "0.2307\n",
      "\n",
      "Epoch 45 Complete. Training Loss: 20.968772888183594\n",
      "TEST ACCURACY: \n",
      "0.2314\n",
      "TRAIN ACCURACY: \n",
      "0.23165454\n",
      "\n",
      "Epoch 46 Complete. Training Loss: 21.858274459838867\n",
      "TEST ACCURACY: \n",
      "0.229\n",
      "\n",
      "Epoch 47 Complete. Training Loss: 23.744976043701172\n",
      "TEST ACCURACY: \n",
      "0.2311\n",
      "\n",
      "Epoch 48 Complete. Training Loss: 20.630508422851562\n",
      "TEST ACCURACY: \n",
      "0.2306\n",
      "\n",
      "Epoch 49 Complete. Training Loss: 20.256317138671875\n",
      "TEST ACCURACY: \n",
      "0.231\n",
      "\n",
      "Epoch 50 Complete. Training Loss: 17.991952896118164\n",
      "TEST ACCURACY: \n",
      "0.2314\n",
      "TRAIN ACCURACY: \n",
      "0.2324\n",
      "\n",
      "Epoch 51 Complete. Training Loss: 24.078025817871094\n",
      "TEST ACCURACY: \n",
      "0.231\n",
      "\n",
      "Epoch 52 Complete. Training Loss: 18.61754035949707\n",
      "TEST ACCURACY: \n",
      "0.2292\n",
      "\n",
      "Epoch 53 Complete. Training Loss: 21.845823287963867\n",
      "TEST ACCURACY: \n",
      "0.2313\n",
      "\n",
      "Epoch 54 Complete. Training Loss: 20.039953231811523\n",
      "TEST ACCURACY: \n",
      "0.2319\n",
      "\n",
      "Epoch 55 Complete. Training Loss: 20.62175941467285\n",
      "TEST ACCURACY: \n",
      "0.2279\n",
      "TRAIN ACCURACY: \n",
      "0.2284909\n",
      "\n",
      "Epoch 56 Complete. Training Loss: 18.680187225341797\n",
      "TEST ACCURACY: \n",
      "0.2313\n",
      "\n",
      "Epoch 57 Complete. Training Loss: 17.837535858154297\n",
      "TEST ACCURACY: \n",
      "0.2298\n",
      "\n",
      "Epoch 58 Complete. Training Loss: 22.589000701904297\n",
      "TEST ACCURACY: \n",
      "0.2307\n",
      "\n",
      "Epoch 59 Complete. Training Loss: 19.794109344482422\n",
      "TEST ACCURACY: \n",
      "0.2306\n",
      "\n",
      "Epoch 60 Complete. Training Loss: 18.620681762695312\n",
      "TEST ACCURACY: \n",
      "0.228\n",
      "TRAIN ACCURACY: \n",
      "0.22998182\n",
      "\n",
      "Epoch 61 Complete. Training Loss: 19.6472225189209\n",
      "TEST ACCURACY: \n",
      "0.226\n",
      "\n",
      "Epoch 62 Complete. Training Loss: 21.495868682861328\n",
      "TEST ACCURACY: \n",
      "0.2264\n",
      "\n",
      "Epoch 63 Complete. Training Loss: 19.251514434814453\n",
      "TEST ACCURACY: \n",
      "0.2283\n",
      "\n",
      "Epoch 64 Complete. Training Loss: 21.421785354614258\n",
      "TEST ACCURACY: \n",
      "0.2291\n",
      "\n",
      "Epoch 65 Complete. Training Loss: 19.05742835998535\n",
      "TEST ACCURACY: \n",
      "0.2285\n",
      "TRAIN ACCURACY: \n",
      "0.22832727\n",
      "\n",
      "Epoch 66 Complete. Training Loss: 19.19036865234375\n",
      "TEST ACCURACY: \n",
      "0.2288\n",
      "\n",
      "Epoch 67 Complete. Training Loss: 18.698389053344727\n",
      "TEST ACCURACY: \n",
      "0.2322\n",
      "\n",
      "Epoch 68 Complete. Training Loss: 21.700937271118164\n",
      "TEST ACCURACY: \n",
      "0.2279\n",
      "\n",
      "Epoch 69 Complete. Training Loss: 22.442123413085938\n",
      "TEST ACCURACY: \n",
      "0.2309\n",
      "\n",
      "Epoch 70 Complete. Training Loss: 18.071744918823242\n",
      "TEST ACCURACY: \n",
      "0.2274\n",
      "TRAIN ACCURACY: \n",
      "0.22698182\n",
      "\n",
      "Epoch 71 Complete. Training Loss: 21.334230422973633\n",
      "TEST ACCURACY: \n",
      "0.2285\n",
      "\n",
      "Epoch 72 Complete. Training Loss: 20.9379825592041\n",
      "TEST ACCURACY: \n",
      "0.228\n",
      "\n",
      "Epoch 73 Complete. Training Loss: 22.176441192626953\n",
      "TEST ACCURACY: \n",
      "0.2294\n",
      "\n",
      "Epoch 74 Complete. Training Loss: 21.215810775756836\n",
      "TEST ACCURACY: \n",
      "0.2281\n",
      "\n",
      "Epoch 75 Complete. Training Loss: 22.898160934448242\n",
      "TEST ACCURACY: \n",
      "0.2278\n",
      "TRAIN ACCURACY: \n",
      "0.2265091\n",
      "\n",
      "Epoch 76 Complete. Training Loss: 22.202655792236328\n",
      "TEST ACCURACY: \n",
      "0.2274\n",
      "\n",
      "Epoch 77 Complete. Training Loss: 17.045507431030273\n",
      "TEST ACCURACY: \n",
      "0.2268\n",
      "\n",
      "Epoch 78 Complete. Training Loss: 24.655654907226562\n",
      "TEST ACCURACY: \n",
      "0.2298\n",
      "\n",
      "Epoch 79 Complete. Training Loss: 24.478431701660156\n",
      "TEST ACCURACY: \n",
      "0.2276\n",
      "\n",
      "Epoch 80 Complete. Training Loss: 19.8112850189209\n",
      "TEST ACCURACY: \n",
      "0.2273\n",
      "TRAIN ACCURACY: \n",
      "0.22656363\n",
      "\n",
      "Epoch 81 Complete. Training Loss: 18.804834365844727\n",
      "TEST ACCURACY: \n",
      "0.2258\n",
      "\n",
      "Epoch 82 Complete. Training Loss: 20.526111602783203\n",
      "TEST ACCURACY: \n",
      "0.2292\n",
      "\n",
      "Epoch 83 Complete. Training Loss: 17.378314971923828\n",
      "TEST ACCURACY: \n",
      "0.2274\n",
      "\n",
      "Epoch 84 Complete. Training Loss: 19.369464874267578\n",
      "TEST ACCURACY: \n",
      "0.2293\n",
      "\n",
      "Epoch 85 Complete. Training Loss: 17.179920196533203\n",
      "TEST ACCURACY: \n",
      "0.2268\n",
      "TRAIN ACCURACY: \n",
      "0.22603637\n",
      "\n",
      "Epoch 86 Complete. Training Loss: 21.438034057617188\n",
      "TEST ACCURACY: \n",
      "0.2284\n",
      "\n",
      "Epoch 87 Complete. Training Loss: 24.538393020629883\n",
      "TEST ACCURACY: \n",
      "0.2246\n",
      "\n",
      "Epoch 88 Complete. Training Loss: 22.317289352416992\n",
      "TEST ACCURACY: \n",
      "0.2275\n",
      "\n",
      "Epoch 89 Complete. Training Loss: 20.352928161621094\n",
      "TEST ACCURACY: \n",
      "0.2261\n",
      "\n",
      "Epoch 90 Complete. Training Loss: 23.119714736938477\n",
      "TEST ACCURACY: \n",
      "0.2282\n",
      "TRAIN ACCURACY: \n",
      "0.22756363\n",
      "\n",
      "Epoch 91 Complete. Training Loss: 18.149219512939453\n",
      "TEST ACCURACY: \n",
      "0.2272\n",
      "\n",
      "Keyboard Interrupted\n",
      "Finished Training the Model\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "try:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        # Epoch == Entire Training Set\n",
    "        for epoch in range(num_epochs):\n",
    "            num_batches = mnist.train.num_examples // batch_size\n",
    "\n",
    "            # 150 batch size\n",
    "            for iteration in range(num_batches):\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "                sess.run(train, feed_dict={X: X_batch, Y: y_batch})\n",
    "\n",
    "            training_loss = loss.eval(feed_dict={X: X_batch, Y: y_batch})   \n",
    "            print(\"Epoch {} Complete. Training Loss: {}\".format(epoch,training_loss))\n",
    "            saver.save(sess, \"../save/slearn_2-2.ckpt\")\n",
    "            \n",
    "            matches = tf.equal(tf.argmax(out_layer4, 1), tf.argmax(Y,1))\n",
    "            acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "\n",
    "            print('TEST ACCURACY: ')\n",
    "            print (sess.run(acc, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))\n",
    "            if epoch % 5 == 0:\n",
    "                print('TRAIN ACCURACY: ')\n",
    "                print (sess.run(acc, feed_dict={X:mnist.train.images, Y:mnist.train.labels}))\n",
    "            print()\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print('Keyboard Interrupted')\n",
    "finally:\n",
    "    print('Finished Training the Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../save/slearn_2-2.ckpt\n",
      "TRAIN ACCURACY: \n",
      "0.22723636\n",
      "TEST ACCURACY: \n",
      "0.2272\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_test_labels = 10\n",
    "start_point = 15\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "#     sess.run(init)\n",
    "    saver.restore(sess,\"../save/slearn_2-2.ckpt\")\n",
    "    \n",
    "    params = sess.run([W1, W2, W3, b1, b2, b3])\n",
    "    \n",
    "    matches = tf.equal(tf.argmax(out_layer4, 1), tf.argmax(Y,1))\n",
    "    acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "    \n",
    "    print('TRAIN ACCURACY: ')\n",
    "    print (sess.run(acc, feed_dict={X:mnist.train.images, Y:mnist.train.labels}))\n",
    "\n",
    "    print('TEST ACCURACY: ')\n",
    "    print (sess.run(acc, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.47204027e-01, -1.63612232e-01, -7.89847821e-02,\n",
       "         -4.25348401e-01, -1.38225392e-01, -9.42045078e-02,\n",
       "         -1.81360096e-01, -1.62829198e-02, -1.15787296e-03,\n",
       "         -1.49353161e-01, -3.11957896e-01, -1.10288970e-01,\n",
       "         -3.19613591e-02, -1.62331983e-01, -5.58279939e-02,\n",
       "         -1.59294918e-01, -6.59467056e-02, -3.56064700e-02,\n",
       "         -3.52460057e-01, -4.30803783e-02, -1.23826161e-01,\n",
       "         -9.70576629e-02, -1.23359226e-01, -1.80440739e-01,\n",
       "         -1.11833684e-01, -9.85604227e-02, -2.71983027e-01,\n",
       "         -3.32316279e-01, -2.73337156e-01, -1.34534109e-02,\n",
       "         -1.54184878e-01, -1.74017325e-01, -8.31290409e-02,\n",
       "         -1.04702219e-01, -7.40552023e-02, -8.44248757e-03,\n",
       "         -5.52955829e-02, -3.41705948e-01, -1.73021987e-01,\n",
       "          5.16400337e-02, -3.24476585e-02, -1.05944194e-01,\n",
       "         -5.48586366e-04, -5.29333465e-02, -5.57320453e-02,\n",
       "         -1.68555632e-01, -2.45512798e-02, -5.84157184e-02,\n",
       "         -1.03683338e-01, -1.47968471e-01, -9.04036537e-02,\n",
       "         -1.60583958e-01, -3.21168751e-01,  3.26075256e-01,\n",
       "         -7.13439509e-02, -3.67458947e-02, -8.82331356e-02,\n",
       "         -9.72316489e-02, -1.57325760e-01, -4.10553217e-01,\n",
       "         -1.19881317e-01, -3.30125362e-01, -1.03455327e-01,\n",
       "         -1.22388311e-01, -1.23182312e-01, -1.44333243e-01,\n",
       "         -1.74962461e-01, -2.43512273e-01, -1.62783593e-01,\n",
       "         -1.59981668e-01, -1.92765549e-01, -1.79466233e-01,\n",
       "         -1.23949155e-01, -9.69644487e-02, -4.71536741e-02,\n",
       "         -9.44001041e-03, -9.78582948e-02, -1.86760589e-01,\n",
       "         -2.43051708e-01, -1.38961703e-01, -1.00157902e-01,\n",
       "         -6.88705295e-02, -3.39847237e-01, -9.86346528e-02,\n",
       "         -1.32756695e-01, -1.68518558e-01, -1.24485359e-01,\n",
       "         -8.64889473e-02, -9.75486562e-02, -1.28417507e-01,\n",
       "         -9.48934723e-03, -1.66137531e-01, -1.31608590e-01,\n",
       "         -5.69322929e-02, -3.83312702e-01, -8.99364054e-02,\n",
       "         -8.96228186e-04, -7.56929517e-02, -1.64611086e-01,\n",
       "         -3.17064047e-01, -3.03598017e-01, -5.98468296e-02,\n",
       "         -2.84277052e-01, -2.98668265e-01, -8.80588740e-02,\n",
       "         -3.62661242e-01, -2.80927926e-01, -1.35559723e-01,\n",
       "         -5.36645018e-02, -3.15765850e-02, -3.19659829e-01,\n",
       "         -6.65461719e-02, -8.85707512e-02, -7.01991767e-02,\n",
       "         -2.22859859e-01, -2.53468871e-01, -2.14694142e-01,\n",
       "         -3.10366005e-01, -4.25579548e-01, -9.68566164e-02,\n",
       "         -1.34591624e-01, -1.12940945e-01, -2.31440559e-01,\n",
       "         -3.65583688e-01, -9.09371972e-02, -2.03900151e-02,\n",
       "         -3.97825092e-01, -5.66721745e-02, -1.09533951e-01,\n",
       "         -3.07408094e-01, -8.02159756e-02, -7.17942044e-02,\n",
       "         -3.26166034e-01, -4.30003256e-02, -2.49403194e-02,\n",
       "         -3.27152580e-01, -8.76838043e-02, -6.68743327e-02,\n",
       "         -3.77626330e-01, -1.58325717e-01, -1.04356416e-01,\n",
       "         -1.04099996e-01, -9.49116796e-02, -2.74024665e-01,\n",
       "         -7.85288513e-02, -1.60234705e-01, -1.15117557e-01,\n",
       "         -1.13882996e-01, -3.55584234e-01, -1.63890168e-01,\n",
       "         -4.43675630e-02, -1.75640061e-02, -4.50637549e-01,\n",
       "         -5.15844151e-02, -1.67640358e-01],\n",
       "        [ 3.86695534e-01,  4.29798812e-01,  2.07487941e-01,\n",
       "          2.88354516e-01,  3.63109231e-01,  3.04844171e-01,\n",
       "          5.79896986e-01,  4.27741855e-02,  3.04165762e-03,\n",
       "          1.29083320e-01,  8.25742662e-01,  2.89722025e-01,\n",
       "          8.39604363e-02,  4.26435679e-01,  1.46656543e-01,\n",
       "          3.07945132e-01,  1.73237756e-01,  9.35359076e-02,\n",
       "          9.26525474e-01,  1.13169380e-01,  3.25283349e-01,\n",
       "          2.54964232e-01,  3.24056745e-01,  4.74006206e-01,\n",
       "          2.93779880e-01,  2.58911908e-01,  6.25381470e-01,\n",
       "          8.60163212e-01,  6.68953836e-01,  3.53412442e-02,\n",
       "          4.05040979e-01,  4.57132310e-01,  2.18374640e-01,\n",
       "          2.75045991e-01,  1.94538251e-01,  2.21778713e-02,\n",
       "          1.45257935e-01,  9.97756302e-01,  4.54517633e-01,\n",
       "          1.11861102e-01,  8.52379054e-02,  2.78308570e-01,\n",
       "          1.44110108e-03,  1.39052495e-01,  1.46404490e-01,\n",
       "          4.42784786e-01,  6.44946322e-02,  1.53454334e-01,\n",
       "          2.72369444e-01,  3.88703644e-01,  2.37484589e-01,\n",
       "          4.21843708e-01,  7.66616523e-01, -1.12904213e-01,\n",
       "          1.87415972e-01,  9.65290964e-02,  2.31782764e-01,\n",
       "          2.55421281e-01,  4.13284659e-01,  9.70970333e-01,\n",
       "          3.14920485e-01,  9.98536706e-01,  2.71770477e-01,\n",
       "          3.21506232e-01,  3.23592007e-01,  3.79154146e-01,\n",
       "          4.59615111e-01,  4.49992269e-01,  4.27622020e-01,\n",
       "          4.20261562e-01,  8.54216695e-01,  4.71446246e-01,\n",
       "          3.25606436e-01,  2.54719377e-01,  1.23869665e-01,\n",
       "          2.47983001e-02,  2.57067442e-01,  3.14379424e-01,\n",
       "          6.66950464e-01,  5.68383813e-01,  2.63108343e-01,\n",
       "          1.80918455e-01,  7.18349576e-01,  2.59106904e-01,\n",
       "          3.48743290e-01,  4.42687392e-01,  3.27015013e-01,\n",
       "          2.27200896e-01,  2.56254047e-01,  3.37344527e-01,\n",
       "          2.49279048e-02,  4.36432600e-01,  3.45727295e-01,\n",
       "          1.49557471e-01,  8.92642081e-01,  2.36257136e-01,\n",
       "          2.35433388e-03,  1.98840514e-01,  4.32422727e-01,\n",
       "          9.75460470e-01,  7.97173142e-01,  1.57213777e-01,\n",
       "          7.95204043e-01,  6.85164571e-01,  2.31325001e-01,\n",
       "          9.08363521e-01,  9.21620071e-01,  3.56106669e-01,\n",
       "          1.40973195e-01,  8.29496607e-02,  7.67416656e-01,\n",
       "          1.74812496e-01,  2.32669652e-01,  1.84408709e-01,\n",
       "          1.07471597e+00,  5.68413854e-01,  6.05079532e-01,\n",
       "          8.88882577e-01,  8.85267198e-01,  2.54436105e-01,\n",
       "          3.53563547e-01,  2.96688616e-01,  8.05135846e-01,\n",
       "          9.66808975e-01,  2.38886178e-01,  5.35632595e-02,\n",
       "          9.20202553e-01,  1.48874149e-01,  2.87738621e-01,\n",
       "          8.86577010e-01,  2.10722193e-01,  1.88598752e-01,\n",
       "          7.79430866e-01,  1.12959094e-01,  6.55166134e-02,\n",
       "          9.01320577e-01,  2.30339706e-01,  1.75674573e-01,\n",
       "          9.34265375e-01,  4.15911466e-01,  2.74137586e-01,\n",
       "          2.73463994e-01,  2.49326885e-01,  7.11586714e-01,\n",
       "          2.06290230e-01,  4.20926243e-01,  3.02406400e-01,\n",
       "          2.99163282e-01,  7.60625482e-01,  4.30528939e-01,\n",
       "          1.16550736e-01,  4.61395122e-02,  8.58297944e-01,\n",
       "          1.35508940e-01,  4.40380424e-01]], dtype=float32),\n",
       " array([[ 0.12928236,  0.00709345,  0.2270143 , ..., -0.11943059,\n",
       "          0.70338863,  0.65438473],\n",
       "        [ 0.5483187 ,  0.02092948,  0.75045013, ...,  0.6519301 ,\n",
       "          0.2481015 , -0.07473479],\n",
       "        [ 0.6927793 ,  0.39143902,  0.849975  , ..., -0.02917881,\n",
       "          0.7804612 ,  0.8315323 ],\n",
       "        ...,\n",
       "        [ 0.75967515,  0.06560745, -0.07362074, ...,  0.61350286,\n",
       "         -0.23335783, -0.08361027],\n",
       "        [ 0.40146548,  0.82552207,  0.22120453, ...,  0.37094074,\n",
       "          0.6960912 ,  0.29183385],\n",
       "        [ 0.7315277 ,  0.3760425 ,  0.20362827, ...,  0.39146245,\n",
       "          0.6885634 ,  0.46912655]], dtype=float32),\n",
       " array([[ 0.57482225, -0.06516784],\n",
       "        [ 0.06134865,  0.8566068 ],\n",
       "        [ 0.37494197,  0.3500001 ],\n",
       "        [ 0.10936419,  0.45058444],\n",
       "        [ 0.0069374 ,  0.7158369 ],\n",
       "        [-1.2362164 , -0.8127782 ],\n",
       "        [-0.6541128 , -0.9321995 ],\n",
       "        [-1.2352173 , -1.0699955 ],\n",
       "        [ 0.11362938,  0.4592496 ],\n",
       "        [ 0.19847476,  1.3302188 ],\n",
       "        [ 0.32163742,  0.679948  ],\n",
       "        [ 0.71722955,  0.16408278],\n",
       "        [ 0.67096   ,  0.15763065],\n",
       "        [ 0.54059774,  0.18572156],\n",
       "        [ 0.8533497 ,  0.49491206],\n",
       "        [ 0.03445137,  0.7236657 ],\n",
       "        [ 0.44364232,  0.95404243],\n",
       "        [ 0.525709  , -0.09010807],\n",
       "        [ 0.25547418,  0.73829997],\n",
       "        [ 0.34375206,  1.2075696 ],\n",
       "        [-1.0275891 , -0.9203052 ],\n",
       "        [ 0.2261106 ,  1.0469728 ],\n",
       "        [-1.0385106 , -0.82115996],\n",
       "        [-0.61035043, -1.0942994 ],\n",
       "        [ 0.539066  , -0.06404839],\n",
       "        [ 0.5277885 , -0.04380083],\n",
       "        [ 0.119352  ,  0.4278459 ],\n",
       "        [ 0.27909547,  0.5659961 ],\n",
       "        [ 0.79385746,  0.69905496],\n",
       "        [ 0.23986459,  1.2070447 ],\n",
       "        [ 0.2852913 ,  0.48834848]], dtype=float32),\n",
       " array([-0.28383029, -0.31458557, -0.15186486,  0.02627035, -0.26639026,\n",
       "         0.55831397,  0.408386  , -0.03149702, -0.00305842, -0.08244827,\n",
       "         0.1294441 , -0.21263544, -0.06160916, -0.31217527, -0.10744675,\n",
       "         0.46646196, -0.12664579, -0.06842119,  0.13242085, -0.08283355,\n",
       "        -0.23805155, -0.1870908 , -0.23751526, -0.3471313 , -0.21489719,\n",
       "        -0.18942702,  0.08842366,  0.2200497 ,  0.15427153, -0.02596493,\n",
       "         0.3951552 , -0.33435127, -0.15995553, -0.20170794, -0.14258547,\n",
       "        -0.01627403, -0.10622326,  0.01148064, -0.3327444 ,  0.6309871 ,\n",
       "        -0.06249094, -0.20345554, -0.00187809, -0.10168617, -0.10713416,\n",
       "        -0.32413107, -0.04729197, -0.11250894, -0.19976912, -0.28495467,\n",
       "        -0.17469911, -0.30920935,  0.21520306,  0.70323765, -0.13741975,\n",
       "        -0.07063854, -0.16984136, -0.18727449, -0.30248228,  0.05160596,\n",
       "        -0.23112178,  0.22489011, -0.19898465, -0.23572895, -0.23653843,\n",
       "        -0.27749056, -0.33645573,  0.04531675, -0.31371927, -0.30730048,\n",
       "        -0.06949659, -0.34579587, -0.23848681, -0.1865959 , -0.09067887,\n",
       "        -0.01844228, -0.18869911,  0.4354983 ,  0.29642573,  0.42704958,\n",
       "        -0.19297187, -0.13239871,  0.35539573, -0.19007044, -0.25604025,\n",
       "        -0.32444695, -0.2393859 , -0.16607864, -0.18790606, -0.24759218,\n",
       "        -0.01826485, -0.31940347, -0.25278816, -0.10948997,  0.14138189,\n",
       "        -0.17285003, -0.00264572, -0.14541171, -0.31624797,  0.12610611,\n",
       "         0.12276934, -0.11497195,  0.20893893,  0.2719466 , -0.17002307,\n",
       "        -0.17280695,  0.16932449, -0.26094735, -0.10315148, -0.06065961,\n",
       "         0.18976559, -0.12804289, -0.17049542, -0.13507186, -0.13124591,\n",
       "         0.38912013,  0.29405993, -0.00732893, -0.10557485, -0.18614395,\n",
       "        -0.25848284, -0.21713065,  0.41024873,  0.13189249, -0.17476822,\n",
       "        -0.0395139 ,  0.13349214, -0.1089573 , -0.21143371,  0.28585228,\n",
       "        -0.15427087, -0.13839707,  0.10054658, -0.08271152, -0.04803753,\n",
       "         0.08208816, -0.16844337, -0.12841488,  0.14108734, -0.30410913,\n",
       "        -0.20056294, -0.20006253, -0.18251912,  0.17823128, -0.15121455,\n",
       "        -0.30812535, -0.22134608, -0.21894099,  0.13931683, -0.31525296,\n",
       "        -0.08532681, -0.03395198,  0.14848828, -0.09930573, -0.32260868],\n",
       "       dtype=float32),\n",
       " array([-0.0016851 ,  0.20803913, -0.03794378, -0.1261838 ,  0.14734754,\n",
       "        -0.80236775,  1.3694651 , -1.108322  , -0.3242387 , -0.29094225,\n",
       "        -0.16830452, -0.00180403, -0.00171605, -0.00238488, -0.00246153,\n",
       "         0.1843792 , -0.26121667, -0.00164058, -0.03963173,  0.11434846,\n",
       "        -0.9969773 ,  0.07313187, -0.94354445,  1.3514565 , -0.00163347,\n",
       "        -0.0016225 , -0.36221963,  0.00500064, -0.03398295, -0.40402508,\n",
       "        -0.14030187], dtype=float32),\n",
       " array([[-0.7436718, -0.9111148]], dtype=float32)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(params)):\n",
    "    np.save('../save/weights/slearn_parms{}_2-2.npy'.format(i),params[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
