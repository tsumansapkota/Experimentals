{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-b22782e32495>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE parameters\n",
    "num_inputs = 784 # 28*28\n",
    "neurons_hid1 = 392\n",
    "neurons_hid2 = 196\n",
    "neurons_hid3 = 98\n",
    "neurons_hid4 = 49\n",
    "neurons_hid5 = 25\n",
    "neurons_hid6 = 12\n",
    "neurons_hid7 = 6\n",
    "neurons_hid8 = 3\n",
    "neurons_hid9 = 2\n",
    "neurons_hid10 = 1\n",
    "\n",
    "learning_rate = 0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PINV(PSEUDO-INVERSE) function\n",
    "def pinv(a, rcond=1e-15):\n",
    "    s, u, v = tf.svd(a)\n",
    "    # Ignore singular values close to zero to prevent numerical overflow\n",
    "    limit = rcond * tf.reduce_max(s)\n",
    "    non_zero = tf.greater(s, limit)\n",
    "\n",
    "    reciprocal = tf.where(non_zero, tf.reciprocal(s), tf.zeros(s.shape))\n",
    "    lhs = tf.matmul(v, tf.diag(reciprocal))\n",
    "    return tf.matmul(lhs, u, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSPOSE OR PINV\n",
    "tie_weight = tf.transpose #pinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLACEHOLDER DEFINATION\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WEIGHTS DEFINATION\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "w1 = tf.Variable(np.load('./save/weights/w1.npy'))\n",
    "w2 = tf.Variable(np.load('./save/weights/w2.npy'))\n",
    "w3 = tf.Variable(np.load('./save/weights/w3.npy'))\n",
    "w4 = tf.Variable(np.load('./save/weights/w4.npy'))\n",
    "w5 = tf.Variable(np.load('./save/weights/w5.npy'))\n",
    "w6 = tf.Variable(np.load('./save/weights/w6.npy'))\n",
    "w7 = tf.Variable(np.load('./save/weights/w7.npy'))\n",
    "w8 = tf.Variable(np.load('./save/weights/w8.npy'))\n",
    "w9 = tf.Variable(np.load('./save/weights/w9.npy'))\n",
    "w10 = tf.Variable(np.load('./save/weights/w10.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTIVATION FUNCTION  [ lambda X:X  <OR>  tf.nn.relu  ]\n",
    "act_func = lambda X:X #tf.nn.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYER MODELING OF :NN\n",
    "hid_layer1 = act_func(tf.matmul(X, w1))\n",
    "hid_layer2 = act_func(tf.matmul(hid_layer1, w2))\n",
    "hid_layer3 = act_func(tf.matmul(hid_layer2, w3))\n",
    "hid_layer4 = act_func(tf.matmul(hid_layer3, w4))\n",
    "hid_layer5 = act_func(tf.matmul(hid_layer4, w5))\n",
    "hid_layer6 = act_func(tf.matmul(hid_layer5, w6))\n",
    "hid_layer7 = act_func(tf.matmul(hid_layer6, w7))\n",
    "hid_layer8 = act_func(tf.matmul(hid_layer7, w8))\n",
    "hid_layer9 = act_func(tf.matmul(hid_layer8, w9))\n",
    "\n",
    "hid_layer10 = act_func(tf.matmul(hid_layer9, w10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = tf.placeholder(tf.float32, shape=[None, neurons_hid9])\n",
    "YY = tf.placeholder(tf.float32, shape=[None, neurons_hid10])\n",
    "\n",
    "ww10 = tf.Variable(initializer([neurons_hid9, neurons_hid10]), dtype=tf.float32)\n",
    "yy = tf.matmul(XX,ww10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTIONS\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "loss = tf.reduce_mean(tf.square(yy - YY))\n",
    "# loss = tf.reduce_mean(tf.abs(output_layer - X))\n",
    "# cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y,logits=yout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "train = optimizer.minimize(loss, var_list=[ww10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARATION\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Complete. Training Loss: 122.4677963256836\n",
      "Epoch 1 Complete. Training Loss: 104.9327392578125\n",
      "Epoch 2 Complete. Training Loss: 87.79065704345703\n",
      "Epoch 3 Complete. Training Loss: 73.78736114501953\n",
      "Epoch 4 Complete. Training Loss: 57.59215545654297\n",
      "Epoch 5 Complete. Training Loss: 47.2044792175293\n",
      "Epoch 6 Complete. Training Loss: 38.66794967651367\n",
      "Epoch 7 Complete. Training Loss: 28.8687801361084\n",
      "Epoch 8 Complete. Training Loss: 20.530502319335938\n",
      "Epoch 9 Complete. Training Loss: 15.582742691040039\n",
      "Epoch 10 Complete. Training Loss: 11.892380714416504\n",
      "Epoch 11 Complete. Training Loss: 9.250951766967773\n",
      "Epoch 12 Complete. Training Loss: 6.019069194793701\n",
      "Epoch 13 Complete. Training Loss: 3.805098056793213\n",
      "Epoch 14 Complete. Training Loss: 2.5428600311279297\n",
      "Epoch 15 Complete. Training Loss: 1.7146263122558594\n",
      "Epoch 16 Complete. Training Loss: 1.256859302520752\n",
      "Epoch 17 Complete. Training Loss: 1.1368473768234253\n",
      "Epoch 18 Complete. Training Loss: 0.77633136510849\n",
      "Epoch 19 Complete. Training Loss: 0.5566349625587463\n",
      "Epoch 20 Complete. Training Loss: 0.6120555400848389\n",
      "Epoch 21 Complete. Training Loss: 0.45589280128479004\n",
      "Epoch 22 Complete. Training Loss: 0.6001095771789551\n",
      "Epoch 23 Complete. Training Loss: 0.41298457980155945\n",
      "Epoch 24 Complete. Training Loss: 0.3775966763496399\n",
      "Epoch 25 Complete. Training Loss: 0.2249627709388733\n",
      "Epoch 26 Complete. Training Loss: 0.24162721633911133\n",
      "Epoch 27 Complete. Training Loss: 0.1402161866426468\n",
      "Epoch 28 Complete. Training Loss: 0.1268620491027832\n",
      "Epoch 29 Complete. Training Loss: 0.06847537308931351\n",
      "Epoch 30 Complete. Training Loss: 0.043965887278318405\n",
      "Epoch 31 Complete. Training Loss: 0.023937813937664032\n",
      "Epoch 32 Complete. Training Loss: 0.010396623983979225\n",
      "Epoch 33 Complete. Training Loss: 0.005333629436790943\n",
      "Epoch 34 Complete. Training Loss: 0.001874030800536275\n",
      "Epoch 35 Complete. Training Loss: 0.0005165959591977298\n",
      "Epoch 36 Complete. Training Loss: 0.0001384249044349417\n",
      "Epoch 37 Complete. Training Loss: 2.3551650883746333e-05\n",
      "Epoch 38 Complete. Training Loss: 2.013123094002367e-06\n",
      "Epoch 39 Complete. Training Loss: 1.8341667384902394e-07\n",
      "Epoch 40 Complete. Training Loss: 8.154933617277038e-09\n",
      "Epoch 41 Complete. Training Loss: 3.027111339370947e-10\n",
      "Epoch 42 Complete. Training Loss: 1.2139594884885696e-10\n",
      "Epoch 43 Complete. Training Loss: 9.255333305313584e-11\n",
      "Epoch 44 Complete. Training Loss: 5.970541422373188e-11\n",
      "Keyboard Interrupted\n",
      "Finished Training the Model\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 150\n",
    "\n",
    "try:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        # Epoch == Entire Training Set\n",
    "        for epoch in range(num_epochs):\n",
    "            num_batches = mnist.train.num_examples // batch_size\n",
    "\n",
    "            # 150 batch size\n",
    "            for iteration in range(num_batches):\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "                ip, op = sess.run([hid_layer9, hid_layer10], feed_dict={X: X_batch})\n",
    "                \n",
    "                sess.run(train, feed_dict={XX:ip, YY:op})\n",
    "\n",
    "            training_loss = loss.eval(feed_dict={XX:ip, YY:op})   \n",
    "            print(\"Epoch {} Complete. Training Loss: {}\".format(epoch,training_loss))\n",
    "            saver.save(sess, \"./save/aeRev_10_1.ckpt\")      \n",
    "except KeyboardInterrupt:\n",
    "    print('Keyboard Interrupted')\n",
    "finally:\n",
    "    print('Finished Training the Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save/aeRev_10_1.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_test_images = 10\n",
    "start_point = 15\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver.restore(sess,\"./save/aeRev_10_1.ckpt\")\n",
    "    weight10 = w10.eval()\n",
    "    weigh_10 = ww10.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual\n",
      "[[-0.47105718]\n",
      " [ 1.2374369 ]]\n",
      "\n",
      "Computed\n",
      "[[-0.47105303]\n",
      " [ 1.2374338 ]]\n",
      "\n",
      "Difference\n",
      "[[-4.1425228e-06]\n",
      " [ 3.0994415e-06]]\n"
     ]
    }
   ],
   "source": [
    "print('Actual')\n",
    "print(weight10)\n",
    "print('\\nComputed')\n",
    "print(weigh_10)\n",
    "print('\\nDifference')\n",
    "print(weight10-weigh_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.47105718]\n",
      " [ 1.2374369 ]]\n"
     ]
    }
   ],
   "source": [
    "print(weight10)\n",
    "np.save('./save/weights/w10.npy',weight10)\n",
    "#wt1 = np.load('./save/weights/w1.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
