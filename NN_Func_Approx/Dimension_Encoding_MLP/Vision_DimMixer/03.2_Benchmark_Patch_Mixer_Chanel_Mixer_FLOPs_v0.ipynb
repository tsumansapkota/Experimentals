{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os, sys, pathlib, random, time, pickle, copy, json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED = 147\n",
    "# SEED = 258\n",
    "SEED = 369\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpBLock(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_layers_ratio=[2], actf=nn.GELU):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        #### convert hidden layers ratio to list if integer is inputted\n",
    "        if isinstance(hidden_layers_ratio, int):\n",
    "            hidden_layers_ratio = [hidden_layers_ratio]\n",
    "            \n",
    "        self.hlr = [1]+hidden_layers_ratio+[1]\n",
    "        \n",
    "        self.mlp = []\n",
    "        ### for 1 hidden layer, we iterate 2 times\n",
    "        for h in range(len(self.hlr)-1):\n",
    "            i, o = int(self.hlr[h]*self.input_dim),\\\n",
    "                    int(self.hlr[h+1]*self.input_dim)\n",
    "            self.mlp.append(nn.Linear(i, o))\n",
    "            self.mlp.append(actf())\n",
    "        self.mlp = self.mlp[:-1]\n",
    "        \n",
    "        self.mlp = nn.Sequential(*self.mlp)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MlpBLock(\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=6, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=6, out_features=8, bias=True)\n",
       "    (3): GELU(approximate='none')\n",
       "    (4): Linear(in_features=8, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MlpBLock(2, [3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP-Mixer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, patch_dim, channel_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln0 = nn.LayerNorm(channel_dim)\n",
    "        self.mlp_patch = MlpBLock(patch_dim, [2])\n",
    "        self.ln1 = nn.LayerNorm(channel_dim)\n",
    "        self.mlp_channel = MlpBLock(channel_dim, [2])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## x has shape-> N, nP, nC/hidden_dims; C=Channel, P=Patch\n",
    "        \n",
    "        ######## !!!! Can use same mixer on shape of -> N, C, P;\n",
    "        \n",
    "        #### mix per patch\n",
    "        y = self.ln0(x) ### per channel layer normalization ?? \n",
    "        y = torch.swapaxes(y, -1, -2)\n",
    "        y = self.mlp_patch(y)\n",
    "        y = torch.swapaxes(y, -1, -2)\n",
    "        x = x+y\n",
    "        \n",
    "        #### mix per channel \n",
    "        y = self.ln1(x)\n",
    "        y = self.mlp_channel(y)\n",
    "        x = x+y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpMixer(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_dim:tuple, patch_size:tuple, hidden_expansion:float, num_blocks:int, num_classes:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_dim = image_dim ### must contain (C, H, W) or (H, W)\n",
    "        self.scaler = nn.UpsamplingBilinear2d(size=(self.img_dim[-2], self.img_dim[-1]))\n",
    "        \n",
    "        ### find patch dim\n",
    "        d0 = int(image_dim[-2]/patch_size[0])\n",
    "        d1 = int(image_dim[-1]/patch_size[1])\n",
    "        assert d0*patch_size[0]==image_dim[-2], \"Image must be divisible into patch size\"\n",
    "        assert d1*patch_size[1]==image_dim[-1], \"Image must be divisible into patch size\"\n",
    "#         self.d0, self.d1 = d0, d1 ### number of patches in each axis\n",
    "        __patch_size = patch_size[0]*patch_size[1]*image_dim[0] ## number of channels in each patch\n",
    "    \n",
    "        ### find channel dim\n",
    "        channel_size = d0*d1 ## number of patches\n",
    "        \n",
    "        ### after the number of channels are changed\n",
    "        init_dim = __patch_size\n",
    "#         final_dim = int(patch_size[0]*patch_size[1]*hidden_expansion)\n",
    "        final_dim = int(init_dim*hidden_expansion)\n",
    "\n",
    "        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        #### rescale the patches (patch wise image non preserving transform, unlike bilinear interpolation)\n",
    "        self.channel_change = nn.Linear(init_dim, final_dim)\n",
    "        print(f\"MLP Mixer : Channes per patch -> Initial:{init_dim} Final:{final_dim}\")\n",
    "        \n",
    "        \n",
    "        self.channel_dim = final_dim\n",
    "        self.patch_dim = channel_size\n",
    "        \n",
    "        self.mixer_blocks = []\n",
    "        for i in range(num_blocks):\n",
    "            self.mixer_blocks.append(MixerBlock(self.patch_dim, self.channel_dim))\n",
    "        self.mixer_blocks = nn.Sequential(*self.mixer_blocks)\n",
    "        \n",
    "        self.linear = nn.Linear(self.patch_dim*self.channel_dim, num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.scaler(x)\n",
    "        x = self.unfold(x).swapaxes(-1, -2)\n",
    "        x = self.channel_change(x)\n",
    "        x = self.mixer_blocks(x)\n",
    "        x = self.linear(x.view(bs, -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Mixer : Channes per patch -> Initial:48 Final:120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MlpMixer(\n",
       "  (scaler): UpsamplingBilinear2d(size=(32, 32), mode='bilinear')\n",
       "  (unfold): Unfold(kernel_size=(4, 4), dilation=1, padding=0, stride=(4, 4))\n",
       "  (channel_change): Linear(in_features=48, out_features=120, bias=True)\n",
       "  (mixer_blocks): Sequential(\n",
       "    (0): MixerBlock(\n",
       "      (ln0): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=120, out_features=240, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=240, out_features=120, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=7680, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixer = MlpMixer((3, 32, 32), (4, 4), hidden_expansion=2.5, num_blocks=1, num_classes=10)\n",
    "mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  157706\n"
     ]
    }
   ],
   "source": [
    "print(\"number of params: \", sum(p.numel() for p in mixer.parameters())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1648, -0.0510, -0.0486, -0.1505, -0.3137, -0.5226,  0.0023,  0.5233,\n",
       "         -0.2587, -0.5398]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixer(torch.randn(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMixerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, patch_size, num_channel):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "#         self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        ps = None\n",
    "        if isinstance(patch_size, int):\n",
    "            ps = patch_size**2\n",
    "        else:\n",
    "            ps = patch_size[0]*patch_size[1]\n",
    "        ps = ps*num_channel\n",
    "        \n",
    "        self.ln0 = nn.LayerNorm(ps)\n",
    "        self.mlp_patch = MlpBLock(ps, [2])\n",
    "        \n",
    "#         self.fold = nn.Fold(kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## x has shape-> N, C, H, W; C=Channel\n",
    "        \n",
    "        sz = x.shape\n",
    "        \n",
    "        y = nn.functional.unfold(x, \n",
    "                                 kernel_size=self.patch_size, \n",
    "                                 stride=self.patch_size\n",
    "                                )\n",
    "        #### mix per patch\n",
    "        y = torch.swapaxes(y, -1, -2)\n",
    "        y = self.ln0(y) \n",
    "        y = self.mlp_patch(y)\n",
    "        y = torch.swapaxes(y, -1, -2)\n",
    "        \n",
    "        y = nn.functional.fold(y, (sz[-2], sz[-1]), \n",
    "                               kernel_size=self.patch_size, \n",
    "                               stride=self.patch_size\n",
    "                              )\n",
    "        x = x+y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatchMixerBlock(\n",
       "  (ln0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp_patch): MlpBLock(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=192, out_features=384, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=384, out_features=192, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmb = PatchMixerBlock(8, 3)\n",
    "pmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pmb(torch.randn(1, 3, 35, 35)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_factors(n):\n",
    "    facts = []\n",
    "    for i in range(2, n+1):\n",
    "        if n%i == 0:\n",
    "            facts.append(i)\n",
    "    return facts\n",
    "\n",
    "class PatchMlpMixer(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_dim:tuple, patch_sizes:tuple, hidden_channels:int, num_blocks:int, num_classes:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_dim = image_dim ### must contain (C, H, W)\n",
    "        self.target_dim = np.prod(patch_sizes)\n",
    "        \n",
    "        ### find number of channel for input, the channel is \n",
    "        num_channel = image_dim[0]\n",
    "        \n",
    "        self.conv1x1 = nn.Conv2d(num_channel, hidden_channels, kernel_size=1, stride=1)\n",
    "        if num_channel == hidden_channels:\n",
    "            self.conv1x1 = nn.Identity()\n",
    "        \n",
    "        self.mixer_blocks = []\n",
    "        for i in range(num_blocks):\n",
    "            for ps in patch_sizes:\n",
    "                self.mixer_blocks.append(PatchMixerBlock(ps, hidden_channels))\n",
    "                \n",
    "        self.mixer_blocks = nn.Sequential(*self.mixer_blocks)\n",
    "        self.linear = nn.Linear(self.target_dim*self.target_dim*hidden_channels, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        \n",
    "        x = nn.functional.interpolate(x, size=self.target_dim, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        x = self.conv1x1(x) \n",
    "        x = self.mixer_blocks(x)\n",
    "        x = self.linear(x.view(bs, -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*3*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_mixer = PatchMlpMixer((3, 35, 35), patch_sizes=[5, 7], hidden_channels=3, num_blocks=1, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatchMlpMixer(\n",
       "  (conv1x1): Identity()\n",
       "  (mixer_blocks): Sequential(\n",
       "    (0): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((75,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=75, out_features=150, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=150, out_features=75, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=147, out_features=294, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=294, out_features=147, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=3675, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  146806\n"
     ]
    }
   ],
   "source": [
    "print(\"number of params: \", sum(p.numel() for p in patch_mixer.parameters())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_mixer(torch.randn(1, 3, 32, 32)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Mixer : Channes per patch -> Initial:48 Final:144\n",
      "original_mixer0_l7_10_s-1\n",
      "Computational complexity:       74.65 MMac\n",
      "Number of parameters:           896.78 k\n",
      "\n",
      "MLP Mixer : Channes per patch -> Initial:48 Final:153\n",
      "original_mixer1_l7_10_s-1\n",
      "Computational complexity:       60.48 MMac\n",
      "Number of parameters:           884.41 k\n",
      "\n",
      "patchonly_mixer0_l7_10_s-1\n",
      "Computational complexity:       23.04 MMac\n",
      "Number of parameters:           807.08 k\n",
      "\n",
      "MLP Mixer : Channes per patch -> Initial:48 Final:144\n",
      "original_mixer0_l10_10_s-1\n",
      "Computational complexity:       106.36 MMac\n",
      "Number of parameters:           1.23 M  \n",
      "\n",
      "MLP Mixer : Channes per patch -> Initial:48 Final:153\n",
      "original_mixer1_l10_10_s-1\n",
      "Computational complexity:       86.16 MMac\n",
      "Number of parameters:           1.22 M  \n",
      "\n",
      "patchonly_mixer0_l10_10_s-1\n",
      "Computational complexity:       32.9 MMac\n",
      "Number of parameters:           1.14 M  \n",
      "\n",
      "MLP Mixer : Channes per patch -> Initial:48 Final:144\n",
      "original_mixer0_l7_100_s-1\n",
      "Computational complexity:       75.7 MMac\n",
      "Number of parameters:           1.95 M  \n",
      "\n",
      "MLP Mixer : Channes per patch -> Initial:48 Final:153\n",
      "original_mixer1_l7_100_s-1\n",
      "Computational complexity:       61.36 MMac\n",
      "Number of parameters:           1.77 M  \n",
      "\n",
      "patchonly_mixer0_l7_100_s-1\n",
      "Computational complexity:       23.37 MMac\n",
      "Number of parameters:           1.14 M  \n",
      "\n",
      "MLP Mixer : Channes per patch -> Initial:48 Final:144\n",
      "original_mixer0_l10_100_s-1\n",
      "Computational complexity:       107.41 MMac\n",
      "Number of parameters:           2.28 M  \n",
      "\n",
      "MLP Mixer : Channes per patch -> Initial:48 Final:153\n",
      "original_mixer1_l10_100_s-1\n",
      "Computational complexity:       87.04 MMac\n",
      "Number of parameters:           2.1 M   \n",
      "\n",
      "patchonly_mixer0_l10_100_s-1\n",
      "Computational complexity:       33.23 MMac\n",
      "Number of parameters:           1.47 M  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "SEED = -1\n",
    "for num_cls in [10, 100]:\n",
    "    for num_layers in [7, 10]:\n",
    "        for i in range(3):\n",
    "            if i==0:\n",
    "                ## hard core ignore\n",
    "                model = MlpMixer((3, 4*9, 4*9), (4, 4), hidden_expansion=3.0, num_blocks=num_layers, num_classes=num_cls)\n",
    "                model_name = f'original_mixer0_l{num_layers}_{num_cls}_s{SEED}'\n",
    "            elif i == 1:\n",
    "                ### FOR ORIGINAL MIXER V1\n",
    "                model = MlpMixer((3, 32, 32), (4, 4), hidden_expansion=3.2, num_blocks=num_layers, num_classes=num_cls)\n",
    "                model_name = f'original_mixer1_l{num_layers}_{num_cls}_s{SEED}'\n",
    "            elif i == 2:\n",
    "                model = PatchMlpMixer((3, 35, 35), patch_sizes=[5,7], hidden_channels=3, num_blocks=num_layers, num_classes=num_cls)\n",
    "                model_name = f'patchonly_mixer0_l{num_layers}_{num_cls}_s{SEED}'\n",
    "\n",
    "            macs, params = get_model_complexity_info(model, (3, 32, 32), as_strings=True, ignore_modules=['channel_change'],\n",
    "                                           print_per_layer_stat=False, verbose=False)\n",
    "            \n",
    "            print(model_name)\n",
    "            print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "            print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noriginal_mixer0_l7_10_s-1\\nComputational complexity:       74.65 MMac\\nNumber of parameters:           896.78 k\\nchannel_change: 560.02 KMac\\n\\noriginal_mixer1_l7_10_s-1\\nComputational complexity:       60.48 MMac\\nNumber of parameters:           884.41 k\\nchannel_change: 470.17 KMac\\n\\npatchonly_mixer0_l7_10_s-1\\nComputational complexity:       23.04 MMac\\nNumber of parameters:           807.08 k\\n\\noriginal_mixer0_l10_10_s-1\\nComputational complexity:       106.36 MMac\\nNumber of parameters:           1.23 M  \\n\\noriginal_mixer1_l10_10_s-1\\nComputational complexity:       86.16 MMac\\nNumber of parameters:           1.22 M  \\n\\npatchonly_mixer0_l10_10_s-1\\nComputational complexity:       32.9 MMac\\nNumber of parameters:           1.14 M  \\n\\noriginal_mixer0_l7_100_s-1\\nComputational complexity:       75.7 MMac\\nNumber of parameters:           1.95 M  \\n\\noriginal_mixer1_l7_100_s-1\\nComputational complexity:       61.36 MMac\\nNumber of parameters:           1.77 M  \\n\\npatchonly_mixer0_l7_100_s-1\\nComputational complexity:       23.37 MMac\\nNumber of parameters:           1.14 M  \\n\\noriginal_mixer0_l10_100_s-1\\nComputational complexity:       107.41 MMac\\nNumber of parameters:           2.28 M  \\n\\noriginal_mixer1_l10_100_s-1\\nComputational complexity:       87.04 MMac\\nNumber of parameters:           2.1 M   \\n\\npatchonly_mixer0_l10_100_s-1\\nComputational complexity:       33.23 MMac\\nNumber of parameters:           1.47 M  \\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "original_mixer0_l7_10_s-1\n",
    "Computational complexity:       74.65 MMac\n",
    "Number of parameters:           896.78 k\n",
    "channel_change: 560.02 KMac\n",
    "\n",
    "original_mixer1_l7_10_s-1\n",
    "Computational complexity:       60.48 MMac\n",
    "Number of parameters:           884.41 k\n",
    "channel_change: 470.17 KMac\n",
    "\n",
    "patchonly_mixer0_l7_10_s-1\n",
    "Computational complexity:       23.04 MMac\n",
    "Number of parameters:           807.08 k\n",
    "\n",
    "original_mixer0_l10_10_s-1\n",
    "Computational complexity:       106.36 MMac\n",
    "Number of parameters:           1.23 M  \n",
    "\n",
    "original_mixer1_l10_10_s-1\n",
    "Computational complexity:       86.16 MMac\n",
    "Number of parameters:           1.22 M  \n",
    "\n",
    "patchonly_mixer0_l10_10_s-1\n",
    "Computational complexity:       32.9 MMac\n",
    "Number of parameters:           1.14 M  \n",
    "\n",
    "original_mixer0_l7_100_s-1\n",
    "Computational complexity:       75.7 MMac\n",
    "Number of parameters:           1.95 M  \n",
    "\n",
    "original_mixer1_l7_100_s-1\n",
    "Computational complexity:       61.36 MMac\n",
    "Number of parameters:           1.77 M  \n",
    "\n",
    "patchonly_mixer0_l7_100_s-1\n",
    "Computational complexity:       23.37 MMac\n",
    "Number of parameters:           1.14 M  \n",
    "\n",
    "original_mixer0_l10_100_s-1\n",
    "Computational complexity:       107.41 MMac\n",
    "Number of parameters:           2.28 M  \n",
    "\n",
    "original_mixer1_l10_100_s-1\n",
    "Computational complexity:       87.04 MMac\n",
    "Number of parameters:           2.1 M   \n",
    "\n",
    "patchonly_mixer0_l10_100_s-1\n",
    "Computational complexity:       33.23 MMac\n",
    "Number of parameters:           1.47 M  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Mixer : Channes per patch -> Initial:48 Final:153\n"
     ]
    }
   ],
   "source": [
    "L = 7\n",
    "C = 10\n",
    "\n",
    "# model = MlpMixer((3, 4*9, 4*9), (4, 4), hidden_expansion=3.0, num_blocks=L, num_classes=C)\n",
    "# model_name = f'original_mixer0_l7_c10'\n",
    "\n",
    "model = MlpMixer((3, 32, 32), (4, 4), hidden_expansion=3.2, num_blocks=L, num_classes=C)\n",
    "# model_name = f'original_mixer1_l7_c10'\n",
    "\n",
    "# model = PatchMlpMixer((3, 35, 35), patch_sizes=[5,7], hidden_channels=3, num_blocks=L, num_classes=C)\n",
    "# model_name = f'patchonly_mixer0_l7_c10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MlpMixer(\n",
       "  (scaler): UpsamplingBilinear2d(size=(32, 32), mode='bilinear')\n",
       "  (unfold): Unfold(kernel_size=(4, 4), dilation=1, padding=0, stride=(4, 4))\n",
       "  (channel_change): Linear(in_features=48, out_features=153, bias=True)\n",
       "  (mixer_blocks): Sequential(\n",
       "    (0): MixerBlock(\n",
       "      (ln0): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=153, out_features=306, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=306, out_features=153, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): MixerBlock(\n",
       "      (ln0): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=153, out_features=306, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=306, out_features=153, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): MixerBlock(\n",
       "      (ln0): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=153, out_features=306, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=306, out_features=153, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): MixerBlock(\n",
       "      (ln0): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=153, out_features=306, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=306, out_features=153, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): MixerBlock(\n",
       "      (ln0): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=153, out_features=306, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=306, out_features=153, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): MixerBlock(\n",
       "      (ln0): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=153, out_features=306, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=306, out_features=153, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): MixerBlock(\n",
       "      (ln0): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=153, out_features=306, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=306, out_features=153, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=9792, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptflops import get_model_complexity_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module UpsamplingBilinear2d is treated as a zero-op.\n",
      "Warning: module Unfold is treated as a zero-op.\n",
      "Warning: module MlpBLock is treated as a zero-op.\n",
      "Warning: module MixerBlock is treated as a zero-op.\n",
      "Warning: module MlpMixer is treated as a zero-op.\n",
      "MlpMixer(\n",
      "  884.41 k, 100.000% Params, 60.48 MMac, 100.000% MACs, \n",
      "  (scaler): UpsamplingBilinear2d(0, 0.000% Params, 0.0 Mac, 0.000% MACs, size=(32, 32), mode='bilinear')\n",
      "  (unfold): Unfold(0, 0.000% Params, 0.0 Mac, 0.000% MACs, kernel_size=(4, 4), dilation=1, padding=0, stride=(4, 4))\n",
      "  (channel_change): Linear(7.5 k, 0.848% Params, 470.17 KMac, 0.777% MACs, in_features=48, out_features=153, bias=True)\n",
      "  (mixer_blocks): Sequential(\n",
      "    778.98 k, 88.079% Params, 59.91 MMac, 99.061% MACs, \n",
      "    (0): MixerBlock(\n",
      "      111.28 k, 12.583% Params, 8.56 MMac, 14.152% MACs, \n",
      "      (ln0): LayerNorm(306, 0.035% Params, 9.79 KMac, 0.016% MACs, (153,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp_patch): MlpBLock(\n",
      "        16.58 k, 1.874% Params, 2.53 MMac, 4.177% MACs, \n",
      "        (mlp): Sequential(\n",
      "          16.58 k, 1.874% Params, 2.53 MMac, 4.177% MACs, \n",
      "          (0): Linear(8.32 k, 0.941% Params, 1.25 MMac, 2.073% MACs, in_features=64, out_features=128, bias=True)\n",
      "          (1): GELU(0, 0.000% Params, 19.58 KMac, 0.032% MACs, approximate='none')\n",
      "          (2): Linear(8.26 k, 0.934% Params, 1.25 MMac, 2.072% MACs, in_features=128, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm(306, 0.035% Params, 9.79 KMac, 0.016% MACs, (153,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp_channel): MlpBLock(\n",
      "        94.09 k, 10.639% Params, 6.01 MMac, 9.942% MACs, \n",
      "        (mlp): Sequential(\n",
      "          94.09 k, 10.639% Params, 6.01 MMac, 9.942% MACs, \n",
      "          (0): Linear(47.12 k, 5.328% Params, 3.0 MMac, 4.955% MACs, in_features=153, out_features=306, bias=True)\n",
      "          (1): GELU(0, 0.000% Params, 19.58 KMac, 0.032% MACs, approximate='none')\n",
      "          (2): Linear(46.97 k, 5.311% Params, 3.0 MMac, 4.955% MACs, in_features=306, out_features=153, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): MixerBlock(\n",
      "      111.28 k, 12.583% Params, 8.56 MMac, 14.152% MACs, \n",
      "      (ln0): LayerNorm(306, 0.035% Params, 9.79 KMac, 0.016% MACs, (153,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp_patch): MlpBLock(\n",
      "        16.58 k, 1.874% Params, 2.53 MMac, 4.177% MACs, \n",
      "        (mlp): Sequential(\n",
      "          16.58 k, 1.874% Params, 2.53 MMac, 4.177% MACs, \n",
      "          (0): Linear(8.32 k, 0.941% Params, 1.25 MMac, 2.073% MACs, in_features=64, out_features=128, bias=True)\n",
      "          (1): GELU(0, 0.000% Params, 19.58 KMac, 0.032% MACs, approximate='none')\n",
      "          (2): Linear(8.26 k, 0.934% Params, 1.25 MMac, 2.072% MACs, in_features=128, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm(306, 0.035% Params, 9.79 KMac, 0.016% MACs, (153,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp_channel): MlpBLock(\n",
      "        94.09 k, 10.639% Params, 6.01 MMac, 9.942% MACs, \n",
      "        (mlp): Sequential(\n",
      "          94.09 k, 10.639% Params, 6.01 MMac, 9.942% MACs, \n",
      "          (0): Linear(47.12 k, 5.328% Params, 3.0 MMac, 4.955% MACs, in_features=153, out_features=306, bias=True)\n",
      "          (1): GELU(0, 0.000% Params, 19.58 KMac, 0.032% MACs, approximate='none')\n",
      "          (2): Linear(46.97 k, 5.311% Params, 3.0 MMac, 4.955% MACs, in_features=306, out_features=153, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): MixerBlock(\n",
      "      111.28 k, 12.583% Params, 8.56 MMac, 14.152% MACs, \n",
      "      (ln0): LayerNorm(306, 0.035% Params, 9.79 KMac, 0.016% MACs, (153,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp_patch): MlpBLock(\n",
      "        16.58 k, 1.874% Params, 2.53 MMac, 4.177% MACs, \n",
      "        (mlp): Sequential(\n",
      "          16.58 k, 1.874% Params, 2.53 MMac, 4.177% MACs, \n",
      "          (0): Linear(8.32 k, 0.941% Params, 1.25 MMac, 2.073% MACs, in_features=64, out_features=128, bias=True)\n",
      "          (1): GELU(0, 0.000% Params, 19.58 KMac, 0.032% MACs, approximate='none')\n",
      "          (2): Linear(8.26 k, 0.934% Params, 1.25 MMac, 2.072% MACs, in_features=128, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm(306, 0.035% Params, 9.79 KMac, 0.016% MACs, (153,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp_channel): MlpBLock(\n",
      "        94.09 k, 10.639% Params, 6.01 MMac, 9.942% MACs, \n",
      "        (mlp): Sequential(\n",
      "          94.09 k, 10.639% Params, 6.01 MMac, 9.942% MACs, \n",
      "          (0): Linear(47.12 k, 5.328% Params, 3.0 MMac, 4.955% MACs, in_features=153, out_features=306, bias=True)\n",
      "          (1): GELU(0, 0.000% Params, 19.58 KMac, 0.032% MACs, approximate='none')\n",
      "          (2): Linear(46.97 k, 5.311% Params, 3.0 MMac, 4.955% MACs, in_features=306, out_features=153, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): MixerBlock(\n",
      "      111.28 k, 12.583% Params, 8.56 MMac, 14.152% MACs, \n",
      "      (ln0): LayerNorm(306, 0.035% Params, 9.79 KMac, 0.016% MACs, (153,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp_patch): MlpBLock(\n",
      "        16.58 k, 1.874% Params, 2.53 MMac, 4.177% MACs, \n",
      "        (mlp): Sequential(\n",
      "          16.58 k, 1.874% Params, 2.53 MMac, 4.177% MACs, \n",
      "          (0): Linear(8.32 k, 0.941% Params, 1.25 MMac, 2.073% MACs, in_features=64, out_features=128, bias=True)\n",
      "          (1): GELU(0, 0.000% Params, 19.58 KMac, 0.032% MACs, approximate='none')\n",
      "          (2): Linear(8.26 k, 0.934% Params, 1.25 MMac, 2.072% MACs, in_features=128, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm(306, 0.035% Params, 9.79 KMac, 0.016% MACs, (153,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp_channel): MlpBLock(\n",
      "        94.09 k, 10.639% Params, 6.01 MMac, 9.942% MACs, \n",
      "        (mlp): Sequential(\n",
      "          94.09 k, 10.639% Params, 6.01 MMac, 9.942% MACs, \n",
      "          (0): Linear(47.12 k, 5.328% Params, 3.0 MMac, 4.955% MACs, in_features=153, out_features=306, bias=True)\n",
      "          (1): GELU(0, 0.000% Params, 19.58 KMac, 0.032% MACs, approximate='none')\n",
      "          (2): Linear(46.97 k, 5.311% Params, 3.0 MMac, 4.955% MACs, in_features=306, out_features=153, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): MixerBlock(\n",
      "      111.28 k, 12.583% Params, 8.56 MMac, 14.152% MACs, \n",
      "      (ln0): LayerNorm(306, 0.035% Params, 9.79 KMac, 0.016% MACs, (153,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp_patch): MlpBLock(\n",
      "        16.58 k, 1.874% Params, 2.53 MMac, 4.177% MACs, \n",
      "        (mlp): Sequential(\n",
      "          16.58 k, 1.874% Params, 2.53 MMac, 4.177% MACs, \n",
      "          (0): Linear(8.32 k, 0.941% Params, 1.25 MMac, 2.073% MACs, in_features=64, out_features=128, bias=True)\n",
      "          (1): GELU(0, 0.000% Params, 19.58 KMac, 0.032% MACs, approximate='none')\n",
      "          (2): Linear(8.26 k, 0.934% Params, 1.25 MMac, 2.072% MACs, in_features=128, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm(306, 0.035% Params, 9.79 KMac, 0.016% MACs, (153,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp_channel): MlpBLock(\n",
      "        94.09 k, 10.639% Params, 6.01 MMac, 9.942% MACs, \n",
      "        (mlp): Sequential(\n",
      "          94.09 k, 10.639% Params, 6.01 MMac, 9.942% MACs, \n",
      "          (0): Linear(47.12 k, 5.328% Params, 3.0 MMac, 4.955% MACs, in_features=153, out_features=306, bias=True)\n",
      "          (1): GELU(0, 0.000% Params, 19.58 KMac, 0.032% MACs, approximate='none')\n",
      "          (2): Linear(46.97 k, 5.311% Params, 3.0 MMac, 4.955% MACs, in_features=306, out_features=153, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): MixerBlock(\n",
      "      111.28 k, 12.583% Params, 8.56 MMac, 14.152% MACs, \n",
      "      (ln0): LayerNorm(306, 0.035% Params, 9.79 KMac, 0.016% MACs, (153,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp_patch): MlpBLock(\n",
      "        16.58 k, 1.874% Params, 2.53 MMac, 4.177% MACs, \n",
      "        (mlp): Sequential(\n",
      "          16.58 k, 1.874% Params, 2.53 MMac, 4.177% MACs, \n",
      "          (0): Linear(8.32 k, 0.941% Params, 1.25 MMac, 2.073% MACs, in_features=64, out_features=128, bias=True)\n",
      "          (1): GELU(0, 0.000% Params, 19.58 KMac, 0.032% MACs, approximate='none')\n",
      "          (2): Linear(8.26 k, 0.934% Params, 1.25 MMac, 2.072% MACs, in_features=128, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm(306, 0.035% Params, 9.79 KMac, 0.016% MACs, (153,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp_channel): MlpBLock(\n",
      "        94.09 k, 10.639% Params, 6.01 MMac, 9.942% MACs, \n",
      "        (mlp): Sequential(\n",
      "          94.09 k, 10.639% Params, 6.01 MMac, 9.942% MACs, \n",
      "          (0): Linear(47.12 k, 5.328% Params, 3.0 MMac, 4.955% MACs, in_features=153, out_features=306, bias=True)\n",
      "          (1): GELU(0, 0.000% Params, 19.58 KMac, 0.032% MACs, approximate='none')\n",
      "          (2): Linear(46.97 k, 5.311% Params, 3.0 MMac, 4.955% MACs, in_features=306, out_features=153, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): MixerBlock(\n",
      "      111.28 k, 12.583% Params, 8.56 MMac, 14.152% MACs, \n",
      "      (ln0): LayerNorm(306, 0.035% Params, 9.79 KMac, 0.016% MACs, (153,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp_patch): MlpBLock(\n",
      "        16.58 k, 1.874% Params, 2.53 MMac, 4.177% MACs, \n",
      "        (mlp): Sequential(\n",
      "          16.58 k, 1.874% Params, 2.53 MMac, 4.177% MACs, \n",
      "          (0): Linear(8.32 k, 0.941% Params, 1.25 MMac, 2.073% MACs, in_features=64, out_features=128, bias=True)\n",
      "          (1): GELU(0, 0.000% Params, 19.58 KMac, 0.032% MACs, approximate='none')\n",
      "          (2): Linear(8.26 k, 0.934% Params, 1.25 MMac, 2.072% MACs, in_features=128, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm(306, 0.035% Params, 9.79 KMac, 0.016% MACs, (153,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp_channel): MlpBLock(\n",
      "        94.09 k, 10.639% Params, 6.01 MMac, 9.942% MACs, \n",
      "        (mlp): Sequential(\n",
      "          94.09 k, 10.639% Params, 6.01 MMac, 9.942% MACs, \n",
      "          (0): Linear(47.12 k, 5.328% Params, 3.0 MMac, 4.955% MACs, in_features=153, out_features=306, bias=True)\n",
      "          (1): GELU(0, 0.000% Params, 19.58 KMac, 0.032% MACs, approximate='none')\n",
      "          (2): Linear(46.97 k, 5.311% Params, 3.0 MMac, 4.955% MACs, in_features=306, out_features=153, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(97.93 k, 11.073% Params, 97.93 KMac, 0.162% MACs, in_features=9792, out_features=10, bias=True)\n",
      ")\n",
      "Computational complexity:       60.48 MMac\n",
      "Number of parameters:           884.41 k\n"
     ]
    }
   ],
   "source": [
    "macs, params = get_model_complexity_info(model, (3, 32, 32), as_strings=True,\n",
    "                                           print_per_layer_stat=True, verbose=True)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from thop import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_inputs = torch.rand(1, 3, 32, 32)\n",
    "# macs, params, info = profile(model, inputs=(dummy_inputs, ), ret_layer_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flopth import flopth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Or use input tensors\n",
    "# dummy_inputs = torch.rand(1, 3, 32, 32)\n",
    "# flops, params = flopth(model, inputs=(dummy_inputs,), show_detail=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(flops, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mlpmixer0 -> 996.543K 896.779K\n",
    "## mlpmixer1 -> 899.374K 884.408K\n",
    "## patchmixer -> 233.464K 807.082K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Mixer : Channes per patch -> Initial:48 Final:144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MlpMixer(\n",
       "  (scaler): UpsamplingBilinear2d(size=(36, 36), mode='bilinear')\n",
       "  (unfold): Unfold(kernel_size=(4, 4), dilation=1, padding=0, stride=(4, 4))\n",
       "  (channel_change): Linear(in_features=48, out_features=144, bias=True)\n",
       "  (mixer_blocks): Sequential(\n",
       "    (0): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=81, out_features=162, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=162, out_features=81, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=144, out_features=288, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=288, out_features=144, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=81, out_features=162, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=162, out_features=81, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=144, out_features=288, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=288, out_features=144, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=81, out_features=162, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=162, out_features=81, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=144, out_features=288, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=288, out_features=144, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=81, out_features=162, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=162, out_features=81, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=144, out_features=288, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=288, out_features=144, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=81, out_features=162, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=162, out_features=81, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=144, out_features=288, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=288, out_features=144, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=81, out_features=162, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=162, out_features=81, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=144, out_features=288, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=288, out_features=144, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=81, out_features=162, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=162, out_features=81, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=144, out_features=288, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=288, out_features=144, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=81, out_features=162, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=162, out_features=81, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=144, out_features=288, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=288, out_features=144, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=81, out_features=162, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=162, out_features=81, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=144, out_features=288, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=288, out_features=144, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=81, out_features=162, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=162, out_features=81, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=144, out_features=288, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=288, out_features=144, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=11664, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MlpMixer((3, 4*9, 4*9), (4, 4), hidden_expansion=3.0, num_blocks=num_layers, num_classes=num_cls)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Mixer : Channes per patch -> Initial:48 Final:153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MlpMixer(\n",
       "  (scaler): UpsamplingBilinear2d(size=(32, 32), mode='bilinear')\n",
       "  (unfold): Unfold(kernel_size=(4, 4), dilation=1, padding=0, stride=(4, 4))\n",
       "  (channel_change): Linear(in_features=48, out_features=153, bias=True)\n",
       "  (mixer_blocks): Sequential(\n",
       "    (0): MixerBlock(\n",
       "      (ln0): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=153, out_features=306, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=306, out_features=153, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): MixerBlock(\n",
       "      (ln0): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=153, out_features=306, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=306, out_features=153, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): MixerBlock(\n",
       "      (ln0): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=153, out_features=306, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=306, out_features=153, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): MixerBlock(\n",
       "      (ln0): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=153, out_features=306, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=306, out_features=153, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): MixerBlock(\n",
       "      (ln0): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=153, out_features=306, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=306, out_features=153, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): MixerBlock(\n",
       "      (ln0): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=153, out_features=306, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=306, out_features=153, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): MixerBlock(\n",
       "      (ln0): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=153, out_features=306, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=306, out_features=153, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): MixerBlock(\n",
       "      (ln0): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=153, out_features=306, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=306, out_features=153, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): MixerBlock(\n",
       "      (ln0): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=153, out_features=306, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=306, out_features=153, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): MixerBlock(\n",
       "      (ln0): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((153,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=153, out_features=306, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=306, out_features=153, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=9792, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### FOR ORIGINAL MIXER V1\n",
    "model = MlpMixer((3, 32, 32), (4, 4), hidden_expansion=3.2, num_blocks=num_layers, num_classes=num_cls)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatchMlpMixer(\n",
       "  (conv1x1): Identity()\n",
       "  (mixer_blocks): Sequential(\n",
       "    (0): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((75,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=75, out_features=150, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=150, out_features=75, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=147, out_features=294, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=294, out_features=147, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((75,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=75, out_features=150, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=150, out_features=75, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=147, out_features=294, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=294, out_features=147, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((75,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=75, out_features=150, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=150, out_features=75, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=147, out_features=294, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=294, out_features=147, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((75,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=75, out_features=150, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=150, out_features=75, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=147, out_features=294, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=294, out_features=147, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((75,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=75, out_features=150, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=150, out_features=75, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=147, out_features=294, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=294, out_features=147, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((75,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=75, out_features=150, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=150, out_features=75, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=147, out_features=294, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=294, out_features=147, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((75,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=75, out_features=150, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=150, out_features=75, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (13): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=147, out_features=294, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=294, out_features=147, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (14): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((75,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=75, out_features=150, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=150, out_features=75, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (15): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=147, out_features=294, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=294, out_features=147, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (16): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((75,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=75, out_features=150, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=150, out_features=75, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (17): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=147, out_features=294, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=294, out_features=147, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (18): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((75,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=75, out_features=150, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=150, out_features=75, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (19): PatchMixerBlock(\n",
       "      (ln0): LayerNorm((147,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=147, out_features=294, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=294, out_features=147, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=3675, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PatchMlpMixer((3, 35, 35), patch_sizes=[5,7], hidden_channels=3, num_blocks=num_layers, num_classes=num_cls)\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
