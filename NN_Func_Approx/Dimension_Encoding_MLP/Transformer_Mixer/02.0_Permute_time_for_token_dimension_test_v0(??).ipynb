{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6287145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os, sys, pathlib, random, time, json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0173cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcaa3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_lib import TransformerBlock, \\\n",
    "        Mixer_TransformerBlock_Encoder, \\\n",
    "        PositionalEncoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2e11b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b45f6816",
   "metadata": {},
   "outputs": [],
   "source": [
    "### add randomize patches for clear benefit\n",
    "class Mixer_Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_size:int, block_seq_size:int, channel:int, num_blocks:int):\n",
    "        super().__init__()\n",
    "        self.transformer_blocks = []\n",
    "        for i in range(num_blocks):\n",
    "            L = Mixer_TransformerBlock_Encoder(seq_size, block_seq_size, channel, 8, 0.0, 2.0, nn.GELU, None)\n",
    "            self.transformer_blocks.append(L)\n",
    "        self.transformer_blocks = nn.Sequential(*self.transformer_blocks)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.transformer_blocks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7094b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81cc9685",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 128\n",
    "model = Mixer_Transformer(256, 16, M, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b0bf0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mixer_Transformer(\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): Mixer_TransformerBlock_Encoder(\n",
       "      (sparse_transformers): ModuleList(\n",
       "        (0): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention Sparse: [embed:128 heads:8]\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): ResMlpBlock(\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (1): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention Sparse: [embed:128 heads:8]\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): ResMlpBlock(\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Mixer_TransformerBlock_Encoder(\n",
       "      (sparse_transformers): ModuleList(\n",
       "        (0): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention Sparse: [embed:128 heads:8]\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): ResMlpBlock(\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (1): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention Sparse: [embed:128 heads:8]\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): ResMlpBlock(\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e45c5393",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "def get_time_taken(model, input_shape, bs=64, steps=50):\n",
    "\n",
    "    time_taken = []\n",
    "#     with torch.no_grad():\n",
    "    if True:\n",
    "        for i in tqdm(range(steps)):\n",
    "            inputs = torch.randn(bs, *input_shape).to(device)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            outputs.mean().backward()\n",
    "\n",
    "            start = time.time()-start\n",
    "            time_taken.append(start)\n",
    "            \n",
    "    return np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de152cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2031b339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:64 heads:8]\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (1): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:64 heads:8]\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  66944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 50/50 [00:00<00:00, 201.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:64 heads:8]\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:64 heads:8]\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  66944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 50/50 [00:00<00:00, 230.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:128 heads:8]\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (1): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:128 heads:8]\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  264960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 50/50 [00:00<00:00, 191.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:128 heads:8]\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:128 heads:8]\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  264960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 50/50 [00:00<00:00, 201.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:256 heads:8]\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (1): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:256 heads:8]\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  1054208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 50/50 [00:00<00:00, 116.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:256 heads:8]\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:256 heads:8]\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  1054208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 50/50 [00:00<00:00, 122.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:512 heads:8]\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (1): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:512 heads:8]\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  4205568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:00<00:00, 53.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:512 heads:8]\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:512 heads:8]\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  4205568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:00<00:00, 55.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:1024 heads:8]\n",
      "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (1): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:1024 heads:8]\n",
      "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  16799744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:02<00:00, 16.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:1024 heads:8]\n",
      "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:1024 heads:8]\n",
      "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  16799744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:02<00:00, 17.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:2048 heads:8]\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (1): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:2048 heads:8]\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  67153920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:11<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:2048 heads:8]\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:2048 heads:8]\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  67153920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:10<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:64 heads:8]\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (1): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:64 heads:8]\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  66944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:00<00:00, 76.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:64 heads:8]\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:64 heads:8]\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  66944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:01<00:00, 41.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:128 heads:8]\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (1): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:128 heads:8]\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  264960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:00<00:00, 55.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:128 heads:8]\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:128 heads:8]\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  264960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:01<00:00, 33.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:256 heads:8]\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (1): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:256 heads:8]\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  1054208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:01<00:00, 31.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:256 heads:8]\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:256 heads:8]\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  1054208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:02<00:00, 23.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:512 heads:8]\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (1): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:512 heads:8]\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  4205568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:03<00:00, 13.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:512 heads:8]\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:512 heads:8]\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  4205568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:04<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:1024 heads:8]\n",
      "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (1): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:1024 heads:8]\n",
      "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  16799744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:12<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:1024 heads:8]\n",
      "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:1024 heads:8]\n",
      "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  16799744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:13<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:2048 heads:8]\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (1): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:2048 heads:8]\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  67153920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:52<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:2048 heads:8]\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:2048 heads:8]\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  67153920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:52<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:64 heads:8]\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (1): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:64 heads:8]\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  66944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:02<00:00, 20.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixer_Transformer(\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:64 heads:8]\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Mixer_TransformerBlock_Encoder(\n",
      "      (sparse_transformers): ModuleList(\n",
      "        (0): Sparse_TransformerBlock(\n",
      "          (attention): SelfAttention Sparse: [embed:64 heads:8]\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): ResMlpBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "number of params:  66944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                              | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 10.91 GiB total capacity; 8.28 GiB already allocated; 523.00 MiB free; 10.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             input_shape \u001b[38;5;241m=\u001b[39m [seq_len, token_size]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#             print(input_shape)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m             t \u001b[38;5;241m=\u001b[39m \u001b[43mget_time_taken\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m             outputs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [(seq_len, token_size, sparse_att, num_params, t)] \n\u001b[1;32m     24\u001b[0m             \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mget_time_taken\u001b[0;34m(model, input_shape, bs, steps)\u001b[0m\n\u001b[1;32m     10\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m---> 13\u001b[0m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart\n\u001b[1;32m     16\u001b[0m time_taken\u001b[38;5;241m.\u001b[39mappend(start)\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 10.91 GiB total capacity; 8.28 GiB already allocated; 523.00 MiB free; 10.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for p in range(6, 11, 2): ## (4, 11)\n",
    "    seq_len = int(2**p)\n",
    "    for token_size in range(6, 11):\n",
    "        token_size = int(2**token_size)\n",
    "        for sparse_att in [True, False]:\n",
    "            blocks = 2\n",
    "            seq_block = seq_len\n",
    "            if sparse_att:\n",
    "                blocks = blocks//2\n",
    "                seq_block = int(2**np.ceil(np.log2(np.sqrt(seq_len))))\n",
    "\n",
    "            model = Mixer_Transformer(seq_len, seq_block, token_size, blocks).to(device)\n",
    "#             model.eval()\n",
    "            print(model)\n",
    "            num_params = sum(p.numel() for p in model.parameters())\n",
    "            print(\"number of params: \", num_params)\n",
    "    \n",
    "            input_shape = [seq_len, token_size]\n",
    "#             print(input_shape)\n",
    "            t = get_time_taken(model, input_shape)\n",
    "            \n",
    "            outputs += [(seq_len, token_size, sparse_att, num_params, t)] \n",
    "            del model\n",
    "            \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96619d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee5909d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(64, 64, True, 66944, 0.0025853347778320313),\n",
       " (64, 64, False, 66944, 0.002426915168762207),\n",
       " (64, 128, True, 264960, 0.0024931812286376953),\n",
       " (64, 128, False, 264960, 0.002423653602600098),\n",
       " (64, 256, True, 1054208, 0.0025581026077270506),\n",
       " (64, 256, False, 1054208, 0.0024839115142822267),\n",
       " (64, 512, True, 4205568, 0.002698359489440918),\n",
       " (64, 512, False, 4205568, 0.0026272010803222658),\n",
       " (64, 1024, True, 16799744, 0.0027275371551513674),\n",
       " (64, 1024, False, 16799744, 0.002667121887207031),\n",
       " (64, 2048, True, 67153920, 0.0028499650955200197),\n",
       " (64, 2048, False, 67153920, 0.002659769058227539),\n",
       " (256, 64, True, 66944, 0.0026912593841552732),\n",
       " (256, 64, False, 66944, 0.0026161861419677734),\n",
       " (256, 128, True, 264960, 0.0026175880432128904),\n",
       " (256, 128, False, 264960, 0.002594738006591797),\n",
       " (256, 256, True, 1054208, 0.0026099395751953123),\n",
       " (256, 256, False, 1054208, 0.0025563621520996093),\n",
       " (256, 512, True, 4205568, 0.0029493141174316406),\n",
       " (256, 512, False, 4205568, 0.0028754472732543945),\n",
       " (256, 1024, True, 16799744, 0.0029409646987915037),\n",
       " (256, 1024, False, 16799744, 0.0028627872467041015),\n",
       " (256, 2048, True, 67153920, 0.002887120246887207),\n",
       " (256, 2048, False, 67153920, 0.0028055143356323243),\n",
       " (1024, 64, True, 66944, 0.0027277517318725587)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eea0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
