{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mylibrary.nnlib as tnn\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import mylibrary.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = datasets.MNIST()\n",
    "train_data, train_label_, test_data, test_label_ = mnist.load()\n",
    "\n",
    "train_data = train_data / 255.\n",
    "test_data = test_data / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = tnn.Logits.index_to_logit(train_label_)\n",
    "train_size = len(train_label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.Tensor(train_data)\n",
    "test_data = torch.Tensor(test_data)\n",
    "yy = torch.LongTensor(train_label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(784, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10),\n",
    "#     nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.003)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Error =  0.35279321670532227\n",
      "Accuracy:  0.9032166666666667\n",
      "1 Error =  0.34422287344932556\n",
      "Accuracy:  0.9061166666666667\n",
      "2 Error =  0.3359905779361725\n",
      "Accuracy:  0.9057333333333333\n",
      "3 Error =  0.33292174339294434\n",
      "Accuracy:  0.9085333333333333\n",
      "4 Error =  0.31938502192497253\n",
      "Accuracy:  0.90815\n",
      "5 Error =  0.3179899752140045\n",
      "Accuracy:  0.9106\n",
      "6 Error =  0.3103329539299011\n",
      "Accuracy:  0.9138\n",
      "7 Error =  0.3011729121208191\n",
      "Accuracy:  0.9153333333333333\n",
      "8 Error =  0.2993481159210205\n",
      "Accuracy:  0.9182833333333333\n",
      "9 Error =  0.29042762517929077\n",
      "Accuracy:  0.9189\n",
      "10 Error =  0.2861914038658142\n",
      "Accuracy:  0.9194833333333333\n",
      "11 Error =  0.281129390001297\n",
      "Accuracy:  0.9214166666666667\n",
      "12 Error =  0.2754765748977661\n",
      "Accuracy:  0.9233\n",
      "13 Error =  0.27143123745918274\n",
      "Accuracy:  0.9249\n",
      "14 Error =  0.26632726192474365\n",
      "Accuracy:  0.92625\n",
      "15 Error =  0.26193252205848694\n",
      "Accuracy:  0.9268666666666666\n",
      "16 Error =  0.25749829411506653\n",
      "Accuracy:  0.9268333333333333\n",
      "17 Error =  0.25309884548187256\n",
      "Accuracy:  0.9284333333333333\n",
      "18 Error =  0.24824562668800354\n",
      "Accuracy:  0.9301333333333334\n",
      "19 Error =  0.2445286065340042\n",
      "Accuracy:  0.9322666666666667\n",
      "20 Error =  0.2394513040781021\n",
      "Accuracy:  0.9328666666666666\n",
      "21 Error =  0.23551343381404877\n",
      "Accuracy:  0.9338666666666666\n",
      "22 Error =  0.23140302300453186\n",
      "Accuracy:  0.9354666666666667\n",
      "23 Error =  0.22668011486530304\n",
      "Accuracy:  0.93645\n",
      "24 Error =  0.22328878939151764\n",
      "Accuracy:  0.9377\n",
      "25 Error =  0.21863387525081635\n",
      "Accuracy:  0.9388833333333333\n",
      "26 Error =  0.21488483250141144\n",
      "Accuracy:  0.9402166666666667\n",
      "27 Error =  0.2109791487455368\n",
      "Accuracy:  0.9413166666666667\n",
      "28 Error =  0.20687314867973328\n",
      "Accuracy:  0.9424666666666667\n",
      "29 Error =  0.20328658819198608\n",
      "Accuracy:  0.94355\n",
      "30 Error =  0.1994556188583374\n",
      "Accuracy:  0.9448833333333333\n",
      "31 Error =  0.19583484530448914\n",
      "Accuracy:  0.9461333333333334\n",
      "32 Error =  0.1923399716615677\n",
      "Accuracy:  0.9470833333333334\n",
      "33 Error =  0.18886718153953552\n",
      "Accuracy:  0.9477166666666667\n",
      "34 Error =  0.1855190098285675\n",
      "Accuracy:  0.9485166666666667\n",
      "35 Error =  0.18238921463489532\n",
      "Accuracy:  0.9493666666666667\n",
      "36 Error =  0.17908740043640137\n",
      "Accuracy:  0.9502666666666667\n",
      "37 Error =  0.17629730701446533\n",
      "Accuracy:  0.9508833333333333\n",
      "38 Error =  0.1732330620288849\n",
      "Accuracy:  0.9516333333333333\n",
      "39 Error =  0.17048349976539612\n",
      "Accuracy:  0.9521333333333334\n",
      "40 Error =  0.16782619059085846\n",
      "Accuracy:  0.9532833333333334\n",
      "41 Error =  0.1650935560464859\n",
      "Accuracy:  0.9539\n",
      "42 Error =  0.1626521646976471\n",
      "Accuracy:  0.9545333333333333\n",
      "43 Error =  0.16010893881320953\n",
      "Accuracy:  0.9551833333333334\n",
      "44 Error =  0.15777088701725006\n",
      "Accuracy:  0.9560666666666666\n",
      "45 Error =  0.15541179478168488\n",
      "Accuracy:  0.9565166666666667\n",
      "46 Error =  0.15316152572631836\n",
      "Accuracy:  0.9571333333333333\n",
      "47 Error =  0.15097913146018982\n",
      "Accuracy:  0.9577333333333333\n",
      "48 Error =  0.14883637428283691\n",
      "Accuracy:  0.9579666666666666\n",
      "49 Error =  0.1467573046684265\n",
      "Accuracy:  0.9587\n",
      "50 Error =  0.14476662874221802\n",
      "Accuracy:  0.95925\n",
      "51 Error =  0.14276905357837677\n",
      "Accuracy:  0.9597\n",
      "52 Error =  0.14087873697280884\n",
      "Accuracy:  0.9602833333333334\n",
      "53 Error =  0.1389848291873932\n",
      "Accuracy:  0.9609\n",
      "54 Error =  0.13717272877693176\n",
      "Accuracy:  0.9614\n",
      "55 Error =  0.13536831736564636\n",
      "Accuracy:  0.9620333333333333\n",
      "56 Error =  0.13363099098205566\n",
      "Accuracy:  0.96265\n",
      "57 Error =  0.1319095939397812\n",
      "Accuracy:  0.9630333333333333\n",
      "58 Error =  0.1302337795495987\n",
      "Accuracy:  0.9634\n",
      "59 Error =  0.1285766065120697\n",
      "Accuracy:  0.9637\n",
      "60 Error =  0.12696130573749542\n",
      "Accuracy:  0.9641666666666666\n",
      "61 Error =  0.12536749243736267\n",
      "Accuracy:  0.9645666666666667\n",
      "62 Error =  0.12381713837385178\n",
      "Accuracy:  0.9650333333333333\n",
      "63 Error =  0.1222875639796257\n",
      "Accuracy:  0.96555\n",
      "64 Error =  0.12079431116580963\n",
      "Accuracy:  0.96595\n",
      "65 Error =  0.1193302720785141\n",
      "Accuracy:  0.9664333333333334\n",
      "66 Error =  0.11790022253990173\n",
      "Accuracy:  0.96685\n",
      "67 Error =  0.11648465692996979\n",
      "Accuracy:  0.9672833333333334\n",
      "68 Error =  0.11510507762432098\n",
      "Accuracy:  0.9677166666666667\n",
      "69 Error =  0.11374503374099731\n",
      "Accuracy:  0.96815\n",
      "70 Error =  0.1124049574136734\n",
      "Accuracy:  0.9683833333333334\n",
      "71 Error =  0.11108913272619247\n",
      "Accuracy:  0.969\n",
      "72 Error =  0.1097908541560173\n",
      "Accuracy:  0.9692333333333333\n",
      "73 Error =  0.10852141678333282\n",
      "Accuracy:  0.96965\n",
      "74 Error =  0.10727164149284363\n",
      "Accuracy:  0.97005\n",
      "75 Error =  0.10604547709226608\n",
      "Accuracy:  0.9703166666666667\n",
      "76 Error =  0.10484400391578674\n",
      "Accuracy:  0.97065\n",
      "77 Error =  0.10366284102201462\n",
      "Accuracy:  0.9709333333333333\n",
      "78 Error =  0.10250173509120941\n",
      "Accuracy:  0.9713\n",
      "79 Error =  0.10135480016469955\n",
      "Accuracy:  0.9714333333333334\n",
      "80 Error =  0.10023215413093567\n",
      "Accuracy:  0.9717666666666667\n",
      "81 Error =  0.09912550449371338\n",
      "Accuracy:  0.9721333333333333\n",
      "82 Error =  0.09803669899702072\n",
      "Accuracy:  0.9724333333333334\n",
      "83 Error =  0.0969637930393219\n",
      "Accuracy:  0.9729833333333333\n",
      "84 Error =  0.09590883553028107\n",
      "Accuracy:  0.9731666666666666\n",
      "85 Error =  0.09487242251634598\n",
      "Accuracy:  0.9737333333333333\n",
      "86 Error =  0.09386257827281952\n",
      "Accuracy:  0.9739833333333333\n",
      "87 Error =  0.09290289133787155\n",
      "Accuracy:  0.9742833333333333\n",
      "88 Error =  0.09203331917524338\n",
      "Accuracy:  0.9746833333333333\n",
      "89 Error =  0.0912904441356659\n",
      "Accuracy:  0.9744833333333334\n",
      "90 Error =  0.0906917154788971\n",
      "Accuracy:  0.9751166666666666\n",
      "91 Error =  0.08994479477405548\n",
      "Accuracy:  0.97495\n",
      "92 Error =  0.089026540517807\n",
      "Accuracy:  0.9758333333333333\n",
      "93 Error =  0.08763410896062851\n",
      "Accuracy:  0.9758333333333333\n",
      "94 Error =  0.0863136574625969\n",
      "Accuracy:  0.97615\n",
      "95 Error =  0.0853738784790039\n",
      "Accuracy:  0.9764166666666667\n",
      "96 Error =  0.08477447926998138\n",
      "Accuracy:  0.9763833333333334\n",
      "97 Error =  0.08419166505336761\n",
      "Accuracy:  0.9769333333333333\n",
      "98 Error =  0.08327975869178772\n",
      "Accuracy:  0.9772\n",
      "99 Error =  0.08217678219079971\n",
      "Accuracy:  0.97765\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    yout = net(xx)\n",
    "\n",
    "    loss = criterion(yout, yy)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    error = float(loss)\n",
    "    print(epoch, 'Error = ', error)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        yout = net(train_data)\n",
    "        out = torch.argmax(yout, axis=1)\n",
    "#         print(out.shape)\n",
    "#         print(np.array(train_label_).shape)\n",
    "        acc = (out.data.numpy() == np.array(train_label_)).astype(np.float).mean()\n",
    "        print(\"Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', Linear(in_features=784, out_features=256, bias=True)),\n",
       " ('1', ReLU()),\n",
       " ('2', Linear(in_features=256, out_features=128, bias=True)),\n",
       " ('3', ReLU()),\n",
       " ('4', Linear(in_features=128, out_features=64, bias=True)),\n",
       " ('5', ReLU()),\n",
       " ('6', Linear(in_features=64, out_features=10, bias=True))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(net._modules.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Oracle():\n",
    "    \n",
    "    def __init__(self, net):\n",
    "        self.net = net\n",
    "        self.activations = {}\n",
    "        self.gradients = {}\n",
    "        self.forward_hook = {}\n",
    "        self.backward_hook = {}\n",
    "        self.keys = []\n",
    "        for name, module in list(self.net._modules.items()):\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                hook = module.register_backward_hook(self.capture_gradients)\n",
    "                self.backward_hook[module] = hook\n",
    "                hook = module.register_forward_hook(self.capture_inputs)\n",
    "                self.forward_hook[module] = hook\n",
    "                \n",
    "                self.activations[module] = None\n",
    "                self.gradients[module] = None\n",
    "                self.keys.append(module)\n",
    "        pass\n",
    "            \n",
    "        \n",
    "    def capture_inputs(self, module, inp, out):\n",
    "#         print(out.shape)\n",
    "        self.activations[module] = out.data\n",
    "        \n",
    "    def capture_gradients(self, module, gradi, grado):\n",
    "#         print(\"Grad\")\n",
    "#         print(gradi[-1])\n",
    "#         for gi in gradi:\n",
    "#             print(gi.shape)\n",
    "#         print(grado)\n",
    "#         print(grado[0].shape)\n",
    "        self.gradients[module] = grado[0]\n",
    "        \n",
    "    def gather_inputs_gradients(self, x, t):\n",
    "        self.net.zero_grad()\n",
    "        y = self.net(x)\n",
    "        \n",
    "        error = criterion(y, t)\n",
    "        error.backward()\n",
    "        \n",
    "        for module in self.keys:\n",
    "            hook = self.forward_hook[module]\n",
    "            hook.remove()\n",
    "            hook = self.backward_hook[module]\n",
    "            hook.remove()\n",
    "        return\n",
    "    \n",
    "    def compute_significance(self, x, t):\n",
    "        self.gather_inputs_gradients(x, t)\n",
    "        \n",
    "        ## compute importance score\n",
    "        importance = []\n",
    "        for module in self.keys:\n",
    "            z = self.activations[module] * self.gradients[module]\n",
    "            z = z.mean(dim=0).abs()\n",
    "            importance.append(z)\n",
    "            \n",
    "        return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([8.5159e-10, 2.2340e-09, 0.0000e+00, 1.4452e-09, 1.8545e-09, 1.3532e-10,\n",
       "         1.0369e-09, 9.3642e-10, 1.3956e-09, 1.4765e-11, 3.5169e-09, 3.3023e-09,\n",
       "         8.0959e-10, 1.7632e-09, 2.5364e-09, 7.2729e-10, 2.4542e-09, 1.8144e-09,\n",
       "         2.5195e-09, 3.0664e-10, 2.0408e-09, 1.3529e-09, 7.3862e-10, 2.1734e-10,\n",
       "         2.2921e-09, 4.5361e-11, 5.3660e-09, 1.4120e-09, 1.5193e-09, 1.1403e-09,\n",
       "         3.9473e-09, 1.8232e-09, 1.4800e-09, 0.0000e+00, 5.9791e-11, 1.4199e-09,\n",
       "         0.0000e+00, 5.8153e-10, 0.0000e+00, 1.9371e-09, 0.0000e+00, 8.9673e-10,\n",
       "         4.2191e-10, 0.0000e+00, 1.2422e-09, 1.8172e-09, 0.0000e+00, 1.5679e-09,\n",
       "         2.0490e-09, 2.2882e-09, 1.8317e-09, 6.2868e-09, 1.4803e-09, 4.7809e-09,\n",
       "         2.0764e-09, 0.0000e+00, 1.3784e-09, 1.1671e-09, 8.6811e-10, 4.6927e-09,\n",
       "         5.1318e-10, 2.4203e-09, 0.0000e+00, 9.0236e-10, 1.7595e-09, 2.2716e-09,\n",
       "         3.7795e-10, 8.4219e-09, 1.2402e-09, 1.4659e-08, 7.2019e-10, 1.0749e-09,\n",
       "         1.1212e-09, 0.0000e+00, 2.0065e-09, 5.9920e-10, 1.5386e-09, 7.9299e-10,\n",
       "         6.8341e-10, 0.0000e+00, 7.0915e-10, 1.5358e-08, 0.0000e+00, 0.0000e+00,\n",
       "         7.3529e-09, 2.1365e-09, 3.2054e-09, 2.5359e-09, 1.1887e-09, 4.6887e-09,\n",
       "         1.9359e-09, 1.3698e-09, 1.6611e-09, 9.9920e-10, 9.6189e-10, 3.3980e-10,\n",
       "         5.3776e-09, 2.1592e-09, 6.6513e-10, 3.2233e-09, 1.4924e-09, 1.1264e-09,\n",
       "         5.9164e-10, 7.9501e-10, 9.8867e-10, 5.2372e-10, 2.7965e-09, 2.6339e-09,\n",
       "         0.0000e+00, 8.1561e-10, 4.3637e-10, 7.4850e-11, 0.0000e+00, 2.4404e-09,\n",
       "         3.6904e-10, 3.3646e-09, 2.7544e-09, 0.0000e+00, 1.4104e-09, 1.2391e-09,\n",
       "         9.2522e-10, 6.4375e-10, 1.1541e-09, 3.0880e-10, 3.6820e-09, 1.2400e-09,\n",
       "         2.3889e-09, 2.8814e-09, 2.9601e-09, 9.7712e-10, 3.2698e-11, 8.7306e-11,\n",
       "         4.1816e-10, 1.0787e-09, 3.9492e-09, 5.6835e-09, 3.4069e-09, 1.5627e-10,\n",
       "         1.9367e-09, 6.2257e-10, 7.5225e-10, 2.1194e-10, 1.0370e-09, 1.2029e-09,\n",
       "         2.4930e-09, 1.2611e-08, 0.0000e+00, 8.8687e-10, 2.1140e-09, 3.2959e-09,\n",
       "         1.0256e-09, 8.6114e-09, 4.7324e-09, 1.9609e-09, 2.9599e-10, 1.1467e-09,\n",
       "         2.7075e-10, 1.8827e-09, 1.6931e-09, 9.8612e-10, 3.4200e-09, 1.9759e-09,\n",
       "         2.4324e-10, 8.0729e-10, 4.7557e-10, 2.9778e-09, 2.7979e-09, 3.2749e-09,\n",
       "         1.4423e-09, 4.4409e-10, 9.1807e-10, 2.2966e-11, 6.9347e-10, 0.0000e+00,\n",
       "         2.6027e-13, 3.9365e-10, 1.1485e-09, 1.1840e-09, 2.4591e-09, 1.9168e-09,\n",
       "         6.6974e-10, 1.5041e-10, 1.7100e-09, 2.0525e-09, 1.9877e-09, 3.8578e-10,\n",
       "         2.8045e-10, 4.1225e-09, 1.7738e-09, 5.0434e-10, 1.3126e-09, 1.3578e-08,\n",
       "         1.5496e-09, 6.2810e-10, 2.3538e-09, 1.6728e-08, 3.0440e-09, 6.1348e-10,\n",
       "         3.2127e-10, 3.2593e-09, 0.0000e+00, 1.8479e-08, 0.0000e+00, 8.9898e-10,\n",
       "         8.8675e-10, 1.7723e-09, 1.9867e-09, 3.0488e-09, 1.5189e-09, 2.6984e-09,\n",
       "         7.4753e-10, 6.5805e-10, 2.3581e-09, 1.5726e-09, 0.0000e+00, 9.9019e-10,\n",
       "         3.7059e-11, 2.3789e-09, 4.8387e-09, 4.6078e-10, 5.3390e-10, 5.1256e-10,\n",
       "         3.8842e-09, 2.9094e-09, 1.5694e-08, 2.1038e-09, 4.1506e-10, 0.0000e+00,\n",
       "         3.1450e-09, 2.2310e-10, 6.1746e-10, 1.7345e-09, 1.2660e-09, 5.3080e-10,\n",
       "         1.3697e-09, 1.0009e-09, 2.4432e-10, 9.8973e-10, 4.7627e-10, 4.4471e-10,\n",
       "         1.1489e-11, 2.7612e-10, 1.0764e-09, 1.5983e-09, 1.5939e-08, 1.7186e-09,\n",
       "         1.9479e-09, 5.2354e-10, 1.7451e-11, 1.2906e-09, 8.4283e-09, 0.0000e+00,\n",
       "         2.0809e-09, 6.4620e-10, 1.0857e-08, 6.7261e-10]),\n",
       " tensor([2.7231e-09, 2.4717e-09, 1.9394e-09, 1.2828e-08, 2.4280e-09, 5.3017e-09,\n",
       "         8.6891e-09, 2.0787e-09, 0.0000e+00, 1.3747e-09, 4.0772e-09, 0.0000e+00,\n",
       "         5.7409e-10, 0.0000e+00, 0.0000e+00, 1.5461e-09, 8.9717e-09, 2.7909e-09,\n",
       "         4.2550e-09, 2.3102e-09, 9.8011e-10, 3.6872e-09, 9.0602e-09, 0.0000e+00,\n",
       "         2.6851e-10, 9.7926e-09, 6.9313e-09, 1.0256e-08, 6.7582e-09, 2.2829e-09,\n",
       "         1.5458e-10, 3.0227e-09, 5.9545e-10, 1.3864e-09, 3.2476e-09, 6.6593e-09,\n",
       "         3.0966e-09, 1.4897e-09, 4.5078e-10, 6.3382e-09, 0.0000e+00, 5.4813e-10,\n",
       "         9.9793e-10, 8.4231e-09, 6.4829e-09, 3.6765e-09, 2.2522e-09, 6.5618e-10,\n",
       "         1.6055e-09, 4.3869e-10, 2.1821e-09, 3.4466e-09, 0.0000e+00, 4.6472e-09,\n",
       "         2.2935e-09, 6.5330e-09, 1.1448e-08, 3.0433e-09, 8.4062e-09, 3.2081e-09,\n",
       "         1.2246e-09, 3.6936e-09, 1.8383e-09, 1.1179e-09, 4.9644e-09, 3.4163e-09,\n",
       "         3.3915e-09, 1.7477e-09, 6.6170e-09, 7.4112e-09, 6.1468e-09, 4.8017e-09,\n",
       "         8.4289e-11, 0.0000e+00, 6.7591e-09, 2.7119e-09, 2.5341e-09, 7.5896e-10,\n",
       "         2.1918e-09, 3.3517e-09, 0.0000e+00, 1.9044e-09, 0.0000e+00, 0.0000e+00,\n",
       "         2.5704e-09, 3.4422e-09, 4.8494e-09, 3.8025e-09, 1.7850e-09, 1.7971e-09,\n",
       "         7.0767e-09, 8.5040e-09, 3.2597e-09, 6.6765e-09, 2.4928e-09, 3.4256e-09,\n",
       "         8.7067e-10, 5.4378e-09, 3.2444e-09, 0.0000e+00, 1.9880e-09, 7.6716e-09,\n",
       "         1.6387e-09, 5.6442e-09, 5.3538e-09, 0.0000e+00, 2.4644e-09, 9.4878e-09,\n",
       "         3.0942e-09, 1.0616e-08, 1.5502e-10, 0.0000e+00, 8.2018e-09, 2.9804e-10,\n",
       "         8.2247e-10, 2.2299e-09, 0.0000e+00, 1.0873e-08, 4.0709e-09, 6.7457e-09,\n",
       "         1.2040e-08, 9.4627e-09, 5.4181e-09, 4.4931e-09, 1.5415e-09, 6.3308e-10,\n",
       "         6.3249e-09, 0.0000e+00]),\n",
       " tensor([1.4940e-09, 1.1283e-08, 6.3260e-09, 8.6540e-09, 0.0000e+00, 4.4950e-09,\n",
       "         9.7717e-09, 1.8968e-10, 6.4634e-09, 1.8431e-09, 1.1548e-08, 0.0000e+00,\n",
       "         8.5334e-09, 5.9328e-09, 1.3772e-08, 4.8755e-09, 0.0000e+00, 1.1804e-08,\n",
       "         1.0446e-09, 5.1791e-09, 4.1923e-09, 1.1558e-09, 2.4139e-09, 9.5352e-09,\n",
       "         4.3267e-09, 8.5663e-09, 1.3389e-09, 1.7639e-08, 6.5449e-09, 8.8392e-09,\n",
       "         1.1197e-08, 4.4142e-09, 1.1569e-08, 8.1475e-09, 3.1205e-08, 5.4766e-10,\n",
       "         0.0000e+00, 5.1663e-10, 8.7795e-09, 0.0000e+00, 1.9791e-08, 2.5532e-09,\n",
       "         5.7486e-09, 1.0407e-08, 1.3250e-08, 1.0860e-08, 3.0648e-09, 8.2534e-09,\n",
       "         7.0437e-09, 1.0666e-08, 9.3273e-09, 1.1584e-09, 4.0250e-09, 0.0000e+00,\n",
       "         9.1850e-09, 2.3133e-08, 1.3050e-08, 5.3136e-09, 1.0204e-08, 0.0000e+00,\n",
       "         0.0000e+00, 7.1596e-09, 6.4144e-09, 2.0615e-09]),\n",
       " tensor([2.0093e-08, 2.1391e-08, 4.1294e-08, 3.1187e-08, 4.6447e-08, 6.7782e-08,\n",
       "         2.5905e-08, 4.5468e-08, 5.3269e-08, 6.8068e-08])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle = Oracle(net)\n",
    "oracle.compute_significance(xx, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Oracle_Modified():\n",
    "    \n",
    "    def __init__(self, net, mode=\"oracle\"):\n",
    "        self.net = net\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.activations = {}\n",
    "        self.gradients = {}\n",
    "        self.forward_hook = {}\n",
    "        self.backward_hook = {}\n",
    "        self.keys = []\n",
    "        for name, module in list(self.net._modules.items()):\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                hook = module.register_backward_hook(self.capture_gradients)\n",
    "                self.backward_hook[module] = hook\n",
    "                hook = module.register_forward_hook(self.capture_inputs)\n",
    "                self.forward_hook[module] = hook\n",
    "                \n",
    "                self.activations[module] = None\n",
    "                self.gradients[module] = None\n",
    "                self.keys.append(module)\n",
    "        \n",
    "#         self.importance = None\n",
    "        pass\n",
    "            \n",
    "        \n",
    "    def capture_inputs(self, module, inp, out):\n",
    "        self.activations[module] = out.data\n",
    "        \n",
    "    def capture_gradients(self, module, gradi, grado):\n",
    "        self.gradients[module] = grado[0]\n",
    "        \n",
    "    def gather_inputs_gradients(self, x, t):\n",
    "        self.net.zero_grad()\n",
    "        y = self.net(x)\n",
    "        \n",
    "        error = criterion(y,t)\n",
    "        error.backward()\n",
    "        \n",
    "        for module in self.keys:\n",
    "            hook = self.forward_hook[module]\n",
    "            hook.remove()\n",
    "            hook = self.backward_hook[module]\n",
    "            hook.remove()\n",
    "        return\n",
    "    \n",
    "    def compute_significance(self, x, t, mode=None, normalize=False):\n",
    "        self.gather_inputs_gradients(x, t)\n",
    "        \n",
    "        if mode is None:\n",
    "            mode = self.mode\n",
    "        \n",
    "        ## compute importance score\n",
    "        importance = []\n",
    "        if mode == \"oracle\":\n",
    "            for module in self.keys:\n",
    "                z = self.activations[module] * self.gradients[module]\n",
    "                z = z.sum(dim=0).abs()\n",
    "                importance.append(z)\n",
    "\n",
    "        elif mode == \"oracle_absolute\":\n",
    "            for module in self.keys:\n",
    "                z = (self.activations[module] * self.gradients[module])\n",
    "#                 z = z.abs().sum(dim=0)\n",
    "                z = z.pow(2).sum(dim=0)\n",
    "                importance.append(z)\n",
    "        \n",
    "        elif mode == \"oracle_normalized\":\n",
    "            scaler = torch.norm(self.gradients[self.keys[-1]], dim=1, keepdim=True) + 1e-5\n",
    "#             print(scaler.shape)\n",
    "            for module in self.keys:\n",
    "                z = (self.activations[module] * self.gradients[module])/scaler\n",
    "                z = z.sum(dim=0).abs()\n",
    "                importance.append(z)\n",
    "                                \n",
    "        elif mode == \"oracle_abs_norm\":\n",
    "            scaler = torch.norm(self.gradients[self.keys[-1]], p=2, dim=1, keepdim=True) + 1e-5\n",
    "#             print(scaler)\n",
    "            for module in self.keys:\n",
    "                z = (self.activations[module] * self.gradients[module])/scaler\n",
    "                z = z.abs().sum(dim=0)\n",
    "#                 z = z.pow(2).sum(dim=0)\n",
    "                importance.append(z)               \n",
    "        \n",
    "        importance = importance[:-1]\n",
    "        if normalize:\n",
    "            sums = 0\n",
    "            count = 0\n",
    "            for imp in importance:\n",
    "                sums += imp.sum()\n",
    "                count += len(imp)\n",
    "#             print(sums, count)\n",
    "            divider = sums/count ## total importance is number of neurons\n",
    "            for i in range(len(importance)):\n",
    "                importance[i] = importance[i]/divider\n",
    "            \n",
    "        return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_mod = Oracle_Modified(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = [\"oracle\", \"oracle_absolute\", \"oracle_normalized\", \"oracle_abs_norm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([2.7180e-01, 7.1302e-01, 0.0000e+00, 4.6125e-01, 5.9188e-01, 4.3188e-02,\n",
       "         3.3094e-01, 2.9887e-01, 4.4542e-01, 4.7126e-03, 1.1225e+00, 1.0540e+00,\n",
       "         2.5839e-01, 5.6276e-01, 8.0953e-01, 2.3213e-01, 7.8328e-01, 5.7911e-01,\n",
       "         8.0413e-01, 9.7869e-02, 6.5135e-01, 4.3181e-01, 2.3574e-01, 6.9368e-02,\n",
       "         7.3158e-01, 1.4478e-02, 1.7127e+00, 4.5066e-01, 4.8489e-01, 3.6393e-01,\n",
       "         1.2598e+00, 5.8190e-01, 4.7238e-01, 0.0000e+00, 1.9083e-02, 4.5319e-01,\n",
       "         0.0000e+00, 1.8560e-01, 0.0000e+00, 6.1826e-01, 0.0000e+00, 2.8621e-01,\n",
       "         1.3466e-01, 0.0000e+00, 3.9646e-01, 5.7998e-01, 0.0000e+00, 5.0043e-01,\n",
       "         6.5398e-01, 7.3030e-01, 5.8462e-01, 2.0065e+00, 4.7246e-01, 1.5259e+00,\n",
       "         6.6273e-01, 0.0000e+00, 4.3994e-01, 3.7250e-01, 2.7707e-01, 1.4977e+00,\n",
       "         1.6379e-01, 7.7247e-01, 0.0000e+00, 2.8800e-01, 5.6158e-01, 7.2502e-01,\n",
       "         1.2063e-01, 2.6880e+00, 3.9583e-01, 4.6785e+00, 2.2986e-01, 3.4308e-01,\n",
       "         3.5786e-01, 0.0000e+00, 6.4040e-01, 1.9124e-01, 4.9108e-01, 2.5309e-01,\n",
       "         2.1812e-01, 0.0000e+00, 2.2634e-01, 4.9017e+00, 0.0000e+00, 0.0000e+00,\n",
       "         2.3468e+00, 6.8189e-01, 1.0231e+00, 8.0938e-01, 3.7938e-01, 1.4965e+00,\n",
       "         6.1788e-01, 4.3719e-01, 5.3017e-01, 3.1891e-01, 3.0700e-01, 1.0845e-01,\n",
       "         1.7163e+00, 6.8916e-01, 2.1229e-01, 1.0288e+00, 4.7634e-01, 3.5951e-01,\n",
       "         1.8883e-01, 2.5374e-01, 3.1555e-01, 1.6716e-01, 8.9255e-01, 8.4065e-01,\n",
       "         0.0000e+00, 2.6031e-01, 1.3927e-01, 2.3890e-02, 0.0000e+00, 7.7891e-01,\n",
       "         1.1778e-01, 1.0739e+00, 8.7912e-01, 0.0000e+00, 4.5015e-01, 3.9548e-01,\n",
       "         2.9530e-01, 2.0546e-01, 3.6836e-01, 9.8558e-02, 1.1752e+00, 3.9575e-01,\n",
       "         7.6247e-01, 9.1964e-01, 9.4475e-01, 3.1186e-01, 1.0436e-02, 2.7865e-02,\n",
       "         1.3346e-01, 3.4429e-01, 1.2605e+00, 1.8140e+00, 1.0874e+00, 4.9875e-02,\n",
       "         6.1814e-01, 1.9870e-01, 2.4009e-01, 6.7645e-02, 3.3098e-01, 3.8392e-01,\n",
       "         7.9569e-01, 4.0249e+00, 0.0000e+00, 2.8306e-01, 6.7473e-01, 1.0519e+00,\n",
       "         3.2734e-01, 2.7485e+00, 1.5104e+00, 6.2586e-01, 9.4469e-02, 3.6600e-01,\n",
       "         8.6415e-02, 6.0089e-01, 5.4038e-01, 3.1474e-01, 1.0916e+00, 6.3064e-01,\n",
       "         7.7634e-02, 2.5766e-01, 1.5178e-01, 9.5041e-01, 8.9300e-01, 1.0452e+00,\n",
       "         4.6033e-01, 1.4174e-01, 2.9302e-01, 7.3300e-03, 2.2133e-01, 0.0000e+00,\n",
       "         8.3068e-05, 1.2564e-01, 3.6657e-01, 3.7788e-01, 7.8487e-01, 6.1177e-01,\n",
       "         2.1376e-01, 4.8006e-02, 5.4577e-01, 6.5508e-01, 6.3442e-01, 1.2313e-01,\n",
       "         8.9509e-02, 1.3158e+00, 5.6615e-01, 1.6097e-01, 4.1895e-01, 4.3337e+00,\n",
       "         4.9459e-01, 2.0047e-01, 7.5124e-01, 5.3390e+00, 9.7153e-01, 1.9580e-01,\n",
       "         1.0254e-01, 1.0402e+00, 0.0000e+00, 5.8978e+00, 0.0000e+00, 2.8692e-01,\n",
       "         2.8302e-01, 5.6565e-01, 6.3408e-01, 9.7309e-01, 4.8480e-01, 8.6124e-01,\n",
       "         2.3859e-01, 2.1003e-01, 7.5262e-01, 5.0192e-01, 0.0000e+00, 3.1603e-01,\n",
       "         1.1828e-02, 7.5925e-01, 1.5444e+00, 1.4707e-01, 1.7040e-01, 1.6359e-01,\n",
       "         1.2397e+00, 9.2857e-01, 5.0090e+00, 6.7145e-01, 1.3247e-01, 0.0000e+00,\n",
       "         1.0038e+00, 7.1207e-02, 1.9707e-01, 5.5360e-01, 4.0407e-01, 1.6941e-01,\n",
       "         4.3718e-01, 3.1946e-01, 7.7980e-02, 3.1589e-01, 1.5201e-01, 1.4194e-01,\n",
       "         3.6670e-03, 8.8127e-02, 3.4355e-01, 5.1013e-01, 5.0871e+00, 5.4852e-01,\n",
       "         6.2169e-01, 1.6710e-01, 5.5699e-03, 4.1191e-01, 2.6900e+00, 0.0000e+00,\n",
       "         6.6416e-01, 2.0625e-01, 3.4653e+00, 2.1467e-01]),\n",
       " tensor([0.8691, 0.7889, 0.6190, 4.0941, 0.7749, 1.6921, 2.7733, 0.6634, 0.0000,\n",
       "         0.4388, 1.3013, 0.0000, 0.1832, 0.0000, 0.0000, 0.4935, 2.8635, 0.8907,\n",
       "         1.3581, 0.7373, 0.3128, 1.1768, 2.8917, 0.0000, 0.0857, 3.1255, 2.2123,\n",
       "         3.2734, 2.1570, 0.7286, 0.0493, 0.9647, 0.1900, 0.4425, 1.0365, 2.1254,\n",
       "         0.9883, 0.4755, 0.1439, 2.0229, 0.0000, 0.1749, 0.3185, 2.6884, 2.0691,\n",
       "         1.1734, 0.7188, 0.2094, 0.5124, 0.1400, 0.6964, 1.1000, 0.0000, 1.4832,\n",
       "         0.7320, 2.0851, 3.6538, 0.9713, 2.6830, 1.0239, 0.3909, 1.1789, 0.5867,\n",
       "         0.3568, 1.5845, 1.0904, 1.0825, 0.5578, 2.1119, 2.3654, 1.9619, 1.5325,\n",
       "         0.0269, 0.0000, 2.1573, 0.8655, 0.8088, 0.2422, 0.6995, 1.0698, 0.0000,\n",
       "         0.6078, 0.0000, 0.0000, 0.8204, 1.0986, 1.5478, 1.2136, 0.5697, 0.5736,\n",
       "         2.2587, 2.7142, 1.0404, 2.1309, 0.7956, 1.0933, 0.2779, 1.7356, 1.0355,\n",
       "         0.0000, 0.6345, 2.4485, 0.5230, 1.8014, 1.7088, 0.0000, 0.7866, 3.0282,\n",
       "         0.9876, 3.3882, 0.0495, 0.0000, 2.6177, 0.0951, 0.2625, 0.7117, 0.0000,\n",
       "         3.4703, 1.2993, 2.1530, 3.8427, 3.0202, 1.7293, 1.4341, 0.4920, 0.2021,\n",
       "         2.0187, 0.0000]),\n",
       " tensor([0.4768, 3.6011, 2.0190, 2.7621, 0.0000, 1.4347, 3.1188, 0.0605, 2.0629,\n",
       "         0.5882, 3.6859, 0.0000, 2.7236, 1.8935, 4.3954, 1.5561, 0.0000, 3.7674,\n",
       "         0.3334, 1.6530, 1.3380, 0.3689, 0.7704, 3.0433, 1.3809, 2.7341, 0.4273,\n",
       "         5.6298, 2.0889, 2.8212, 3.5737, 1.4089, 3.6924, 2.6004, 9.9596, 0.1748,\n",
       "         0.0000, 0.1649, 2.8021, 0.0000, 6.3167, 0.8149, 1.8348, 3.3214, 4.2289,\n",
       "         3.4661, 0.9782, 2.6342, 2.2481, 3.4043, 2.9770, 0.3697, 1.2846, 0.0000,\n",
       "         2.9315, 7.3834, 4.1651, 1.6959, 3.2567, 0.0000, 0.0000, 2.2851, 2.0473,\n",
       "         0.6580])]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle_mod.compute_significance(xx, yy, mode=modes[0], normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.4430,  0.0295,  0.0000,  0.1659,  0.2032,  0.1729,  0.1887,  0.2304,\n",
       "          0.0856,  0.1015,  0.2566,  0.3412,  0.1336,  0.0799,  0.3109,  0.3287,\n",
       "          0.0927,  0.1783,  0.3302,  0.1829,  0.1044,  0.0370,  0.1718,  0.1816,\n",
       "          0.0439,  0.0691,  0.1097,  0.0572,  0.1989,  0.2707,  0.1961,  0.1230,\n",
       "          0.1407,  0.0000,  0.0468,  0.2188,  0.0000,  0.1459,  0.0000,  0.2626,\n",
       "          0.0000,  0.0808,  0.2883,  0.0000,  0.0339,  0.2565,  0.0000,  0.2341,\n",
       "          0.1036,  0.1811,  0.2531,  0.0888,  0.2588,  0.4036,  0.2096,  0.0000,\n",
       "          0.3346,  0.2301,  0.2726,  0.4533,  5.6797,  0.2210,  0.0000,  0.1348,\n",
       "          0.0679,  0.3212,  0.2382,  0.7364,  0.0275,  7.7658,  0.2186,  0.1298,\n",
       "          0.3976,  0.0000,  0.5514,  0.0588,  0.1190,  0.2136,  0.1831,  0.0000,\n",
       "          0.1302,  6.7365,  0.0000,  0.0000,  0.8187,  0.1947,  0.1121,  0.1243,\n",
       "          0.1052,  1.1182,  0.3266,  0.3680,  0.1048,  0.1245,  0.1369,  0.0670,\n",
       "          0.8273,  0.0900,  0.2481,  0.0397,  6.3573,  0.1695,  0.0935,  0.2408,\n",
       "          0.3002,  0.1558,  0.3260,  0.0613,  0.0000,  0.0837,  0.1036,  0.1208,\n",
       "          0.0000,  0.3012,  0.1287,  0.2526,  0.3306,  0.0000,  0.0333,  0.2053,\n",
       "          0.3539,  0.1266,  0.1407,  0.1055,  0.4130,  0.1083,  0.3426,  0.1925,\n",
       "          0.5328,  0.1016,  0.1205,  0.5870,  0.1561,  0.0934,  0.4069,  0.4539,\n",
       "          0.3486,  0.2776,  0.4367,  0.2882,  0.1894,  0.5245,  0.1661,  0.0786,\n",
       "          0.1354,  1.9738,  0.0000,  0.1868,  0.0747,  0.0530,  0.2454,  1.6774,\n",
       "          0.6077,  0.1976,  0.2821,  0.2417,  4.9513,  0.0734,  0.1837,  0.1106,\n",
       "          0.1282,  0.2827,  0.2489,  0.2747,  0.1234,  0.1953,  0.2193,  0.2258,\n",
       "          0.1842,  0.1556,  0.1301,  0.1915,  0.2436,  0.0000,  0.0436,  0.2600,\n",
       "          0.3064,  0.1239,  0.2270,  0.3215,  0.1133,  0.2767,  0.1560,  0.2900,\n",
       "          0.2855,  0.3432,  0.1594,  0.1814,  0.2702,  0.1991,  0.1084,  5.8227,\n",
       "          0.0751,  0.1089,  0.2790, 10.2794,  0.3158,  0.3010,  0.1576,  0.1977,\n",
       "          0.0000,  1.8886,  0.0000,  0.1916,  0.3169,  0.1638,  0.2037,  0.4160,\n",
       "          0.3478,  0.0196,  0.2373,  0.1649,  0.3253,  0.4655,  0.0000,  0.2760,\n",
       "          0.1086,  0.0652,  0.7358,  0.1401, 10.8706,  0.0590,  0.3623,  0.1097,\n",
       "          3.3055,  0.2750,  0.2572,  0.0000,  0.0501,  0.2657,  0.1144,  0.0656,\n",
       "          0.0614,  0.4738,  0.2623,  0.1028,  0.2954,  0.1264,  0.0591,  0.0643,\n",
       "          0.2557,  0.0640,  0.2398,  0.1464,  3.4323,  0.1165,  0.3158,  0.0211,\n",
       "          0.3630,  0.0649,  0.3741,  0.0000,  0.1126,  0.1424,  3.7096,  0.0464]),\n",
       " tensor([0.0554, 0.9318, 0.2024, 3.4782, 0.6655, 0.3731, 0.4360, 1.2922, 0.0000,\n",
       "         1.5630, 1.0987, 0.0000, 1.0092, 0.0000, 0.0000, 0.7720, 0.6937, 0.4184,\n",
       "         1.5637, 1.0879, 1.9115, 0.9138, 0.9619, 0.0000, 0.7749, 1.9793, 0.9060,\n",
       "         2.9189, 1.6885, 1.7969, 0.7356, 1.4880, 0.3797, 0.9699, 2.0841, 0.3754,\n",
       "         2.3114, 0.2074, 3.2798, 0.2746, 0.0000, 0.5639, 1.3301, 1.1977, 0.4687,\n",
       "         0.6155, 1.2819, 1.0564, 0.4157, 0.8053, 1.1081, 0.1987, 0.0000, 0.4745,\n",
       "         0.4228, 0.4071, 2.4379, 1.4251, 0.1936, 0.3950, 0.8104, 1.7863, 0.5932,\n",
       "         0.3679, 0.2026, 0.5591, 2.0565, 0.4856, 0.2692, 1.4891, 0.4266, 0.2191,\n",
       "         0.7016, 0.0000, 1.0283, 3.3115, 1.8570, 0.0115, 1.7759, 0.8680, 0.0000,\n",
       "         1.7900, 0.0000, 0.0000, 1.3474, 0.2856, 0.7797, 1.0930, 0.6331, 0.5626,\n",
       "         1.1440, 1.0263, 0.1630, 0.9087, 1.3109, 0.7247, 0.3835, 0.7775, 0.8611,\n",
       "         0.0000, 0.6519, 1.6839, 2.3198, 1.1395, 2.5143, 0.0000, 0.2488, 1.2635,\n",
       "         0.1675, 2.3370, 0.3004, 0.0000, 1.2092, 1.5497, 0.8175, 0.7278, 0.0000,\n",
       "         2.1136, 0.7884, 1.4866, 1.4822, 1.7751, 0.5552, 2.2711, 0.6361, 0.3734,\n",
       "         0.5739, 0.0000]),\n",
       " tensor([ 8.2367,  4.4774,  1.7566,  0.7045,  0.0000,  4.4952,  1.0070,  5.2185,\n",
       "          2.8874,  3.0372,  4.3059,  0.0000,  0.9091,  5.6100,  4.3585,  6.0922,\n",
       "          0.0000,  2.6083,  1.9377, 12.6985,  0.9498,  2.1614,  7.2686,  2.4771,\n",
       "          2.7518,  3.1756,  2.2788,  3.7402,  1.6205,  5.4123,  2.5454,  0.5587,\n",
       "          2.7830,  2.3684,  9.6194,  3.9855,  0.0000,  1.4287,  1.1635,  0.0000,\n",
       "          5.7823,  1.3779,  5.9078,  0.8547,  2.9144,  3.6263,  1.2750,  0.7589,\n",
       "          1.5028,  4.3457,  3.8669,  8.8680,  4.0252,  0.0000,  3.9904,  8.6936,\n",
       "          7.0286,  0.3020,  5.5416,  0.0000,  0.0000,  5.1891,  3.5640,  2.0464])]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle_mod.compute_significance(xx, yy, mode=modes[1], normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([18.7008, 17.2124,  0.0000, 18.8252, 17.7527,  9.8356, 19.3307,  8.0831,\n",
       "         19.3016, 10.0615, 26.9800, 27.1140, 20.8047, 11.8404, 18.7308, 28.9580,\n",
       "         17.6353, 15.3055, 23.9021, 13.0712, 23.2089,  5.5523, 22.3621, 16.1522,\n",
       "          6.1595,  4.3828, 30.8578, 17.9765,  3.8927,  8.2727, 32.7008, 23.6238,\n",
       "         22.8824,  0.0000,  5.8859,  3.9255,  0.0000, 18.5689,  0.0000, 21.9841,\n",
       "          0.0000,  8.2741, 23.8558,  0.0000,  9.9601,  8.1086,  0.0000,  1.7687,\n",
       "         10.4644, 12.8090, 27.3795, 27.8015,  1.1915, 26.8675, 13.7592,  0.0000,\n",
       "         21.3335,  1.5240, 26.4493, 27.1177, 10.6793, 20.6754,  0.0000, 13.3105,\n",
       "          7.5875, 17.6572,  2.7125, 58.2491, 14.1572, 61.3366, 11.3919,  8.5474,\n",
       "         18.3741,  0.0000, 27.4239,  0.6300, 10.0370,  4.5102,  7.7792,  0.0000,\n",
       "          7.7157, 89.8425,  0.0000,  0.0000, 38.8277, 12.7981, 24.9589, 14.5716,\n",
       "         12.7680, 43.5608, 30.8543,  6.3961, 14.7989, 10.3001, 14.2710,  3.8125,\n",
       "         21.3587, 12.2650,  8.3320, 16.0910, 15.2016,  7.3257,  7.9952,  4.2904,\n",
       "         20.2614, 10.4131, 22.4820, 15.8042,  0.0000, 14.5852, 13.1529, 10.0958,\n",
       "          0.0000, 14.9253,  2.1776, 25.7457, 22.0556,  0.0000, 18.4809, 12.0041,\n",
       "          4.0441,  7.5494, 11.8237,  9.3473, 10.8815,  0.1939, 17.5133, 22.1935,\n",
       "         34.5781, 17.7563, 11.8707, 23.1929, 10.2031, 13.6190, 37.4394, 33.7217,\n",
       "         30.7804, 17.0284, 22.1155, 10.7189,  7.9357, 21.3740,  4.8495, 17.7488,\n",
       "         15.1541, 16.5895,  0.0000, 11.3359, 11.5288, 18.1127,  6.2069, 15.9089,\n",
       "         31.4230, 20.0709, 12.8865, 15.2509, 38.4393, 13.9659, 25.5688, 12.9837,\n",
       "         24.0123, 26.1711,  8.8030,  8.6609,  7.7908, 15.5984, 18.9307, 22.3500,\n",
       "         26.7500,  7.0579,  9.7820, 21.7475, 13.4808,  0.0000,  2.7322, 14.4950,\n",
       "         13.8497,  1.8542, 29.8980, 12.8710,  7.6028, 17.2825, 18.2906, 26.8188,\n",
       "          3.4046,  8.6183,  2.7441, 25.7008,  7.7697,  0.7611,  7.3129, 74.0929,\n",
       "         21.2119,  5.5562,  5.4166, 37.8909, 17.8100, 13.8600,  3.5008, 21.0132,\n",
       "          0.0000, 91.9574,  0.0000,  8.6692,  4.5785, 12.3194, 18.7724, 36.0346,\n",
       "         26.9648, 11.0000, 12.8361,  1.2221,  7.3495,  1.8989,  0.0000, 11.2685,\n",
       "          3.1661, 18.3568, 39.3767, 13.3936, 97.0400,  2.3137, 27.4628, 21.7274,\n",
       "         62.6005, 16.7347, 15.2062,  0.0000, 15.8189,  1.9611,  5.5365, 12.1412,\n",
       "         13.8513, 10.4533, 21.3794, 20.3050,  4.8837,  6.8359,  4.6459, 11.3560,\n",
       "         12.4505,  1.5823,  2.0138, 10.6198, 62.5517, 21.7970, 17.2555,  6.9995,\n",
       "          9.3482, 17.4675, 41.0421,  0.0000,  9.5808, 14.3842, 15.2926,  6.1941]),\n",
       " tensor([ 26.2134,   9.2363,  10.9279, 117.3457,  14.4199,  33.3507,  71.6808,\n",
       "          10.7166,   0.0000,   9.0030,  46.9419,   0.0000,  31.8140,   0.0000,\n",
       "           0.0000,  42.6276,  48.2580,  38.8541,  10.0710,  48.4804,  18.7230,\n",
       "          31.8900,  62.9837,   0.0000,   0.9558,  67.9370,  43.5290, 108.2561,\n",
       "          74.3420,  38.0669,  32.2709,  34.2985,   5.1955,  15.8322,  16.3156,\n",
       "          60.6267,   3.3127,  14.7627,  68.1203,  55.1096,   0.0000,  11.4992,\n",
       "          14.5870,  74.6060,  42.1355,  12.4668,  21.6810,   6.9487,  35.6861,\n",
       "          15.6431,  13.8981,  32.3079,   0.0000,  32.9965,  16.3468,  64.3605,\n",
       "          87.6367,  16.9861,  34.1372,  13.8663,  12.0656,   9.3590,  20.6291,\n",
       "          18.7721,  34.9848,  45.8689,  19.1823,  17.7273,  45.6009,  78.6687,\n",
       "          59.2267,  30.9407,   5.2942,   0.0000,  27.4166,  82.0703,  16.4076,\n",
       "           7.6407,  30.9022,  46.2857,   0.0000,  34.4169,   0.0000,   0.0000,\n",
       "           5.1588,  30.6727,  46.7541,  29.3057,   8.0858,  12.2148,  50.0578,\n",
       "          81.4236,  21.6091,  50.1410,  13.6890,  29.2004,  13.0121,  23.1729,\n",
       "          41.2018,   0.0000,  17.8238,  69.9192,  54.2858,  25.6249,  37.7324,\n",
       "           0.0000,  33.2772,  90.2307,  29.5417,  86.4254,   2.2135,   0.0000,\n",
       "          76.2124,   0.4226,  10.8044,   5.4151,   0.0000,  57.8489,  57.8029,\n",
       "          44.6775,  72.4406,  44.5549,  45.5508,  84.7088,  31.3094,   0.9770,\n",
       "          72.6745,   0.0000]),\n",
       " tensor([  6.2629, 149.1393,  57.5013,  65.9586,   0.0000,  62.6665,  59.5411,\n",
       "          30.2254, 124.6814,  37.3544, 136.6496,   0.0000,  60.3245,  36.0604,\n",
       "          71.3641,   1.3102,   0.0000,  78.8658,   6.3189,  20.3595,  56.1121,\n",
       "           8.4033,   4.8656,  60.8678,  56.1724, 104.7422,  50.4687, 136.8293,\n",
       "          65.0016,  31.2256,  90.8763,  65.8467, 103.8327,  66.1494, 233.9263,\n",
       "          45.9197,   0.0000,  23.2113,  86.5111,   0.0000, 166.9097,  58.9597,\n",
       "          40.4958,  79.2208, 123.2669,  86.2163,  29.7293,  65.9182,  90.3809,\n",
       "          64.4326, 121.8572,  16.7724,  31.6986,   0.0000, 136.3651, 102.1196,\n",
       "          89.9423,  42.8864, 114.3981,   0.0000,   0.0000, 105.6743,  59.9172,\n",
       "           5.8244])]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle_mod.compute_significance(xx, yy, mode=modes[2], normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.9568, 0.1873, 0.0000, 0.4858, 0.4294, 0.5639, 0.6374, 0.6543, 0.3412,\n",
       "         0.3774, 0.6058, 0.9263, 0.4005, 0.3072, 0.7555, 0.8900, 0.4419, 0.4136,\n",
       "         0.8522, 0.6309, 0.4823, 0.2580, 0.4870, 0.5977, 0.2362, 0.3728, 0.2536,\n",
       "         0.2668, 0.6639, 0.7721, 0.5009, 0.4780, 0.5293, 0.0000, 0.2381, 0.6056,\n",
       "         0.0000, 0.4263, 0.0000, 0.5496, 0.0000, 0.4056, 0.6556, 0.0000, 0.1994,\n",
       "         0.7624, 0.0000, 0.4966, 0.4863, 0.6035, 0.6720, 0.3221, 0.7453, 0.9458,\n",
       "         0.4804, 0.0000, 0.7611, 0.7352, 0.6815, 0.6337, 1.9885, 0.5199, 0.0000,\n",
       "         0.4648, 0.2490, 0.8173, 0.6866, 0.8768, 0.1923, 2.6001, 0.4840, 0.5367,\n",
       "         0.8049, 0.0000, 0.9763, 0.3590, 0.4352, 0.6047, 0.5474, 0.0000, 0.4659,\n",
       "         2.4419, 0.0000, 0.0000, 0.8745, 0.6818, 0.3374, 0.2981, 0.4229, 1.1137,\n",
       "         0.5367, 0.8340, 0.2817, 0.5369, 0.4976, 0.3909, 1.0648, 0.3270, 0.7534,\n",
       "         0.1697, 2.4210, 0.5860, 0.3401, 0.6916, 0.7055, 0.4560, 0.8908, 0.2610,\n",
       "         0.0000, 0.3835, 0.4999, 0.4641, 0.0000, 0.6444, 0.4211, 0.5640, 0.8032,\n",
       "         0.0000, 0.1866, 0.6678, 0.9435, 0.4261, 0.4156, 0.3412, 0.8083, 0.4011,\n",
       "         0.9133, 0.6723, 1.0568, 0.3636, 0.4092, 1.0267, 0.4634, 0.3058, 0.9530,\n",
       "         0.7314, 0.8386, 0.7297, 0.8181, 0.8042, 0.5248, 1.0419, 0.5879, 0.3924,\n",
       "         0.4329, 1.0794, 0.0000, 0.3792, 0.3170, 0.2688, 0.6908, 1.4142, 1.1480,\n",
       "         0.6101, 0.7014, 0.6699, 1.7794, 0.3264, 0.4140, 0.4304, 0.3575, 0.7488,\n",
       "         0.7319, 0.7440, 0.4009, 0.6111, 0.6134, 0.6659, 0.4800, 0.5045, 0.4631,\n",
       "         0.4949, 0.5389, 0.0000, 0.2698, 0.5715, 0.7812, 0.4451, 0.6277, 0.8628,\n",
       "         0.4706, 0.6305, 0.4975, 0.7246, 0.8172, 0.8220, 0.5462, 0.4370, 0.7265,\n",
       "         0.6093, 0.3588, 1.4472, 0.2895, 0.4738, 0.6450, 2.6783, 0.8503, 0.7795,\n",
       "         0.5659, 0.5998, 0.0000, 1.0752, 0.0000, 0.6515, 0.7924, 0.5287, 0.4583,\n",
       "         0.9888, 0.9351, 0.1714, 0.5539, 0.5555, 0.7884, 0.9072, 0.0000, 0.7503,\n",
       "         0.5139, 0.2738, 1.2478, 0.4266, 3.1385, 0.3507, 0.5838, 0.3076, 1.7926,\n",
       "         0.7864, 0.7623, 0.0000, 0.1751, 0.8224, 0.4725, 0.3720, 0.2415, 0.9717,\n",
       "         0.7029, 0.4104, 0.7734, 0.4720, 0.2866, 0.2349, 0.7571, 0.3422, 0.7096,\n",
       "         0.4480, 1.7187, 0.4439, 0.8513, 0.1729, 0.8662, 0.3252, 0.4802, 0.0000,\n",
       "         0.4942, 0.4997, 1.9529, 0.2600]),\n",
       " tensor([0.2116, 1.4293, 0.5682, 2.8246, 1.1970, 0.8738, 0.6531, 1.6093, 0.0000,\n",
       "         1.9455, 1.3271, 0.0000, 1.5277, 0.0000, 0.0000, 1.2462, 0.5477, 0.7658,\n",
       "         1.8349, 1.4892, 2.1101, 1.2867, 1.4950, 0.0000, 1.1560, 2.0199, 1.3274,\n",
       "         2.4925, 1.9808, 2.1168, 1.1557, 1.6485, 0.8701, 1.3565, 2.0262, 0.8691,\n",
       "         2.1717, 0.5474, 2.3280, 0.3234, 0.0000, 1.0175, 1.6482, 1.6030, 0.9951,\n",
       "         1.1177, 1.3964, 1.6282, 0.7972, 1.3185, 1.6643, 0.3504, 0.0000, 0.6399,\n",
       "         0.7827, 0.9162, 2.0433, 1.7380, 0.5294, 0.8121, 1.2424, 2.1443, 1.0760,\n",
       "         0.7044, 0.4860, 1.0070, 2.3284, 0.6867, 0.5654, 1.9033, 0.5744, 0.5242,\n",
       "         1.1952, 0.0000, 1.5790, 2.5188, 1.9791, 0.0677, 1.9816, 1.3557, 0.0000,\n",
       "         1.5647, 0.0000, 0.0000, 1.6289, 0.7920, 1.2007, 1.3933, 0.9860, 1.1552,\n",
       "         1.4579, 1.4162, 0.4924, 1.4131, 1.5938, 1.1619, 0.8383, 1.2549, 1.3610,\n",
       "         0.0000, 1.2473, 1.7673, 1.9443, 1.5875, 2.2859, 0.0000, 0.4808, 1.5857,\n",
       "         0.4073, 2.0653, 0.8242, 0.0000, 1.6252, 1.9049, 1.3248, 1.2027, 0.0000,\n",
       "         2.2060, 1.1255, 1.8390, 1.8875, 2.1588, 0.9300, 2.3422, 1.1003, 0.8591,\n",
       "         0.7782, 0.0000]),\n",
       " tensor([4.3035, 3.0862, 1.8256, 0.9435, 0.0000, 3.2124, 0.9440, 3.4987, 2.3060,\n",
       "         2.3553, 3.1018, 0.0000, 1.0751, 3.3800, 3.1017, 3.6023, 0.0000, 2.4177,\n",
       "         1.9368, 5.3095, 1.3398, 2.2526, 3.9754, 2.4089, 2.4010, 2.8795, 1.8977,\n",
       "         2.8619, 1.6989, 3.3962, 2.2146, 0.7148, 2.2297, 2.4576, 4.3698, 2.9915,\n",
       "         0.0000, 1.7627, 1.5397, 0.0000, 3.5528, 1.2620, 3.6244, 1.3294, 2.0708,\n",
       "         2.1950, 1.5467, 1.0986, 1.8002, 3.2636, 2.6860, 4.6710, 2.9751, 0.0000,\n",
       "         2.6739, 4.8632, 4.2385, 0.6279, 3.7521, 0.0000, 0.0000, 3.3243, 2.8704,\n",
       "         1.9089])]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle_mod.compute_significance(xx, yy, mode=modes[3], normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get pruning mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pruning_mask(importance, output_dim, num_prune=1):\n",
    "    layer_dims = []\n",
    "    for imp in importance:\n",
    "        layer_dims.append(len(imp))\n",
    "    \n",
    "    imps = torch.ones(len(imp), max(layer_dims))*sum(layer_dims)*10\n",
    "    imps_shape = imps.shape\n",
    "    for i, imp in enumerate(importance):\n",
    "        imps[i, :len(imp)] = imp\n",
    "        \n",
    "#     print(imps)\n",
    "    imps = imps.reshape(-1)\n",
    "    indices = torch.argsort(imps)\n",
    "#     print(indices)\n",
    "    imps[indices[:num_prune]] = -1.\n",
    "    imps = imps.reshape(imps_shape)\n",
    "    \n",
    "    mask = (imps>=0).type(torch.float)\n",
    "    masks = []\n",
    "    for i, imp in enumerate(importance):\n",
    "        masks.append(mask[i, :len(imp)])\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance = oracle_mod.compute_significance(xx, yy, mode=modes[3], normalize=True)\n",
    "# importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_pruning_mask(importance, 1, num_prune=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = oracle_mod.compute_significance(xx, yy, mode=modes[0], normalize=True)\n",
    "get_pruning_mask(importance, 1, num_prune=num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1.]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = oracle_mod.compute_significance(xx, yy, mode=modes[1], normalize=True)\n",
    "get_pruning_mask(importance, 1, num_prune=num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = oracle_mod.compute_significance(xx, yy, mode=modes[2], normalize=True)\n",
    "get_pruning_mask(importance, 1, num_prune=num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1.]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = oracle_mod.compute_significance(xx, yy, mode=modes[3], normalize=True)\n",
    "get_pruning_mask(importance, 1, num_prune=num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pruning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pruner():\n",
    "    \n",
    "    def __init__(self, net, prune_mask=None):\n",
    "        self.net = net\n",
    "        self.keys = []\n",
    "        self.prune_mask = {}\n",
    "        self.forward_hook = {}\n",
    "        \n",
    "        self.activations = []\n",
    "        \n",
    "        for name, module in list(self.net._modules.items()):\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                self.keys.append(module)\n",
    "\n",
    "        if prune_mask is not None:\n",
    "            self.add_prune_mask(prune_mask)\n",
    "        self.remove_hook()\n",
    "        \n",
    "    def add_prune_mask(self, prune_mask):\n",
    "        for module, pm in zip(self.keys[:-1], prune_mask):\n",
    "            self.prune_mask[module] = pm.type(torch.float)\n",
    "        self.prune_mask[self.keys[-1]] = torch.ones(self.keys[-1].out_features, dtype=torch.float)\n",
    "            \n",
    "        \n",
    "    def prune_neurons(self, module, inp, out):\n",
    "        mask = self.prune_mask[module]\n",
    "        output = out*mask\n",
    "        \n",
    "        self.activations.append(output)\n",
    "        return output\n",
    "        \n",
    "    def forward(self, x, prune_mask=None):\n",
    "        if prune_mask:\n",
    "            self.add_prune_mask(prune_mask)\n",
    "            if len(self.forward_hook) == 0:\n",
    "                self.add_hook()\n",
    "        \n",
    "        y = self.net(x)\n",
    "        self.remove_hook()\n",
    "        return y\n",
    "        \n",
    "        \n",
    "    def add_hook(self):\n",
    "        if len(self.forward_hook) > 0:\n",
    "            self.remove_hook()\n",
    "            \n",
    "        self.forward_hook = {}\n",
    "        for name, module in list(self.net._modules.items()):\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                hook = module.register_forward_hook(self.prune_neurons)\n",
    "                self.forward_hook[module] = hook\n",
    "        return\n",
    "        \n",
    "    def remove_hook(self):       \n",
    "        for module in self.forward_hook.keys():\n",
    "            hook = self.forward_hook[module]\n",
    "            hook.remove()\n",
    "        self.forward_hook = {}\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = oracle_mod.compute_significance(xx, yy, mode=modes[0], normalize=True)\n",
    "pmask = get_pruning_mask(importance, 1, num_prune=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([2.7180e-01, 7.1302e-01, 0.0000e+00, 4.6125e-01, 5.9188e-01, 4.3188e-02,\n",
       "         3.3094e-01, 2.9887e-01, 4.4542e-01, 4.7126e-03, 1.1225e+00, 1.0540e+00,\n",
       "         2.5839e-01, 5.6276e-01, 8.0953e-01, 2.3213e-01, 7.8328e-01, 5.7911e-01,\n",
       "         8.0413e-01, 9.7869e-02, 6.5135e-01, 4.3181e-01, 2.3574e-01, 6.9368e-02,\n",
       "         7.3158e-01, 1.4478e-02, 1.7127e+00, 4.5066e-01, 4.8489e-01, 3.6393e-01,\n",
       "         1.2598e+00, 5.8190e-01, 4.7238e-01, 0.0000e+00, 1.9083e-02, 4.5319e-01,\n",
       "         0.0000e+00, 1.8560e-01, 0.0000e+00, 6.1826e-01, 0.0000e+00, 2.8621e-01,\n",
       "         1.3466e-01, 0.0000e+00, 3.9646e-01, 5.7998e-01, 0.0000e+00, 5.0043e-01,\n",
       "         6.5398e-01, 7.3030e-01, 5.8462e-01, 2.0065e+00, 4.7246e-01, 1.5259e+00,\n",
       "         6.6273e-01, 0.0000e+00, 4.3994e-01, 3.7250e-01, 2.7707e-01, 1.4977e+00,\n",
       "         1.6379e-01, 7.7247e-01, 0.0000e+00, 2.8800e-01, 5.6158e-01, 7.2502e-01,\n",
       "         1.2063e-01, 2.6880e+00, 3.9583e-01, 4.6785e+00, 2.2986e-01, 3.4308e-01,\n",
       "         3.5786e-01, 0.0000e+00, 6.4040e-01, 1.9124e-01, 4.9108e-01, 2.5309e-01,\n",
       "         2.1812e-01, 0.0000e+00, 2.2634e-01, 4.9017e+00, 0.0000e+00, 0.0000e+00,\n",
       "         2.3468e+00, 6.8189e-01, 1.0231e+00, 8.0938e-01, 3.7938e-01, 1.4965e+00,\n",
       "         6.1788e-01, 4.3719e-01, 5.3017e-01, 3.1891e-01, 3.0700e-01, 1.0845e-01,\n",
       "         1.7163e+00, 6.8916e-01, 2.1229e-01, 1.0288e+00, 4.7634e-01, 3.5951e-01,\n",
       "         1.8883e-01, 2.5374e-01, 3.1555e-01, 1.6716e-01, 8.9255e-01, 8.4065e-01,\n",
       "         0.0000e+00, 2.6031e-01, 1.3927e-01, 2.3890e-02, 0.0000e+00, 7.7891e-01,\n",
       "         1.1778e-01, 1.0739e+00, 8.7912e-01, 0.0000e+00, 4.5015e-01, 3.9548e-01,\n",
       "         2.9530e-01, 2.0546e-01, 3.6836e-01, 9.8558e-02, 1.1752e+00, 3.9575e-01,\n",
       "         7.6247e-01, 9.1964e-01, 9.4475e-01, 3.1186e-01, 1.0436e-02, 2.7865e-02,\n",
       "         1.3346e-01, 3.4429e-01, 1.2605e+00, 1.8140e+00, 1.0874e+00, 4.9875e-02,\n",
       "         6.1814e-01, 1.9870e-01, 2.4009e-01, 6.7645e-02, 3.3098e-01, 3.8392e-01,\n",
       "         7.9569e-01, 4.0249e+00, 0.0000e+00, 2.8306e-01, 6.7473e-01, 1.0519e+00,\n",
       "         3.2734e-01, 2.7485e+00, 1.5104e+00, 6.2586e-01, 9.4469e-02, 3.6600e-01,\n",
       "         8.6415e-02, 6.0089e-01, 5.4038e-01, 3.1474e-01, 1.0916e+00, 6.3064e-01,\n",
       "         7.7634e-02, 2.5766e-01, 1.5178e-01, 9.5041e-01, 8.9300e-01, 1.0452e+00,\n",
       "         4.6033e-01, 1.4174e-01, 2.9302e-01, 7.3300e-03, 2.2133e-01, 0.0000e+00,\n",
       "         8.3068e-05, 1.2564e-01, 3.6657e-01, 3.7788e-01, 7.8487e-01, 6.1177e-01,\n",
       "         2.1376e-01, 4.8006e-02, 5.4577e-01, 6.5508e-01, 6.3442e-01, 1.2313e-01,\n",
       "         8.9509e-02, 1.3158e+00, 5.6615e-01, 1.6097e-01, 4.1895e-01, 4.3337e+00,\n",
       "         4.9459e-01, 2.0047e-01, 7.5124e-01, 5.3390e+00, 9.7153e-01, 1.9580e-01,\n",
       "         1.0254e-01, 1.0402e+00, 0.0000e+00, 5.8978e+00, 0.0000e+00, 2.8692e-01,\n",
       "         2.8302e-01, 5.6565e-01, 6.3408e-01, 9.7309e-01, 4.8480e-01, 8.6124e-01,\n",
       "         2.3859e-01, 2.1003e-01, 7.5262e-01, 5.0192e-01, 0.0000e+00, 3.1603e-01,\n",
       "         1.1828e-02, 7.5925e-01, 1.5444e+00, 1.4707e-01, 1.7040e-01, 1.6359e-01,\n",
       "         1.2397e+00, 9.2857e-01, 5.0090e+00, 6.7145e-01, 1.3247e-01, 0.0000e+00,\n",
       "         1.0038e+00, 7.1207e-02, 1.9707e-01, 5.5360e-01, 4.0407e-01, 1.6941e-01,\n",
       "         4.3718e-01, 3.1946e-01, 7.7980e-02, 3.1589e-01, 1.5201e-01, 1.4194e-01,\n",
       "         3.6670e-03, 8.8127e-02, 3.4355e-01, 5.1013e-01, 5.0871e+00, 5.4852e-01,\n",
       "         6.2169e-01, 1.6710e-01, 5.5699e-03, 4.1191e-01, 2.6900e+00, 0.0000e+00,\n",
       "         6.6416e-01, 2.0625e-01, 3.4653e+00, 2.1467e-01]),\n",
       " tensor([0.8691, 0.7889, 0.6190, 4.0941, 0.7749, 1.6921, 2.7733, 0.6634, 0.0000,\n",
       "         0.4388, 1.3013, 0.0000, 0.1832, 0.0000, 0.0000, 0.4935, 2.8635, 0.8907,\n",
       "         1.3581, 0.7373, 0.3128, 1.1768, 2.8917, 0.0000, 0.0857, 3.1255, 2.2123,\n",
       "         3.2734, 2.1570, 0.7286, 0.0493, 0.9647, 0.1900, 0.4425, 1.0365, 2.1254,\n",
       "         0.9883, 0.4755, 0.1439, 2.0229, 0.0000, 0.1749, 0.3185, 2.6884, 2.0691,\n",
       "         1.1734, 0.7188, 0.2094, 0.5124, 0.1400, 0.6964, 1.1000, 0.0000, 1.4832,\n",
       "         0.7320, 2.0851, 3.6538, 0.9713, 2.6830, 1.0239, 0.3909, 1.1789, 0.5867,\n",
       "         0.3568, 1.5845, 1.0904, 1.0825, 0.5578, 2.1119, 2.3654, 1.9619, 1.5325,\n",
       "         0.0269, 0.0000, 2.1573, 0.8655, 0.8088, 0.2422, 0.6995, 1.0698, 0.0000,\n",
       "         0.6078, 0.0000, 0.0000, 0.8204, 1.0986, 1.5478, 1.2136, 0.5697, 0.5736,\n",
       "         2.2587, 2.7142, 1.0404, 2.1309, 0.7956, 1.0933, 0.2779, 1.7356, 1.0355,\n",
       "         0.0000, 0.6345, 2.4485, 0.5230, 1.8014, 1.7088, 0.0000, 0.7866, 3.0282,\n",
       "         0.9876, 3.3882, 0.0495, 0.0000, 2.6177, 0.0951, 0.2625, 0.7117, 0.0000,\n",
       "         3.4703, 1.2993, 2.1530, 3.8427, 3.0202, 1.7293, 1.4341, 0.4920, 0.2021,\n",
       "         2.0187, 0.0000]),\n",
       " tensor([0.4768, 3.6011, 2.0190, 2.7621, 0.0000, 1.4347, 3.1188, 0.0605, 2.0629,\n",
       "         0.5882, 3.6859, 0.0000, 2.7236, 1.8935, 4.3954, 1.5561, 0.0000, 3.7674,\n",
       "         0.3334, 1.6530, 1.3380, 0.3689, 0.7704, 3.0433, 1.3809, 2.7341, 0.4273,\n",
       "         5.6298, 2.0889, 2.8212, 3.5737, 1.4089, 3.6924, 2.6004, 9.9596, 0.1748,\n",
       "         0.0000, 0.1649, 2.8021, 0.0000, 6.3167, 0.8149, 1.8348, 3.3214, 4.2289,\n",
       "         3.4661, 0.9782, 2.6342, 2.2481, 3.4043, 2.9770, 0.3697, 1.2846, 0.0000,\n",
       "         2.9315, 7.3834, 4.1651, 1.6959, 3.2567, 0.0000, 0.0000, 2.2851, 2.0473,\n",
       "         0.6580])]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = Pruner(net, pmask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "yout_normal = net.forward(xx).data.cpu()\n",
    "yout_prune = pnet.forward(xx).data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 10]), torch.Size([60000, 10]))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yout_normal.shape, yout_prune.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0811), tensor(0.0811))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(yout_normal, yy), criterion(yout_prune, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((yout_prune-yout_normal)**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparing for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is  0.08113747835159302\n",
      "oracle\n",
      "new error =  2.858886241912842\n",
      "deviation =  44.69500732421875\n",
      "oracle_absolute\n",
      "new error =  0.904782772064209\n",
      "deviation =  19.14569091796875\n",
      "oracle_normalized\n",
      "new error =  2.2273101806640625\n",
      "deviation =  42.68088912963867\n",
      "oracle_abs_norm\n",
      "new error =  1.6182787418365479\n",
      "deviation =  20.501169204711914\n"
     ]
    }
   ],
   "source": [
    "num = 200\n",
    "pnet = Pruner(net)\n",
    "\n",
    "yout_normal = net.forward(xx).data.cpu()\n",
    "print(\"loss is \", float(criterion(yout_normal, yy)))\n",
    "for i in range(4):\n",
    "    print(modes[i])\n",
    "    importance = oracle_mod.compute_significance(xx, yy, mode=modes[i], normalize=True)\n",
    "    pmask = get_pruning_mask(importance, 1, num_prune=num)\n",
    "\n",
    "#     for pm in pmask:\n",
    "#         print(pm)\n",
    "    \n",
    "    yout_prune = pnet.forward(xx, prune_mask=pmask).data.cpu()\n",
    "    new_err = criterion(yout_prune, yy)\n",
    "    print(\"new error = \", float(new_err))\n",
    "    \n",
    "    deviation = ((yout_prune-yout_normal)**2).mean()\n",
    "    print(\"deviation = \", float(deviation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=784, out_features=256, bias=True),\n",
       " Linear(in_features=256, out_features=128, bias=True),\n",
       " Linear(in_features=128, out_features=64, bias=True),\n",
       " Linear(in_features=64, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnet.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
