{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import datasets\n",
    "import random, sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\")\n",
    "device = torch.device(\"cuda:1\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "test_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"../../../../_Datasets/FMNIST/\", train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.FashionMNIST(root=\"../../../../_Datasets/FMNIST/\", train=False, download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "BS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BS, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BS, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## demo of train loader\n",
    "xx, yy = iter(train_loader).next()\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda -bmm2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bmm2x2_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMM2x2Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "#     @torch.jit.ignore\n",
    "    def forward(ctx, inputs, weights):\n",
    "        outputs = bmm2x2_cuda.forward(inputs, weights)\n",
    "        ctx.save_for_backward(inputs, weights)\n",
    "        return outputs[0]\n",
    "    \n",
    "    @staticmethod\n",
    "#     @torch.jit.ignore\n",
    "    def backward(ctx, grad_output):\n",
    "        inputs, weights = ctx.saved_tensors\n",
    "        del_input, del_weights = bmm2x2_cuda.backward(\n",
    "            inputs, \n",
    "            weights, \n",
    "            grad_output)\n",
    "    \n",
    "        return del_input, del_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairWeight2(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        assert input_dim%2 == 0, \"Input dim must be even number\"\n",
    "        self.weight = torch.eye(2).unsqueeze(0).repeat_interleave(input_dim//2, dim=0)\n",
    "        self.weight = nn.Parameter(self.weight)\n",
    "        self.bmmfunc = BMM2x2Function()\n",
    "        \n",
    "    @torch.jit.ignore\n",
    "    def bmm(self, x, w):\n",
    "        return BMM2x2Function.apply(x, w)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, dim = x.shape[0], x.shape[1]\n",
    "        x = x.view(bs, -1, 2)\n",
    "        x = self.bmm(x, self.weight)\n",
    "        x = x.view(bs, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda - Bilinear2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bilinear2x2_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_input_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLinear2x2Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, weights):\n",
    "        outputs = bilinear2x2_cuda.forward(inputs, weights)\n",
    "        ctx.save_for_backward(inputs, weights)\n",
    "        return outputs[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        global del_input_list\n",
    "        inputs, weights = ctx.saved_tensors\n",
    "#         del_input, del_weights = bmm2x2_cuda.backward(\n",
    "#             grad_output.contiguous(), \n",
    "#             grad_cell.contiguous(), \n",
    "#             grad_output.contiguous())\n",
    "        del_input, del_weights = bilinear2x2_cuda.backward(\n",
    "            inputs, \n",
    "            weights, \n",
    "            grad_output)\n",
    "#         del_input_list.append(del_input)\n",
    "        return del_input, del_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairBilinear2(nn.Module):\n",
    "    def __init__(self, dim, grid_width):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.grid_width = grid_width\n",
    "        \n",
    "        self.num_pairs = self.dim // 2\n",
    "        along_row = torch.linspace(0, 1, self.grid_width).reshape(1, -1).t()\n",
    "        along_col = torch.linspace(0, 1, self.grid_width).reshape(-1, 1).t()\n",
    "        \n",
    "        self.Y = torch.stack([along_row+along_col*0, along_row*0+along_col])\n",
    "        self.Y = torch.repeat_interleave(self.Y.unsqueeze(0), self.num_pairs, dim=0)\n",
    "        self.Y = nn.Parameter(self.Y)\n",
    "        \n",
    "        self.pairW = torch.eye(2).unsqueeze(0).repeat_interleave(self.num_pairs, dim=0)\n",
    "        self.pairW = nn.Parameter(self.pairW)\n",
    "    \n",
    "#     @torch.jit.ignore\n",
    "#     def pairbl2x2(self, x, w):\n",
    "#         return BiLinear2x2Function.apply(x, w)\n",
    "    \n",
    "#     @torch.jit.ignore\n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        \n",
    "############# This block ########################\n",
    "        ### this block is significantly faster\n",
    "    \n",
    "#         x = x.view(bs, -1, 2).transpose(0,1)\n",
    "#         x = torch.bmm(x, self.pairW)\n",
    "#         x = x.transpose(1,0)#.reshape(-1, 2)\n",
    "        \n",
    "############# OR This block ########################\n",
    "        x = x.contiguous().view(bs, -1, 2)\n",
    "#         x = BMM2x2Function.apply(x, self.pairW)\n",
    "####################################################\n",
    "#         x = x.view(bs, -1, 2)\n",
    "        x = BiLinear2x2Function.apply(x, self.Y)\n",
    "        x = x.view(bs, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizedPairBilinearSpline_2(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, grid_width, num_layers=None):\n",
    "        super().__init__()\n",
    "        assert input_dim%2 == 0, \"Input dim must be even number\"\n",
    "        self.input_dim = input_dim\n",
    "        self.num_layers = int(np.ceil(np.log2(self.input_dim)))\n",
    "        if num_layers is not None:\n",
    "            self.num_layers = num_layers\n",
    "            \n",
    "        self.facto_nets = []\n",
    "        for i in range(self.num_layers):\n",
    "            net = PairBilinear2(self.input_dim, grid_width)\n",
    "            self.facto_nets.append(net)\n",
    "        self.facto_nets = nn.ModuleList(self.facto_nets)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        global del_input_list\n",
    "        del_input_list = []\n",
    "        \n",
    "        ## swap first and then forward and reverse-swap\n",
    "        bs = x.shape[0]\n",
    "        y = x\n",
    "#         for i in range(len(self.facto_nets)):\n",
    "        for i, fn in enumerate(self.facto_nets):\n",
    "            y = y.view(-1,2,2**(i)).permute(0, 2,1).contiguous().view(bs, -1)\n",
    "            y = fn(y) \n",
    "            y = y.view(-1,2**(i),2).permute(0, 2,1).contiguous()\n",
    "#         y = x + y ## this is residual addition... remove if only want feed forward\n",
    "        return y.view(bs, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PairBilinearBlock_2(FactorizedPairBilinearSpline_2):\n",
    "    \n",
    "#     def __init__(self, input_dim, grid_width):\n",
    "#         num_layers = int(np.ceil(np.log2(input_dim)))\n",
    "#         extra =  2**num_layers - input_dim\n",
    "#         torch.manual_seed(123)\n",
    "#         self.selector = torch.randperm(input_dim)[:extra]\n",
    "        \n",
    "#         super().__init__(2**num_layers, grid_width)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         '''\n",
    "#         x should have dimension -> bs, M\n",
    "#         '''\n",
    "#         x = torch.cat((x, x[:, self.selector]), dim=1)\n",
    "#         return super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fused 2x2 Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fused2x2ops_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_del_input = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedBiLinear2x2Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, weights, grids):\n",
    "        outputs, input_buffer, dbg = fused2x2ops_cuda.bilinear2x2_forward(inputs, weights, grids)\n",
    "#         print(dbg)\n",
    "\n",
    "#         outputs, input_buffer = fused2x2ops_cuda.bilinear2x2_forward(inputs, weights, grids)\n",
    "        ctx.save_for_backward(input_buffer, weights, grids)\n",
    "        return outputs, dbg\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output, a):\n",
    "        global recent_del_input\n",
    "        input_buffer, weights, grids = ctx.saved_tensors\n",
    "#         del_input, del_weights = bmm2x2_cuda.backward(\n",
    "#             grad_output.contiguous(), \n",
    "#             grad_cell.contiguous(), \n",
    "#             grad_output.contiguous())\n",
    "        del_input, del_weights, del_grids, del_inputs = fused2x2ops_cuda.bilinear2x2_backward(\n",
    "            input_buffer, \n",
    "            weights, \n",
    "            grids,\n",
    "            grad_output)\n",
    "    \n",
    "#         recent_del_input = del_inputs\n",
    "        return del_input, del_weights, del_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fused2x2BiLinear(nn.Module):\n",
    "    def __init__(self, dim, grid_width, num_layers = None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.grid_width = grid_width\n",
    "        \n",
    "        self.num_layers = int(np.ceil(np.log2(self.dim)))\n",
    "        if num_layers is not None:\n",
    "            self.num_layers = num_layers\n",
    "        \n",
    "        \n",
    "        self.num_pairs = self.dim // 2\n",
    "        \n",
    "#         along_row = torch.linspace(0, 1, self.grid_width).reshape(1, -1)\n",
    "#         along_col = torch.linspace(0, 1, self.grid_width).reshape(-1, 1)\n",
    "        \n",
    "        along_row = torch.linspace(0, 1, self.grid_width).reshape(1, -1).t()\n",
    "        along_col = torch.linspace(0, 1, self.grid_width).reshape(-1, 1).t()\n",
    "        \n",
    "        self.Y = torch.stack([along_row+along_col*0, along_row*0+along_col])\n",
    "        \n",
    "        ### repeat same for num_pairs\n",
    "        self.Y = torch.repeat_interleave(self.Y.unsqueeze(0), self.num_pairs, dim=0)\n",
    "        ### repeat same for num_Layers\n",
    "        self.Y = torch.repeat_interleave(self.Y.unsqueeze(0), self.num_layers, dim=0)\n",
    "        \n",
    "        print(self.Y.shape)\n",
    "        self.Y = nn.Parameter(self.Y)\n",
    "        \n",
    "        ### repeat same for num_pairs\n",
    "        self.pairW = torch.eye(2).unsqueeze(0).repeat_interleave(self.num_pairs, dim=0)\n",
    "        ### repeat same for num_Layers\n",
    "        self.pairW = self.pairW.unsqueeze(0).repeat_interleave(self.num_layers, dim=0)\n",
    "        \n",
    "        print(self.pairW.shape)\n",
    "        self.pairW = nn.Parameter(self.pairW)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.Y.data.clamp_(-10, 10)\n",
    "        x, self.dbg = FusedBiLinear2x2Function.apply(x.clone(), self.pairW, self.Y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 2, 3, 3])\n",
      "torch.Size([1, 8, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "N = 16\n",
    "fbl = Fused2x2BiLinear(N, 3, num_layers=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pbl = FactorizedPairBilinearSpline_2(N, 3, num_layers=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, N).to(device)\n",
    "y = fbl(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/IPython/core/formatters.py:707\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    701\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    703\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    704\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    705\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    706\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 707\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m callable(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor.py:305\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor_str.py:434\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_str\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 434\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor_str.py:409\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    408\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m                 tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    412\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor_str.py:264\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor_str.py:100\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_select(tensor_view, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m tensor_view\u001b[38;5;241m.\u001b[39mne(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "fbl.dbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/IPython/core/formatters.py:707\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    701\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    703\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    704\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    705\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    706\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 707\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m callable(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor.py:305\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor_str.py:434\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_str\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 434\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor_str.py:409\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    408\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m                 tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    412\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor_str.py:264\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor_str.py:100\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_select(tensor_view, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m tensor_view\u001b[38;5;241m.\u001b[39mne(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/IPython/core/formatters.py:707\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    701\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    703\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    704\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    705\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    706\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 707\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m callable(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor.py:305\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor_str.py:434\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_str\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 434\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor_str.py:409\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    408\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m                 tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    412\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor_str.py:264\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor_str.py:100\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_select(tensor_view, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m tensor_view\u001b[38;5;241m.\u001b[39mne(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "y.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbl.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbl = FactorizedPairBilinearSpline_2(N, 3, num_layers=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbl.facto_nets[0].Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = pbl(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbl.Y.grad[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbl.facto_nets[0].Y.grad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbl.Y.grad[0] - pbl.Y.grad.transpose(-1, -2)\n",
    "# fbl.Y.grad[0] - pbl.Y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbl.facto_nets[0].pairW.grad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbl.pairW.grad[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbl.facto_nets[0].pairW[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/IPython/core/formatters.py:707\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    701\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    703\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    704\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    705\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    706\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 707\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m callable(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor.py:305\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor_str.py:434\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_str\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 434\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor_str.py:409\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    408\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m                 tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    412\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor_str.py:264\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/_tensor_str.py:100\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_select(tensor_view, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m tensor_view\u001b[38;5;241m.\u001b[39mne(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "fbl.pairW[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m      2\u001b[0m L \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 3\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(a)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "N = 16\n",
    "L = 2\n",
    "a = torch.randn(5, N).to(device)\n",
    "t = torch.randn_like(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(y, t):\n",
    "    return ((y-t)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 2, 2, 2])\n",
      "torch.Size([2, 8, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "fbl = Fused2x2BiLinear(N, 2, num_layers=L).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbl_opt = torch.optim.Adam(fbl.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbl = FactorizedPairBilinearSpline_2(N, 2, num_layers=L).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbl_opt = torch.optim.Adam(pbl.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbl.pairW.data = torch.randn_likee(fbl.pairW.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pbl.pairW.data = fbl.pairW.data[0].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]]],\n",
       "\n",
       "\n",
       "        [[[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]],\n",
       "\n",
       "         [[1., 0.],\n",
       "          [0., 1.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.pairW.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]]], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbl.facto_nets[0].pairW.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbl.Y.data = torch.randn_like(fbl.Y.data)\n",
    "\n",
    "for i in range(L):\n",
    "    pbl.facto_nets[i].Y.data = fbl.Y.data[i].clone()#.transpose(-1,-2).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3791, -0.8440],\n",
       "         [-0.1559, -0.6066]],\n",
       "\n",
       "        [[ 1.6306,  3.1744],\n",
       "         [ 0.5283, -0.7442]]], device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.Y.data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3791, -0.8440],\n",
       "         [-0.1559, -0.6066]],\n",
       "\n",
       "        [[ 1.6306,  3.1744],\n",
       "         [ 0.5283, -0.7442]]], device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbl.facto_nets[0].Y.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fbl(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbl_opt.zero_grad()\n",
    "criterion(y,t).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = pbl(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbl_opt.zero_grad()\n",
    "criterion(y1,t).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.5037e+00, -3.7595e+00, -1.8056e+00,  5.1659e-01, -1.0865e+00,\n",
       "           1.2358e+01, -7.4260e-01,  2.4485e+01,  5.0571e+00,  2.1378e+00,\n",
       "           2.5427e+00,  2.5374e+00,  6.9556e+00, -1.8317e+00, -2.6482e+00,\n",
       "           1.1066e+00],\n",
       "         [ 9.7731e+00,  1.6325e+00, -3.8314e+00,  9.4500e-01,  2.3571e-01,\n",
       "           7.2137e-02, -4.0172e+00,  4.7148e-02, -6.4294e+00,  2.6724e+00,\n",
       "           8.9784e+00,  1.4810e+00, -6.1784e+00, -1.0216e+00, -2.0686e+00,\n",
       "          -2.8565e+00],\n",
       "         [-1.6147e+00, -2.9150e+00,  1.1380e+00, -1.5460e+00, -9.7056e+00,\n",
       "          -2.8597e+01,  2.3266e+01, -4.0729e+01,  7.2298e+00, -8.5021e-01,\n",
       "          -2.1144e+00, -3.2863e+00, -2.5194e-01,  2.2901e-01,  4.0116e-01,\n",
       "          -3.4700e+00],\n",
       "         [-1.4724e+00, -9.0094e-01, -1.0729e+00, -6.8572e-01, -3.9127e+00,\n",
       "           4.6269e+01,  1.8246e+00,  9.7919e+01,  4.7538e+00,  6.3082e-01,\n",
       "          -3.1558e-01, -1.1043e+00,  4.2851e-01, -1.1215e+00,  1.3154e+00,\n",
       "          -2.9661e+00],\n",
       "         [ 2.9581e+00, -2.9159e+00, -1.0895e-03,  3.3070e-02, -6.6026e+00,\n",
       "           2.5266e+01,  1.4058e+01,  5.1332e+01,  1.5162e+01,  2.2213e+00,\n",
       "          -7.6856e+00,  1.8602e+00, -1.4752e+00, -2.3001e+00, -1.8470e-01,\n",
       "           9.8815e-01]], device='cuda:0', grad_fn=<ViewBackward0>),\n",
       " tensor([[-1.5037e+00, -3.7595e+00, -1.8056e+00,  5.1659e-01, -1.0865e+00,\n",
       "           1.2358e+01, -7.4260e-01,  2.4485e+01,  5.0571e+00,  2.1378e+00,\n",
       "           2.5427e+00,  2.5374e+00,  6.9556e+00, -1.8317e+00, -2.6482e+00,\n",
       "           1.1066e+00],\n",
       "         [ 9.7731e+00,  1.6325e+00, -3.8314e+00,  9.4500e-01,  2.3571e-01,\n",
       "           7.2137e-02, -4.0172e+00,  4.7148e-02, -6.4294e+00,  2.6724e+00,\n",
       "           8.9784e+00,  1.4810e+00, -6.1784e+00, -1.0216e+00, -2.0686e+00,\n",
       "          -2.8565e+00],\n",
       "         [-1.6147e+00, -2.9150e+00,  1.1380e+00, -1.5460e+00, -9.7056e+00,\n",
       "          -2.8597e+01,  2.3266e+01, -4.0729e+01,  7.2298e+00, -8.5021e-01,\n",
       "          -2.1144e+00, -3.2863e+00, -2.5194e-01,  2.2901e-01,  4.0116e-01,\n",
       "          -3.4700e+00],\n",
       "         [-1.4724e+00, -9.0094e-01, -1.0729e+00, -6.8572e-01, -3.9127e+00,\n",
       "           4.6269e+01,  1.8246e+00,  9.7919e+01,  4.7538e+00,  6.3082e-01,\n",
       "          -3.1558e-01, -1.1043e+00,  4.2851e-01, -1.1215e+00,  1.3154e+00,\n",
       "          -2.9661e+00],\n",
       "         [ 2.9581e+00, -2.9159e+00, -1.0895e-03,  3.3070e-02, -6.6026e+00,\n",
       "           2.5266e+01,  1.4058e+01,  5.1332e+01,  1.5162e+01,  2.2213e+00,\n",
       "          -7.6856e+00,  1.8602e+00, -1.4752e+00, -2.3001e+00, -1.8470e-01,\n",
       "           9.8815e-01]], device='cuda:0',\n",
       "        grad_fn=<FusedBiLinear2x2FunctionBackward>))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOBSERVATIONS:\\n1. The kernel for copy operation works.; Now try matrix multiplication.. also works\\n2. \\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "OBSERVATIONS:\n",
    "1. The kernel for copy operation works.; Now try matrix multiplication.. also works\n",
    "2. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.data-y.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbl.Y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pbl.Y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbl_opt.step()\n",
    "pbl_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbl.Y.grad[0] - pbl.Y.grad.transpose(-1, -2)\n",
    "# torch.abs(fbl.Y.grad[0] - pbl.Y.grad).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.3132e-10, device='cuda:0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(fbl.Y.data[0] - pbl.facto_nets[0].Y.data).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0022, device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(fbl.Y.grad[0] - pbl.facto_nets[0].Y.grad).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(fbl.pairW.data[0] - pbl.facto_nets[0].pairW.data).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'Tensor' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mabs(\u001b[43mfbl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpairW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpbl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfacto_nets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpairW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'Tensor' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "torch.abs(fbl.pairW.grad[0] - pbl.facto_nets[0].pairW.grad).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pbl.pairW.grad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbl.pairW.grad[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbl.facto_nets[0].pairW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 1.]]], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.pairW[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(fbl.pairW.data[0] - pbl.facto_nets[0].pairW.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dissecting the difference in first gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.0518e-05, -1.5259e-05],\n",
       "          [-1.5259e-05,  1.1444e-05]],\n",
       "\n",
       "         [[-9.5367e-07,  1.6689e-06],\n",
       "          [ 4.7684e-07, -8.3447e-07]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00,  0.0000e+00],\n",
       "          [-1.5259e-05,  3.8147e-06]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00, -1.9073e-06]]],\n",
       "\n",
       "\n",
       "        [[[ 2.4414e-04, -6.1035e-05],\n",
       "          [ 0.0000e+00,  1.2207e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  2.4414e-04],\n",
       "          [-9.7656e-04, -1.2207e-04]]],\n",
       "\n",
       "\n",
       "        [[[-1.2207e-04,  6.1035e-05],\n",
       "          [ 3.0518e-05, -7.6294e-06]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-6.1035e-05,  0.0000e+00],\n",
       "          [-1.5259e-05,  3.0518e-05]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 1.9073e-06,  3.8147e-06],\n",
       "          [ 3.8147e-06,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 7.6294e-06, -7.6294e-06],\n",
       "          [-7.6294e-06,  1.5259e-05]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00]]]], device='cuda:0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.Y.grad[0] - pbl.facto_nets[0].Y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 327.9010, -143.4838],\n",
       "          [-111.4384,   48.9418]],\n",
       " \n",
       "         [[  15.8838,    2.4484],\n",
       "          [  -7.6424,   -1.4596]]], device='cuda:0'),\n",
       " tensor([[[ 327.9010, -143.4838],\n",
       "          [-111.4384,   48.9418]],\n",
       " \n",
       "         [[  15.8838,    2.4484],\n",
       "          [  -7.6424,   -1.4596]]], device='cuda:0'))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.Y.grad[0][0], pbl.facto_nets[0].Y.grad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbl.Y.grad[1] - pbl.facto_nets[1].Y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.3865e+00,  2.1475e+00, -5.0813e+01, -6.7917e+01, -7.7847e+00,\n",
       "         -3.0167e+02, -9.2780e-01,  8.8429e+02, -5.5581e+00,  1.1693e+01,\n",
       "          1.5529e+01,  4.6645e-01, -5.0991e+01,  7.8248e+00, -8.1608e+01,\n",
       "          1.0434e+01],\n",
       "        [ 1.3335e+02,  4.8941e+00, -6.6769e+01,  1.1105e+01,  2.9499e+01,\n",
       "          1.1546e+00, -1.5340e+01, -2.4701e-01, -1.1079e+02, -1.2389e+01,\n",
       "         -1.6451e+02, -5.8750e+00,  4.7702e+01, -1.1734e+01, -4.7073e+01,\n",
       "          1.6948e+01],\n",
       "        [ 2.7652e+00,  1.6842e+00,  3.8038e+01, -1.7971e+01, -7.3647e+02,\n",
       "          1.5797e+03, -2.0910e+02,  1.0404e+03, -2.1347e+01, -2.2273e+01,\n",
       "          1.0161e+02,  5.0339e+01,  4.0611e-01, -1.8502e+01,  4.3898e+00,\n",
       "          1.2963e+01],\n",
       "        [-3.4984e+00, -3.5313e-01, -6.3028e+00,  4.8524e+00, -7.9466e+00,\n",
       "         -5.8966e+03, -2.5341e+00,  2.8729e+03, -1.5595e+01, -4.1621e-01,\n",
       "          3.5140e+01, -1.9838e-01,  6.5884e-01, -1.2603e+01, -1.5375e+00,\n",
       "          1.0425e+01],\n",
       "        [-7.3082e+00,  8.5741e-01,  9.0775e+00, -3.4448e+01, -4.0897e+02,\n",
       "         -1.4387e+03, -8.6803e+01,  1.8491e+03, -2.7548e+02, -2.3281e+01,\n",
       "          2.1421e+02, -2.0885e+01, -2.9085e+00,  9.0631e+00,  8.5252e+00,\n",
       "          4.4185e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recent_del_input[1]#.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.3865e+00, -5.0813e+01],\n",
       "         [ 2.1475e+00, -6.7917e+01],\n",
       "         [-7.7847e+00, -9.2780e-01],\n",
       "         [-3.0167e+02,  8.8429e+02],\n",
       "         [-5.5581e+00,  1.5529e+01],\n",
       "         [ 1.1693e+01,  4.6645e-01],\n",
       "         [-5.0991e+01, -8.1608e+01],\n",
       "         [ 7.8248e+00,  1.0434e+01]],\n",
       "\n",
       "        [[ 1.3335e+02, -6.6769e+01],\n",
       "         [ 4.8941e+00,  1.1105e+01],\n",
       "         [ 2.9499e+01, -1.5340e+01],\n",
       "         [ 1.1546e+00, -2.4701e-01],\n",
       "         [-1.1079e+02, -1.6451e+02],\n",
       "         [-1.2389e+01, -5.8750e+00],\n",
       "         [ 4.7702e+01, -4.7073e+01],\n",
       "         [-1.1734e+01,  1.6948e+01]],\n",
       "\n",
       "        [[ 2.7652e+00,  3.8038e+01],\n",
       "         [ 1.6842e+00, -1.7971e+01],\n",
       "         [-7.3647e+02, -2.0910e+02],\n",
       "         [ 1.5797e+03,  1.0404e+03],\n",
       "         [-2.1347e+01,  1.0161e+02],\n",
       "         [-2.2273e+01,  5.0339e+01],\n",
       "         [ 4.0611e-01,  4.3898e+00],\n",
       "         [-1.8502e+01,  1.2963e+01]],\n",
       "\n",
       "        [[-3.4984e+00, -6.3028e+00],\n",
       "         [-3.5313e-01,  4.8524e+00],\n",
       "         [-7.9466e+00, -2.5341e+00],\n",
       "         [-5.8966e+03,  2.8729e+03],\n",
       "         [-1.5595e+01,  3.5140e+01],\n",
       "         [-4.1621e-01, -1.9838e-01],\n",
       "         [ 6.5884e-01, -1.5375e+00],\n",
       "         [-1.2603e+01,  1.0425e+01]],\n",
       "\n",
       "        [[-7.3082e+00,  9.0775e+00],\n",
       "         [ 8.5741e-01, -3.4448e+01],\n",
       "         [-4.0897e+02, -8.6803e+01],\n",
       "         [-1.4387e+03,  1.8491e+03],\n",
       "         [-2.7548e+02,  2.1421e+02],\n",
       "         [-2.3281e+01, -2.0885e+01],\n",
       "         [-2.9085e+00,  8.5252e+00],\n",
       "         [ 9.0631e+00,  4.4185e+00]]], device='cuda:0')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_input_list[0]#.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3842e-07, -2.3842e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00, -5.9605e-08,  0.0000e+00, -4.7684e-07,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -7.6294e-06,\n",
       "          0.0000e+00],\n",
       "        [-1.5259e-05,  4.7684e-07,  7.6294e-06,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  4.4703e-08,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00,  1.1921e-07,  0.0000e+00,  1.9073e-06,  6.1035e-05,\n",
       "          0.0000e+00,  1.5259e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          7.6294e-06,  0.0000e+00,  0.0000e+00, -1.9073e-06,  4.7684e-07,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00, -4.7684e-07,  0.0000e+00,  9.5367e-07,\n",
       "          0.0000e+00, -2.3842e-07,  0.0000e+00,  0.0000e+00,  2.9802e-08,\n",
       "          3.8147e-06, -4.4703e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         -1.2207e-04,  0.0000e+00, -1.2207e-04,  0.0000e+00,  0.0000e+00,\n",
       "         -1.5259e-05,  0.0000e+00,  0.0000e+00, -9.5367e-07,  0.0000e+00,\n",
       "          0.0000e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_input_list[0].reshape(5, -1, 2, 2).transpose(-1, -2).reshape(5, -1) - recent_del_input[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedPairBilinearBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, grid_width):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_layers = int(np.ceil(np.log2(input_dim)))\n",
    "        \n",
    "        extra =  2**self.num_layers - input_dim\n",
    "        torch.manual_seed(123)\n",
    "        self.selector = torch.randperm(self.input_dim)[:extra]\n",
    "        \n",
    "        self.fused_pair_bilinear = Fused2x2BiLinear(2**self.num_layers, grid_width=grid_width, )#num_layers=5)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x should have dimension -> bs, M\n",
    "        '''\n",
    "        x = torch.cat((x, x[:, self.selector]), dim=1)\n",
    "        return self.fused_pair_bilinear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasLayer(nn.Module):\n",
    "    def __init__(self, dim, init_val=0):\n",
    "        super().__init__()\n",
    "        self.bias = nn.Parameter(torch.ones(dim)*init_val)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x+self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        H = 512\n",
    "        self.bias = nn.Linear(784, H)\n",
    "        self.bn1 = nn.BatchNorm1d(H)\n",
    "        self.fc = nn.Linear(H, 10)\n",
    "        self.la1 = FusedPairBilinearBlock(H, grid_width=3)\n",
    "#         self.la1 = PairBilinearBlock_2(H, grid_width=3)\n",
    "\n",
    "#         self.bias = BiasLayer(784)\n",
    "#         self.la1 = FusedPairBilinearBlock(784, grid_width=3)\n",
    "# #         self.la1 = PairBilinearBlock(784, grid_width=3)\n",
    "#         self.bn1 = nn.BatchNorm1d(1024)\n",
    "#         self.fc = nn.Linear(1024, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bias(x)\n",
    "        x = self.la1(x)\n",
    "#         x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 256, 2, 3, 3])\n",
      "torch.Size([9, 256, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FactorNet2(\n",
       "  (bias): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (la1): FusedPairBilinearBlock(\n",
       "    (fused_pair_bilinear): Fused2x2BiLinear()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = FactorNet2().to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 300/300 [00:02<00:00, 126.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0,  Loss:0.9972912669181824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 50/50 [00:00<00:00, 105.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:63.73%, Test Acc:72.11%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████████████▎     | 266/300 [00:02<00:00, 132.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276,Yout; NAN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|██████████████████████████████████████████████    | 276/300 [00:02<00:00, 124.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1,  Loss:58913986707456.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 50/50 [00:00<00:00, 108.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:76.44%, Test Acc:62.26%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                             | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, NAN Grid back grad; loss 6.467122680123337e+30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                             | 0/300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [89]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, NAN Weight update; loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mfloat\u001b[39m(loss)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m train_accs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_acc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mtrain_count\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     56\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     57\u001b[0m train_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train_acc = 0\n",
    "    train_count = 0\n",
    "    i = -1\n",
    "    for xx, yy in tqdm(train_loader):\n",
    "        i += 1 \n",
    "        xx = xx.view(xx.shape[0], -1)\n",
    "        xx, yy = xx.to(device), yy.to(device)\n",
    "\n",
    "        yout = model(xx)\n",
    "        \n",
    "        if torch.any(torch.isnan(yout.data)):\n",
    "            print(f\"{i},Yout; NAN\", flush=True)\n",
    "            break\n",
    "            \n",
    "        loss = criterion(yout, yy)\n",
    "        \n",
    "        if torch.any(torch.isnan(loss.data)):\n",
    "            print(f\"{i},loss; NAN\", flush=True)\n",
    "            break\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if torch.any(torch.isnan(model.la1.fused_pair_bilinear.Y.grad)):\n",
    "            print(f\"{i}, NAN Grid back grad; loss {float(loss)}\", flush=True)\n",
    "            break\n",
    "        if torch.any(torch.isnan(model.la1.fused_pair_bilinear.pairW.grad)):\n",
    "            print(f\"{i}, NAN Weight back grad; loss {float(loss)}\", flush=True)\n",
    "            break\n",
    "\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(float(loss))\n",
    "\n",
    "        outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "        correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "        train_acc += correct\n",
    "        train_count += len(outputs)\n",
    "        \n",
    "        if torch.any(torch.isnan(model.la1.fused_pair_bilinear.Y)):\n",
    "            print(f\"{i}, NAN Grid update; loss {float(loss)}\", flush=True)\n",
    "            break\n",
    "        if torch.any(torch.isnan(model.la1.fused_pair_bilinear.pairW)):\n",
    "            print(f\"{i}, NAN Weight update; loss {float(loss)}\", flush=True)\n",
    "            break\n",
    "\n",
    "    train_accs.append(float(train_acc)/train_count*100)\n",
    "    train_acc = 0\n",
    "    train_count = 0\n",
    "\n",
    "    print(f'Epoch: {epoch},  Loss:{float(loss)}')\n",
    "    test_count = 0\n",
    "    test_acc = 0\n",
    "    for xx, yy in tqdm(test_loader):\n",
    "        xx = xx.view(xx.shape[0], -1)\n",
    "        xx, yy = xx.to(device), yy.to(device)\n",
    "        with torch.no_grad():\n",
    "            yout = model(xx)\n",
    "        outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "        correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "        test_acc += correct\n",
    "        test_count += len(xx)\n",
    "    test_accs.append(float(test_acc)/test_count*100)\n",
    "    print(f'Train Acc:{train_accs[-1]:.2f}%, Test Acc:{test_accs[-1]:.2f}%')\n",
    "    print()\n",
    "\n",
    "### after each class index is finished training\n",
    "print(f'\\t-> Train Acc {max(train_accs)} ; Test Acc {max(test_accs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0163, device='cuda:0')"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.la1.fused_pair_bilinear.Y.data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:0')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.any(torch.isnan(model.la1.fused_pair_bilinear.Y.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "yout = model(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:0')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.any(torch.isnan(yout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 1, 4, 5, 8, 3, 5, 7, 6, 4, 5, 9, 0, 0, 6, 9, 4, 1, 7, 0, 4, 1, 6, 8,\n",
       "        6, 6, 5, 0, 2, 1, 8, 1, 9, 6, 5, 6, 6, 4, 0, 8, 6, 3, 4, 2, 4, 0, 7, 4,\n",
       "        8, 9, 7, 9, 1, 1, 5, 7, 7, 3, 7, 1, 8, 4, 0, 2, 7, 3, 5, 4, 4, 9, 9, 8,\n",
       "        4, 1, 9, 0, 7, 9, 0, 3, 6, 0, 2, 5, 0, 9, 5, 9, 0, 9, 1, 3, 4, 5, 4, 9,\n",
       "        4, 3, 0, 5, 8, 2, 6, 2, 5, 4, 5, 7, 7, 4, 1, 8, 0, 8, 8, 9, 3, 1, 2, 7,\n",
       "        7, 3, 1, 4, 4, 0, 6, 3, 8, 4, 5, 4, 0, 3, 2, 0, 5, 3, 4, 0, 3, 5, 5, 8,\n",
       "        9, 1, 3, 0, 1, 2, 2, 2, 6, 7, 3, 0, 7, 6, 8, 7, 1, 2, 1, 2, 4, 4, 5, 9,\n",
       "        0, 0, 8, 8, 9, 3, 6, 5, 3, 4, 8, 3, 9, 8, 0, 9, 3, 4, 5, 5, 1, 5, 9, 7,\n",
       "        7, 7, 6, 8, 3, 5, 6, 6], device='cuda:0')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(yout, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(134, device='cuda:0')"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yout.argmax()//10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1605e+29, -7.1041e+28, -1.0751e+29,  9.6640e+27, -1.3566e+29,\n",
       "         1.1653e+29,  1.0879e+29,  1.7212e+29, -1.9855e+28,  1.7179e+29],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yout[134]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1605e+29, -7.1041e+28, -1.0751e+29,  9.6640e+27, -1.3566e+29,\n",
       "         1.1653e+29,  1.0879e+29,  1.7212e+29, -1.9855e+28,  1.7179e+29],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yout[134]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[ 3.8739e-03, -7.1850e-03, -9.2713e-03],\n",
       "           [ 5.0880e-01,  4.9659e-01,  5.0152e-01],\n",
       "           [ 1.0121e+00,  1.0031e+00,  1.0011e+00]],\n",
       "\n",
       "          [[-2.2405e-03,  5.0718e-01,  1.0115e+00],\n",
       "           [-5.2883e-03,  4.9287e-01,  9.8861e-01],\n",
       "           [-8.6327e-03,  4.9361e-01,  9.9927e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 2.1228e-03, -7.1292e-04, -8.8196e-03],\n",
       "           [ 5.0610e-01,  5.0579e-01,  4.9202e-01],\n",
       "           [ 1.0066e+00,  1.0050e+00,  9.9415e-01]],\n",
       "\n",
       "          [[-1.5808e-03,  5.0555e-01,  1.0099e+00],\n",
       "           [-4.6353e-03,  5.0876e-01,  1.0075e+00],\n",
       "           [-4.6236e-03,  5.0164e-01,  1.0042e+00]]],\n",
       "\n",
       "\n",
       "         [[[-4.0497e-03,  3.1962e-03, -1.1414e-02],\n",
       "           [ 5.0873e-01,  4.9291e-01,  5.1071e-01],\n",
       "           [ 1.0103e+00,  9.9000e-01,  1.0000e+00]],\n",
       "\n",
       "          [[-1.8174e-03,  5.0718e-01,  1.0113e+00],\n",
       "           [-1.7545e-03,  4.9468e-01,  9.8872e-01],\n",
       "           [-9.1505e-03,  5.0697e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[ 5.7747e-03,  1.6783e-03,  6.3968e-04],\n",
       "           [ 5.0821e-01,  5.0801e-01,  5.0089e-01],\n",
       "           [ 1.0078e+00,  1.0080e+00,  1.0002e+00]],\n",
       "\n",
       "          [[ 1.2220e-03,  5.0750e-01,  1.0042e+00],\n",
       "           [ 7.9333e-04,  5.0590e-01,  1.0049e+00],\n",
       "           [-1.0170e-02,  4.8871e-01,  1.0021e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 3.7518e-03,  1.9155e-03, -1.5652e-03],\n",
       "           [ 5.0493e-01,  5.0033e-01,  4.9700e-01],\n",
       "           [ 1.0032e+00,  1.0016e+00,  1.0031e+00]],\n",
       "\n",
       "          [[-7.0945e-03,  5.0780e-01,  1.0072e+00],\n",
       "           [-4.1136e-03,  5.0432e-01,  1.0090e+00],\n",
       "           [-4.1467e-03,  4.9808e-01,  1.0054e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 8.6279e-03, -5.7989e-03, -3.5809e-03],\n",
       "           [ 5.0906e-01,  5.0636e-01,  5.0715e-01],\n",
       "           [ 1.0015e+00,  1.0079e+00,  1.0100e+00]],\n",
       "\n",
       "          [[-7.2307e-03,  4.9972e-01,  1.0053e+00],\n",
       "           [ 2.1584e-03,  5.0652e-01,  1.0083e+00],\n",
       "           [-2.4006e-03,  5.0859e-01,  1.0116e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[-2.7277e-03,  3.2876e-03, -5.3106e-04],\n",
       "           [ 5.0623e-01,  5.0842e-01,  5.0946e-01],\n",
       "           [ 1.0055e+00,  1.0117e+00,  1.0108e+00]],\n",
       "\n",
       "          [[-7.0886e-04,  5.0482e-01,  1.0062e+00],\n",
       "           [ 4.2480e-03,  5.0891e-01,  1.0059e+00],\n",
       "           [ 1.0948e-03,  5.0911e-01,  1.0005e+00]]],\n",
       "\n",
       "\n",
       "         [[[-1.9261e-03, -7.1370e-03, -1.0766e-02],\n",
       "           [ 5.0466e-01,  5.0520e-01,  4.9725e-01],\n",
       "           [ 1.0094e+00,  1.0095e+00,  1.0073e+00]],\n",
       "\n",
       "          [[-3.7561e-03,  5.0887e-01,  1.0118e+00],\n",
       "           [ 2.6261e-03,  5.0588e-01,  1.0105e+00],\n",
       "           [-9.6051e-03,  4.8931e-01,  1.0010e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 1.2857e-03, -7.5019e-03, -6.6959e-03],\n",
       "           [ 5.0911e-01,  4.9171e-01,  5.0973e-01],\n",
       "           [ 1.0102e+00,  9.9272e-01,  1.0048e+00]],\n",
       "\n",
       "          [[-2.6751e-03,  5.0491e-01,  1.0115e+00],\n",
       "           [-5.8509e-03,  4.9645e-01,  9.8829e-01],\n",
       "           [ 2.4669e-03,  5.0976e-01,  1.0049e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[-6.0188e-03, -6.3389e-03, -1.7730e-03],\n",
       "           [ 5.0339e-01,  4.9746e-01,  4.9858e-01],\n",
       "           [ 1.0098e+00,  1.0054e+00,  1.0013e+00]],\n",
       "\n",
       "          [[ 5.0044e-03,  5.0903e-01,  1.0037e+00],\n",
       "           [-6.2238e-03,  5.0719e-01,  1.0052e+00],\n",
       "           [ 5.9113e-03,  5.0953e-01,  1.0054e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 1.9923e-03,  1.6868e-03,  4.8960e-03],\n",
       "           [ 5.0086e-01,  5.0327e-01,  5.0301e-01],\n",
       "           [ 1.0008e+00,  1.0061e+00,  1.0012e+00]],\n",
       "\n",
       "          [[ 5.6634e-03,  5.0945e-01,  1.0112e+00],\n",
       "           [-5.8359e-04,  5.0387e-01,  1.0063e+00],\n",
       "           [-4.1251e-03,  4.9557e-01,  9.9933e-01]]],\n",
       "\n",
       "\n",
       "         [[[-6.1368e-03, -5.8157e-03,  5.4839e-04],\n",
       "           [ 5.0413e-01,  5.0546e-01,  5.0684e-01],\n",
       "           [ 1.0071e+00,  1.0095e+00,  1.0096e+00]],\n",
       "\n",
       "          [[-7.8390e-03,  5.0264e-01,  1.0068e+00],\n",
       "           [-5.9887e-03,  5.0770e-01,  1.0106e+00],\n",
       "           [-4.2521e-03,  5.0484e-01,  1.0074e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 2.7471e-03, -5.8900e-03, -3.3923e-03],\n",
       "           [ 5.0811e-01,  4.9171e-01,  5.0208e-01],\n",
       "           [ 1.0120e+00,  9.8809e-01,  1.0000e+00]],\n",
       "\n",
       "          [[-4.4148e-03,  5.0789e-01,  1.0105e+00],\n",
       "           [ 3.8651e-03,  4.9827e-01,  9.8982e-01],\n",
       "           [-8.2856e-03,  5.0855e-01,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "         [[[-4.7712e-03, -7.8765e-04, -8.5125e-03],\n",
       "           [ 5.0639e-01,  4.9365e-01,  4.9149e-01],\n",
       "           [ 1.0115e+00,  9.8846e-01,  9.9957e-01]],\n",
       "\n",
       "          [[-1.1042e-03,  5.0755e-01,  1.0111e+00],\n",
       "           [-6.7578e-03,  5.0632e-01,  1.0100e+00],\n",
       "           [-1.2818e-02,  5.1352e-01,  1.0009e+00]]],\n",
       "\n",
       "\n",
       "         [[[-4.2210e-03,  7.1580e-03, -6.6906e-03],\n",
       "           [ 5.0080e-01,  5.0943e-01,  5.0318e-01],\n",
       "           [ 1.0065e+00,  1.0065e+00,  9.9974e-01]],\n",
       "\n",
       "          [[-3.9952e-03,  5.0538e-01,  1.0084e+00],\n",
       "           [-4.2115e-03,  5.0552e-01,  1.0119e+00],\n",
       "           [-1.7008e-03,  5.0063e-01,  1.0112e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[-7.0055e-03, -2.2304e-03, -5.7440e-03],\n",
       "           [ 4.9506e-01,  5.0709e-01,  5.0152e-01],\n",
       "           [ 1.0097e+00,  1.0068e+00,  1.0105e+00]],\n",
       "\n",
       "          [[-6.0680e-03,  5.0569e-01,  1.0090e+00],\n",
       "           [-4.6240e-03,  5.0574e-01,  1.0094e+00],\n",
       "           [-9.0193e-03,  4.9196e-01,  9.9279e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 6.8388e-03,  5.6375e-03, -5.9970e-03],\n",
       "           [ 5.0997e-01,  5.1092e-01,  4.9826e-01],\n",
       "           [ 1.0094e+00,  1.0100e+00,  1.0008e+00]],\n",
       "\n",
       "          [[ 1.9762e-03,  5.0716e-01,  1.0096e+00],\n",
       "           [ 3.2516e-03,  4.9863e-01,  9.9458e-01],\n",
       "           [-1.0265e-02,  4.8963e-01,  9.9697e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 6.9305e-03, -7.0140e-03,  3.7667e-03],\n",
       "           [ 5.1045e-01,  5.0396e-01,  5.0955e-01],\n",
       "           [ 1.0062e+00,  1.0025e+00,  1.0078e+00]],\n",
       "\n",
       "          [[-5.7817e-03,  5.0722e-01,  1.0087e+00],\n",
       "           [-9.2615e-03,  5.0611e-01,  1.0110e+00],\n",
       "           [-5.8023e-03,  5.0101e-01,  1.0057e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 4.8295e-03, -6.7547e-03, -6.5866e-03],\n",
       "           [ 5.0847e-01,  5.0108e-01,  4.9489e-01],\n",
       "           [ 1.0120e+00,  1.0111e+00,  9.9990e-01]],\n",
       "\n",
       "          [[-2.3302e-03,  5.0487e-01,  1.0099e+00],\n",
       "           [-5.2279e-03,  4.9312e-01,  9.8959e-01],\n",
       "           [ 3.0743e-03,  5.0250e-01,  9.9460e-01]]],\n",
       "\n",
       "\n",
       "         [[[-5.6923e-03,  3.6879e-03, -9.5689e-03],\n",
       "           [ 5.0381e-01,  5.0322e-01,  4.8996e-01],\n",
       "           [ 1.0111e+00,  9.9452e-01,  9.9810e-01]],\n",
       "\n",
       "          [[ 1.5245e-03,  5.0892e-01,  1.0122e+00],\n",
       "           [-3.6260e-03,  4.9916e-01,  1.0063e+00],\n",
       "           [ 4.0377e-03,  5.0554e-01,  1.0019e+00]]],\n",
       "\n",
       "\n",
       "         [[[-3.6029e-03,  5.6698e-03, -7.3938e-03],\n",
       "           [ 5.0464e-01,  5.0960e-01,  4.9766e-01],\n",
       "           [ 1.0076e+00,  1.0061e+00,  9.9993e-01]],\n",
       "\n",
       "          [[ 1.7794e-03,  5.0789e-01,  1.0083e+00],\n",
       "           [-1.8107e-03,  5.0518e-01,  9.9698e-01],\n",
       "           [-4.5790e-03,  4.9639e-01,  9.9890e-01]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[-3.6970e-03,  2.4594e-03, -1.1298e-03],\n",
       "           [ 5.0824e-01,  5.0851e-01,  4.9744e-01],\n",
       "           [ 1.0121e+00,  1.0117e+00,  1.0087e+00]],\n",
       "\n",
       "          [[-5.1148e-03,  5.0403e-01,  1.0050e+00],\n",
       "           [-3.5023e-03,  5.0383e-01,  1.0055e+00],\n",
       "           [-1.0307e-02,  4.9082e-01,  9.9737e-01]]],\n",
       "\n",
       "\n",
       "         [[[-7.5901e-03, -1.3009e-03,  3.4449e-04],\n",
       "           [ 4.9621e-01,  5.0541e-01,  5.0792e-01],\n",
       "           [ 1.0064e+00,  1.0077e+00,  1.0098e+00]],\n",
       "\n",
       "          [[ 8.5764e-03,  5.1062e-01,  1.0051e+00],\n",
       "           [-2.5973e-03,  5.0658e-01,  1.0089e+00],\n",
       "           [-7.5496e-04,  5.0589e-01,  1.0106e+00]]],\n",
       "\n",
       "\n",
       "         [[[-5.0072e-03, -6.8965e-03,  4.0084e-03],\n",
       "           [ 5.0433e-01,  5.0178e-01,  5.0924e-01],\n",
       "           [ 1.0109e+00,  1.0112e+00,  1.0013e+00]],\n",
       "\n",
       "          [[-7.1589e-03,  5.0389e-01,  1.0067e+00],\n",
       "           [-2.0485e-03,  5.0750e-01,  1.0110e+00],\n",
       "           [-5.8941e-03,  4.9186e-01,  1.0065e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 4.2379e-03, -5.2726e-03, -9.4778e-03],\n",
       "           [ 5.0798e-01,  4.9883e-01,  4.9221e-01],\n",
       "           [ 1.0113e+00,  1.0096e+00,  1.0090e+00]],\n",
       "\n",
       "          [[-6.3790e-03,  5.0340e-01,  1.0123e+00],\n",
       "           [ 2.2968e-03,  5.0884e-01,  9.9951e-01],\n",
       "           [ 6.0216e-03,  5.1137e-01,  1.0096e+00]]],\n",
       "\n",
       "\n",
       "         [[[-2.2283e-03, -7.1779e-03, -1.0136e-02],\n",
       "           [ 5.0612e-01,  4.9728e-01,  4.9137e-01],\n",
       "           [ 1.0105e+00,  1.0091e+00,  9.9999e-01]],\n",
       "\n",
       "          [[ 6.5463e-03,  5.1033e-01,  1.0112e+00],\n",
       "           [-1.4670e-03,  5.0934e-01,  1.0100e+00],\n",
       "           [-6.3131e-03,  5.0300e-01,  1.0038e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 2.2493e-03, -2.3190e-03, -9.3639e-03],\n",
       "           [ 5.0810e-01,  5.0309e-01,  4.9134e-01],\n",
       "           [ 1.0065e+00,  1.0054e+00,  1.0022e+00]],\n",
       "\n",
       "          [[-4.7918e-03,  5.0539e-01,  1.0099e+00],\n",
       "           [-1.3765e-03,  5.0777e-01,  1.0105e+00],\n",
       "           [-1.0408e-03,  5.0311e-01,  1.0031e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[-7.6581e-04,  3.4900e-03, -4.8500e-03],\n",
       "           [ 5.1101e-01,  5.1008e-01,  5.0929e-01],\n",
       "           [ 1.0018e+00,  1.0022e+00,  1.0022e+00]],\n",
       "\n",
       "          [[-6.9503e-03,  5.0558e-01,  1.0099e+00],\n",
       "           [ 9.0831e-03,  4.9606e-01,  9.9004e-01],\n",
       "           [ 3.4717e-04,  4.9976e-01,  9.9865e-01]]],\n",
       "\n",
       "\n",
       "         [[[-7.9344e-03, -8.9175e-03, -1.0694e-02],\n",
       "           [ 4.9960e-01,  4.9071e-01,  4.8955e-01],\n",
       "           [ 1.0116e+00,  1.0110e+00,  9.9868e-01]],\n",
       "\n",
       "          [[ 8.1760e-03,  5.0978e-01,  1.0105e+00],\n",
       "           [ 5.4131e-03,  5.0970e-01,  1.0097e+00],\n",
       "           [-1.1124e-02,  4.8938e-01,  1.0048e+00]]],\n",
       "\n",
       "\n",
       "         [[[-8.1870e-03, -6.3868e-03, -7.8573e-03],\n",
       "           [ 5.0661e-01,  4.9947e-01,  4.8831e-01],\n",
       "           [ 1.0125e+00,  1.0122e+00,  9.9356e-01]],\n",
       "\n",
       "          [[-2.9084e-03,  5.0566e-01,  1.0081e+00],\n",
       "           [-4.5987e-03,  5.0522e-01,  1.0101e+00],\n",
       "           [-1.0092e-02,  5.0844e-01,  1.0107e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 1.0860e-03, -3.4550e-04,  5.3849e-04],\n",
       "           [ 5.0761e-01,  5.0724e-01,  5.0399e-01],\n",
       "           [ 1.0102e+00,  1.0119e+00,  1.0108e+00]],\n",
       "\n",
       "          [[-3.8183e-03,  5.0279e-01,  1.0023e+00],\n",
       "           [-4.8226e-03,  5.0183e-01,  1.0024e+00],\n",
       "           [-1.6640e-03,  5.1173e-01,  1.0112e+00]]],\n",
       "\n",
       "\n",
       "         [[[-4.0612e-03, -1.7095e-03, -1.0858e-02],\n",
       "           [ 5.0536e-01,  4.9769e-01,  4.8987e-01],\n",
       "           [ 1.0114e+00,  9.9063e-01,  1.0010e+00]],\n",
       "\n",
       "          [[ 2.8383e-03,  5.0807e-01,  1.0117e+00],\n",
       "           [-7.8657e-03,  4.9372e-01,  9.9500e-01],\n",
       "           [-8.6105e-03,  4.9129e-01,  9.9841e-01]]],\n",
       "\n",
       "\n",
       "         [[[-2.2867e-03,  5.0478e-03, -1.6746e-03],\n",
       "           [ 5.0655e-01,  5.0778e-01,  4.9687e-01],\n",
       "           [ 1.0076e+00,  1.0039e+00,  1.0022e+00]],\n",
       "\n",
       "          [[-2.3439e-03,  5.0042e-01,  1.0010e+00],\n",
       "           [-1.0247e-03,  5.0494e-01,  1.0030e+00],\n",
       "           [ 4.4360e-03,  5.0679e-01,  1.0038e+00]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[-6.6373e-03, -7.5709e-03, -1.7476e-03],\n",
       "           [ 5.0778e-01,  5.0579e-01,  5.0887e-01],\n",
       "           [ 1.0119e+00,  1.0112e+00,  1.0067e+00]],\n",
       "\n",
       "          [[-2.2274e-03,  5.0613e-01,  1.0093e+00],\n",
       "           [-7.5078e-03,  4.9761e-01,  1.0091e+00],\n",
       "           [-1.1068e-02,  4.9402e-01,  1.0017e+00]]],\n",
       "\n",
       "\n",
       "         [[[ 7.3583e-04,  1.4751e-03,  8.9991e-03],\n",
       "           [ 5.0790e-01,  5.0836e-01,  5.1128e-01],\n",
       "           [ 1.0119e+00,  1.0097e+00,  1.0031e+00]],\n",
       "\n",
       "          [[ 9.0251e-04,  5.0726e-01,  1.0084e+00],\n",
       "           [ 9.7070e-03,  5.0988e-01,  1.0118e+00],\n",
       "           [-1.1096e-02,  4.9021e-01,  1.0050e+00]]],\n",
       "\n",
       "\n",
       "         [[[-5.0096e-03, -1.3796e-03,  4.9093e-03],\n",
       "           [ 5.0410e-01,  5.0768e-01,  5.0991e-01],\n",
       "           [ 1.0102e+00,  1.0100e+00,  1.0105e+00]],\n",
       "\n",
       "          [[-6.2256e-03,  5.0347e-01,  1.0049e+00],\n",
       "           [-3.6623e-03,  5.0494e-01,  1.0081e+00],\n",
       "           [ 4.0655e-04,  5.0386e-01,  1.0090e+00]]]]], device='cuda:0')"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.la1.fused_pair_bilinear.Y.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
