{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6287145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os, sys, pathlib, random, time, pickle, copy, json\n",
    "# from tqdm.autonotebook import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0173cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcaa3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_lib import TransformerBlock, \\\n",
    "        Mixer_TransformerBlock_Encoder, \\\n",
    "        PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69ae2ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyImageNet_Preload(data.Dataset):\n",
    "#     https://gist.github.com/z-a-f/b862013c0dc2b540cf96a123a6766e54\n",
    "    \n",
    "    def __init__(self, root, mode='train', transform=None, preload=False):\n",
    "        super().__init__()\n",
    "        self.preload = preload\n",
    "        dataset = datasets.ImageFolder(\n",
    "            root=os.path.join(root, mode),\n",
    "            transform=None\n",
    "        )\n",
    "        self.transform = transform\n",
    "        self.images, self.labels = [], []\n",
    "        print(\"Dataset Size:\",len(dataset))\n",
    "        \n",
    "        if preload:\n",
    "            for i in tqdm(range(len(dataset))):\n",
    "                x, y = dataset[i]\n",
    "                self.images.append(x)\n",
    "                self.labels.append(y)\n",
    "                \n",
    "#         del dataset\n",
    "        self.dataset = dataset\n",
    "            \n",
    "    def _add_channels(img, total_channels=3):\n",
    "        while len(img.shape) < 3:  # third axis is the channels\n",
    "            img = np.expand_dims(img, axis=-1)\n",
    "        while(img.shape[-1]) < 3:\n",
    "            img = np.concatenate([img, img[:, :, -1:]], axis=-1)\n",
    "        return img\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.preload:\n",
    "            img, lbl = self.images[idx], self.labels[idx]\n",
    "        else:\n",
    "            img, lbl = self.dataset[idx]\n",
    "        return self.transform(img), lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2e11b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b45f6816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixer_ViT_Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_dim:tuple, patch_size:tuple, hidden_channel:int, num_blocks:int, num_classes:int, block_seq_size:int, block_mlp_size:int, forward_expansion:float=2.0, pos_emb=True, dropout:float=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_dim = image_dim ### must contain (C, H, W) or (H, W)\n",
    "        \n",
    "        ### find patch dim\n",
    "        d0 = int(image_dim[-2]/patch_size[0])\n",
    "        d1 = int(image_dim[-1]/patch_size[1])\n",
    "        assert d0*patch_size[0]==image_dim[-2], \"Image must be divisible into patch size\"\n",
    "        assert d1*patch_size[1]==image_dim[-1], \"Image must be divisible into patch size\"\n",
    "        \n",
    "#         self.d0, self.d1 = d0, d1 ### number of patches in each axis\n",
    "        __patch_size = patch_size[0]*patch_size[1]*image_dim[0] ## number of channels in each patch\n",
    "    \n",
    "        ### find channel dim\n",
    "        channel_size = d0*d1 ## number of patches\n",
    "        \n",
    "        ### after the number of channels are changed\n",
    "        init_dim = __patch_size\n",
    "        final_dim = hidden_channel\n",
    "        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        #### rescale the patches (patch wise image non preserving transform, unlike bilinear interpolation)\n",
    "        self.channel_change = nn.Linear(init_dim, final_dim)\n",
    "        print(f\"ViT Mixer : Channels per patch -> Initial:{init_dim} Final:{final_dim}\")\n",
    "        \n",
    "        \n",
    "        self.channel_dim = final_dim\n",
    "        self.patch_dim = channel_size\n",
    "        \n",
    "        self.transformer_blocks = []\n",
    "        \n",
    "        f = self.get_factors(self.channel_dim)\n",
    "        print(f)\n",
    "        fi = np.abs(np.array(f) - np.sqrt(self.channel_dim)).argmin()\n",
    "        \n",
    "        _n_heads = f[fi]\n",
    "        \n",
    "        ## number of dims per channel -> channel_dim\n",
    "#         print('Num patches:', self.patch_dim)\n",
    "        print(f'Sequence len: {self.patch_dim} ; Block size: {block_seq_size}')\n",
    "        print('Channel dim:', self.channel_dim, 'num heads:',_n_heads)\n",
    "        \n",
    "        if block_seq_size is None or block_seq_size<2:\n",
    "            ### Find the block size for sequence:\n",
    "            block_seq_size = int(2**np.ceil(np.log2(np.sqrt(self.patch_dim))))\n",
    "            \n",
    "        print(f'MLP dim: {self.channel_dim} ; Block size: {block_mlp_size}')\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            L = Mixer_TransformerBlock_Encoder(self.patch_dim, block_seq_size, self.channel_dim, _n_heads, dropout, forward_expansion, nn.GELU, block_mlp_size)\n",
    "            self.transformer_blocks.append(L)\n",
    "        self.transformer_blocks = nn.Sequential(*self.transformer_blocks)\n",
    "        \n",
    "        self.linear = nn.Linear(self.patch_dim*self.channel_dim, num_classes)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(self.channel_dim, dropout=0)\n",
    "        if not pos_emb:\n",
    "            self.positional_encoding = nn.Identity()\n",
    "        \n",
    "        \n",
    "    def get_factors(self, n):\n",
    "        facts = []\n",
    "        for i in range(2, n+1):\n",
    "            if n%i == 0:\n",
    "                facts.append(i)\n",
    "        return facts\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.unfold(x).swapaxes(-1, -2)\n",
    "        x = self.channel_change(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.linear(x.view(bs, -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d8d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_skip(model_name, ep):\n",
    "    \n",
    "    filename = f\"./output/benchmark/{model_name}_data.json\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename) as f:\n",
    "            data = json.load(f)\n",
    "        ## data consists of lists and dicts.\n",
    "        epochs = data['train_stat'][-1][0]\n",
    "        if epochs == ep-1:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b05ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(dataset:str, patch_size:int, num_layers:int, SEED:int, sparse_att:bool=False, sparse_mlp:bool=False, pos_emb:bool=False, cuda:int=0):\n",
    "    device = torch.device(f\"cuda:{cuda}\")\n",
    "    \n",
    "    if sparse_att:\n",
    "        assert num_layers%2 == 0, 'number of blocks on sparse transformer is (x2)/2 hence it must be even'\n",
    "        num_layers_ = num_layers//2\n",
    "    else:\n",
    "        num_layers_ = num_layers\n",
    "    \n",
    "    BS = 256\n",
    "    NC = -1\n",
    "    EPOCHS = 300\n",
    "    imsize = (3, 32, 32)\n",
    "    expansion_dict = {16:1024, 8:256, 4:128, 2:64, 1:64}\n",
    "    expansion = expansion_dict[patch_size]\n",
    "\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    ##### Data Transforms\n",
    "    if dataset == 'tiny':\n",
    "        NC = 200\n",
    "        EPOCHS = 400\n",
    "        imsize = (3, 64, 64)\n",
    "        tiny_train = transforms.Compose([\n",
    "        transforms.RandAugment(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.5]*3,\n",
    "            std=[0.2]*3,\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        tiny_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.5]*3,\n",
    "                std=[0.2]*3,\n",
    "            ),\n",
    "        ])\n",
    "        \n",
    "        train_dataset = TinyImageNet_Preload(root=\"../../../../../_Datasets/tiny-imagenet-200\",\n",
    "                                     mode='train', transform=tiny_train)\n",
    "        test_dataset = TinyImageNet_Preload(root=\"../../../../../_Datasets/tiny-imagenet-200\",\n",
    "                                     mode='val', transform=tiny_test)\n",
    "        \n",
    "    elif dataset == 'cifar10':\n",
    "        NC = 10\n",
    "        BS = 128\n",
    "        cifar_train = transforms.Compose([\n",
    "            transforms.RandomCrop(size=32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "                std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        cifar_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "                std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        train_dataset = datasets.CIFAR10(root=\"../../../../../_Datasets/cifar10/\", train=True, download=True, transform=cifar_train)\n",
    "        test_dataset = datasets.CIFAR10(root=\"../../../../../_Datasets/cifar10/\", train=False, download=True, transform=cifar_test)\n",
    "\n",
    "    elif dataset == 'cifar100':\n",
    "        NC = 100\n",
    "        BS = 128\n",
    "        cifar_train = transforms.Compose([\n",
    "            transforms.RandomCrop(size=32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.5071, 0.4865, 0.4409],\n",
    "                std=[0.2009, 0.1984, 0.2023],\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        cifar_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.5071, 0.4865, 0.4409],\n",
    "                std=[0.2009, 0.1984, 0.2023],\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        train_dataset = datasets.CIFAR100(root=\"../../../../../_Datasets/cifar100/\", train=True, download=True, transform=cifar_train)\n",
    "        test_dataset = datasets.CIFAR100(root=\"../../../../../_Datasets/cifar100/\", train=False, download=True, transform=cifar_test)\n",
    "        \n",
    "    ##### Now create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BS, shuffle=True, num_workers=4)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BS, shuffle=False, num_workers=4)\n",
    "    \n",
    "    ### Now create models\n",
    "    \n",
    "    seq_len = (imsize[-1]*imsize[-2])//(patch_size*patch_size)\n",
    "    mlp_dim = expansion\n",
    "    print(seq_len, mlp_dim)\n",
    "    \n",
    "    if sparse_att:\n",
    "        seq_len = int(2**np.ceil(np.log2(np.sqrt(seq_len))))\n",
    "    if sparse_mlp:\n",
    "        mlp_dim = int(2**np.ceil(np.log2(np.sqrt(expansion))))\n",
    "    \n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    model = Mixer_ViT_Classifier(imsize, \n",
    "                                 patch_size=[patch_size]*2, \n",
    "                                 hidden_channel=expansion, \n",
    "                                 num_blocks=num_layers_, \n",
    "                                 num_classes=NC, \n",
    "                                 block_seq_size=seq_len, \n",
    "                                 block_mlp_size=mlp_dim,\n",
    "                                 pos_emb=pos_emb).to(device)\n",
    "    \n",
    "#     _x = torch.randn(BS, *imsize).to(device)\n",
    "#     print(\"Output: \",vit_mixer(_x).shape)\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"number of params: \", num_params)\n",
    "    \n",
    "    _a, _b, _c = 'att', 'mlp', 'nPE'\n",
    "    if sparse_att: _a = 'sAtt'\n",
    "    if sparse_mlp: _b = 'sMlp'\n",
    "    if pos_emb: _c = 'PE'\n",
    "    model_name = f'01.3_ViT_{_c}_{dataset}_patch{patch_size}_l{num_layers}_{_a}_{_b}_s{SEED}'\n",
    "    print(f\"Model Name: {model_name}\")\n",
    "    \n",
    "    if experiment_skip(model_name, EPOCHS):\n",
    "        print(f'EXPERIMENT DONE... SKIPPING : {model_name}')\n",
    "        return\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    STAT ={'train_stat':[], 'test_stat':[], 'params':num_params, }\n",
    "\n",
    "    ## Following is copied from \n",
    "    ### https://github.com/kuangliu/pytorch-cifar/blob/master/main.py\n",
    "\n",
    "    # Training\n",
    "    def train(epoch):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "#             break\n",
    "\n",
    "        STAT['train_stat'].append((epoch, train_loss/(batch_idx+1), 100.*correct/total)) ### (Epochs, Loss, Acc)\n",
    "        print(f\"[Train] {epoch} Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "        return\n",
    "\n",
    "    global best_acc\n",
    "    best_acc = -1\n",
    "    def test(epoch):\n",
    "        global best_acc\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        time_taken = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader)):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                start = time.time()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                start = time.time()-start\n",
    "                time_taken.append(start)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        STAT['test_stat'].append((epoch, test_loss/(batch_idx+1), 100.*correct/total, np.mean(time_taken))) ### (Epochs, Loss, Acc, time)\n",
    "        print(f\"[Test] {epoch} Loss: {test_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "\n",
    "        # Save checkpoint.\n",
    "        acc = 100.*correct/total\n",
    "        if acc > best_acc:\n",
    "            print('Saving..')\n",
    "            state = {\n",
    "                'model': model.state_dict(),\n",
    "                'acc': acc,\n",
    "                'epoch': epoch\n",
    "            }\n",
    "            if not os.path.isdir('models'):\n",
    "                os.mkdir('models')\n",
    "            torch.save(state, f'./models/benchmark/{model_name}.pth')\n",
    "            best_acc = acc\n",
    "\n",
    "        with open(f\"./output/benchmark/{model_name}_data.json\", 'w') as f:\n",
    "            json.dump(STAT, f, indent=0)\n",
    "\n",
    "    ### Train the whole damn thing\n",
    "#     EPOCHS = 1\n",
    "    for epoch in range(0, EPOCHS):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "        scheduler.step()\n",
    "        \n",
    "    \n",
    "    train_stat = np.array(STAT['train_stat'])\n",
    "    test_stat = np.array(STAT['test_stat'])\n",
    "\n",
    "    plt.plot(train_stat[:,1], label='train')\n",
    "    plt.plot(test_stat[:,1], label='test')\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"./output/benchmark/plots/{model_name}_loss.svg\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_stat[:,2], label='train')\n",
    "    plt.plot(test_stat[:,2], label='test')\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"./output/benchmark/plots/{model_name}_accs.svg\")\n",
    "    plt.show()\n",
    "    \n",
    "    del model, optimizer\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88a2ebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark(dataset='tiny', \n",
    "#           patch_size=4, num_layers=10, SEED=123, sparse_att=True, sparse_mlp=True, cuda=0\n",
    "#          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "176808bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Automate the benchmark\n",
    "# ###### for tiny\n",
    "# cuda_idx = 0\n",
    "# # for seed in [147, 258, 369]:\n",
    "# for seed in [147]:\n",
    "#     for patch_size in [16, 8, 4]:\n",
    "#         for sparse_attention in [False, True]:\n",
    "#             for sparse_mlp in [False, True]:\n",
    "# #                 for nlayers in [6, 10, 14]:\n",
    "#                 for nlayers in [6]:\n",
    "#                     print(f'''\n",
    "#                         Experimenting on Tiny Dataset \n",
    "#                         patch:{patch_size},\n",
    "#                         sparse_att: {sparse_attention},\n",
    "#                         sparse_mlp: {sparse_mlp},\n",
    "#                         num_layers : {nlayers},\n",
    "#                         seed: {seed}\n",
    "#                     ''')\n",
    "            \n",
    "#                     benchmark(dataset='tiny', \n",
    "#                               patch_size=patch_size, \n",
    "#                               num_layers=nlayers, \n",
    "#                               SEED=seed, \n",
    "#                               sparse_att=sparse_attention, sparse_mlp=sparse_mlp, \n",
    "#                               cuda=cuda_idx\n",
    "#                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5773c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Automate the benchmark\n",
    "# ###### for tiny\n",
    "\n",
    "# not_working = [\n",
    "#     (4, False, True, 6),\n",
    "#     (4, False, False, 10),\n",
    "#     (4, False, False, 14),\n",
    "#     (4, False, True, 10),\n",
    "#     (4, False, True, 14),\n",
    "#     (4, True, True, 14),\n",
    "# ]\n",
    "\n",
    "# cuda_idx = 0\n",
    "# for seed in [147, 258, 369]:\n",
    "# # for seed in [147]:\n",
    "#     for patch_size in [16, 8, 4]:\n",
    "#         for sparse_attention in [False, True]:\n",
    "#             for sparse_mlp in [False, True]:\n",
    "#                 for nlayers in [6, 10, 14]:\n",
    "\n",
    "#                     print(f'''\n",
    "#                         Experimenting on Tiny Dataset \n",
    "#                         patch:{patch_size},\n",
    "#                         sparse_att: {sparse_attention},\n",
    "#                         sparse_mlp: {sparse_mlp},\n",
    "#                         num_layers : {nlayers},\n",
    "#                         seed: {seed}\n",
    "#                     ''')\n",
    "            \n",
    "#                 ### check if config is in not_working case\n",
    "#                     exit = False\n",
    "#                     for nw in not_working:\n",
    "#                         if patch_size==nw[0] and \\\n",
    "#                             sparse_attention==nw[1] and \\\n",
    "#                             sparse_mlp==nw[2] and\\\n",
    "#                             nlayers==nw[3]:\n",
    "                            \n",
    "#                             exit=True\n",
    "#                             break\n",
    "#                     if exit:\n",
    "#                         print(f'Exiting as the config is in NOT WORKING')\n",
    "#                         continue\n",
    "\n",
    "\n",
    "#                     benchmark(dataset='tiny', \n",
    "#                               patch_size=patch_size, \n",
    "#                               num_layers=nlayers, \n",
    "#                               SEED=seed, \n",
    "#                               sparse_att=sparse_attention, sparse_mlp=sparse_mlp, \n",
    "#                               cuda=cuda_idx\n",
    "#                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8645f1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                Experimenting on cifar10 Dataset \n",
      "                                patch:4,\n",
      "                                sparse_att: False,\n",
      "                                sparse_mlp: False,\n",
      "                                num_layers: 4,\n",
      "                                pos_embed: False,\n",
      "                                seed: 147\n",
      "                            \n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "64 128\n",
      "ViT Mixer : Channels per patch -> Initial:48 Final:128\n",
      "[2, 4, 8, 16, 32, 64, 128]\n",
      "Sequence len: 64 ; Block size: 64\n",
      "Channel dim: 128 num heads: 8\n",
      "MLP dim: 128 ; Block size: 128\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 101: invalid device ordinal",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExiting as the config is in NOT WORKING\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m          \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m          \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m          \u001b[49m\u001b[43mSEED\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m          \u001b[49m\u001b[43msparse_att\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_attention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_mlp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_mlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m          \u001b[49m\u001b[43mpos_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcuda_idx\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m         \u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mbenchmark\u001b[0;34m(dataset, patch_size, num_layers, SEED, sparse_att, sparse_mlp, pos_emb, cuda)\u001b[0m\n\u001b[1;32m    108\u001b[0m         mlp_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mceil(np\u001b[38;5;241m.\u001b[39mlog2(np\u001b[38;5;241m.\u001b[39msqrt(expansion))))\n\u001b[1;32m    111\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmanual_seed(SEED)\n\u001b[0;32m--> 112\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mMixer_ViT_Classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mhidden_channel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpansion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mnum_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mblock_seq_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mblock_mlp_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpos_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_emb\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m#     _x = torch.randn(BS, *imsize).to(device)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m#     print(\"Output: \",vit_mixer(_x).shape)\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     num_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:907\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    904\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 907\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 601\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    904\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/cuda/__init__.py:216\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    220\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 101: invalid device ordinal"
     ]
    }
   ],
   "source": [
    "### Automate the benchmark\n",
    "###### for c10 / c100\n",
    "\n",
    "not_working = [\n",
    "    (1, False, False, 4),\n",
    "    (1, False, False, 8),\n",
    "    (1, False, False, 12),\n",
    "    (1, False, True, 4),\n",
    "    (1, False, True, 8),\n",
    "    (1, False, True, 12),\n",
    "\n",
    "    (1, True, False, 12),\n",
    "    (1, True, True, 12),\n",
    "]\n",
    "\n",
    "cuda_idx = 0\n",
    "for dataset in ['cifar10']:\n",
    "    for seed in [147, 258, 369]:\n",
    "        for nlayers in [4, 8, 12]:\n",
    "            for patch_size in [4, 2]:\n",
    "#             for patch_size in [8, 4, 2, 1]:\n",
    "                for sparse_attention in [False, True]:\n",
    "                    for sparse_mlp in [False]:\n",
    "#                         for PE in [True, False]:\n",
    "                        for PE in [False]:\n",
    "\n",
    "                            print(f'''\n",
    "                                Experimenting on {dataset} Dataset \n",
    "                                patch:{patch_size},\n",
    "                                sparse_att: {sparse_attention},\n",
    "                                sparse_mlp: {sparse_mlp},\n",
    "                                num_layers: {nlayers},\n",
    "                                pos_embed: {PE},\n",
    "                                seed: {seed}\n",
    "                            ''')\n",
    "\n",
    "                        ### check if config is in not_working case\n",
    "                            exit = False\n",
    "                            for nw in not_working:\n",
    "                                if patch_size==nw[0] and \\\n",
    "                                    sparse_attention==nw[1] and \\\n",
    "                                    sparse_mlp==nw[2] and\\\n",
    "                                    nlayers==nw[3]:\n",
    "\n",
    "                                    exit=True\n",
    "                                    break\n",
    "                            if exit:\n",
    "                                print(f'Exiting as the config is in NOT WORKING')\n",
    "                                continue\n",
    "\n",
    "\n",
    "                            benchmark(dataset=dataset, \n",
    "                                      patch_size=patch_size, \n",
    "                                      num_layers=nlayers, \n",
    "                                      SEED=seed, \n",
    "                                      sparse_att=sparse_attention, sparse_mlp=sparse_mlp, \n",
    "                                      pos_emb=PE,\n",
    "                                      cuda=cuda_idx\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c9c9683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdasdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea1d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use mixing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d174ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Experiments\n",
    "'''\n",
    "1. Sparse Attention\n",
    "2. Sparse MLP\n",
    "3. Sparse Att + MLP\n",
    "\n",
    "Datasets:\n",
    "A. Cifar-10/100 -> 4x4 vs 2x2 vs 1x1 patch\n",
    "B. Tiny-Imagenet -> 16x16 vs 4x4 vs 2x2 patch\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca1c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "## Cifar 10/100\n",
    "\n",
    "vit_mixer = Mixer_ViT_Classifier((3, 32, 32), patch_size=[8, 8], hidden_channel=256, num_blocks=6, num_classes=200, block_seq_size=4, block_mlp_size=16)\n",
    "\n",
    "vit_mixer = Mixer_ViT_Classifier((3, 32, 32), patch_size=[4, 4], hidden_channel=128, num_blocks=3, num_classes=10, block_seq_size=8, block_mlp_size=128)\n",
    "\n",
    "vit_mixer = Mixer_ViT_Classifier((3, 32, 32), patch_size=[2, 2], hidden_channel=64, num_blocks=3, num_classes=10, block_seq_size=16, block_mlp_size=8)\n",
    "\n",
    "vit_mixer = Mixer_ViT_Classifier((3, 32, 32), patch_size=[1, 1], hidden_channel=64, num_blocks=3, num_classes=10, block_seq_size=32, block_mlp_size=8)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1eee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "vit_mixer = Mixer_ViT_Classifier((3, 64, 64), patch_size=[4, 4], hidden_channel=128, num_blocks=9, num_classes=200, block_seq_size=16, block_mlp_size=16)\n",
    "Sequence len: 256 ; Block size: 16\n",
    "Channel dim: 128 num heads: 8\n",
    "MLP dim: 128 ; Block size: 16\n",
    "\n",
    "\n",
    "vit_mixer = Mixer_ViT_Classifier((3, 64, 64), patch_size=[8, 8], hidden_channel=256, num_blocks=9, num_classes=200, block_seq_size=8, block_mlp_size=16)\n",
    "Sequence len: 64 ; Block size: 16\n",
    "Channel dim: 256 num heads: 16\n",
    "MLP dim: 256 ; Block size: 16\n",
    "\n",
    "vit_mixer = Mixer_ViT_Classifier((3, 64, 64), patch_size=[16, 16], hidden_channel=1024, num_blocks=9, num_classes=200, block_seq_size=4, block_mlp_size=32)\n",
    "Sequence len: 16 ; Block size: 4\n",
    "Channel dim: 1024 num heads: 32\n",
    "MLP dim: 1024 ; Block size: 32\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57171612",
   "metadata": {},
   "source": [
    "### Automate the model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b409b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = 64\n",
    "patch_size = [16, 8, 4, 2, 1]\n",
    "expansion = [1024, 256, 128, 64, 64]\n",
    "\n",
    "for i in range(len(patch_size)):\n",
    "    ps, exp = patch_size[i], expansion[i]\n",
    "    print(i, ps, exp)\n",
    "    seq_len = (img//ps)**2\n",
    "    print('seq:', seq_len)\n",
    "    \n",
    "    fact_seq = int(2**np.ceil(np.log2(np.sqrt(seq_len))))\n",
    "    print(f'fact_seq: {fact_seq}')\n",
    "    \n",
    "    \n",
    "    fact_mlp = int(2**np.ceil(np.log2(np.sqrt(exp))))\n",
    "    print(f'fact_mlp: {fact_mlp}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
