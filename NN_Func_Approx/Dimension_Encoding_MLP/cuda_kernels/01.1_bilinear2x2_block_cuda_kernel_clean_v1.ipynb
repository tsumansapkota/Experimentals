{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import datasets\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cuda:1\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bmm2x2_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda -bmm2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMM2x2Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "#     @torch.jit.ignore\n",
    "    def forward(ctx, inputs, weights):\n",
    "        outputs = bmm2x2_cuda.forward(inputs, weights)\n",
    "        ctx.save_for_backward(inputs, weights)\n",
    "        return outputs[0]\n",
    "    \n",
    "    @staticmethod\n",
    "#     @torch.jit.ignore\n",
    "    def backward(ctx, grad_output):\n",
    "        inputs, weights = ctx.saved_tensors\n",
    "        del_input, del_weights = bmm2x2_cuda.backward(\n",
    "            inputs, \n",
    "            weights, \n",
    "            grad_output)\n",
    "    \n",
    "        return del_input, del_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairWeight2(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        assert input_dim%2 == 0, \"Input dim must be even number\"\n",
    "        self.weight = torch.eye(2).unsqueeze(0).repeat_interleave(input_dim//2, dim=0)\n",
    "        self.weight = nn.Parameter(self.weight)\n",
    "        self.bmmfunc = BMM2x2Function()\n",
    "        \n",
    "    @torch.jit.ignore\n",
    "    def bmm(self, x, w):\n",
    "        return BMM2x2Function.apply(x, w)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, dim = x.shape[0], x.shape[1]\n",
    "        x = x.view(bs, -1, 2)\n",
    "        x = self.bmm(x, self.weight)\n",
    "        x = x.view(bs, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0134,  2.1818,  1.9082,  ..., -0.3174,  2.0114,  0.8128],\n",
       "        [-0.4477, -0.1729,  0.5523,  ...,  0.3847, -1.6661, -1.3617]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pw = PairWeight2(784).to(device)\n",
    "pw(torch.randn(2,784).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bilinear2x2_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda - Bilinear2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLinear2x2Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, weights):\n",
    "        outputs = bilinear2x2_cuda.forward(inputs, weights)\n",
    "        ctx.save_for_backward(inputs, weights)\n",
    "        return outputs[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        inputs, weights = ctx.saved_tensors\n",
    "#         del_input, del_weights = bmm2x2_cuda.backward(\n",
    "#             grad_output.contiguous(), \n",
    "#             grad_cell.contiguous(), \n",
    "#             grad_output.contiguous())\n",
    "        del_input, del_weights = bilinear2x2_cuda.backward(\n",
    "            inputs, \n",
    "            weights, \n",
    "            grad_output)\n",
    "    \n",
    "        return del_input, del_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairBilinear2(nn.Module):\n",
    "    def __init__(self, dim, grid_width):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.grid_width = grid_width\n",
    "        \n",
    "        self.num_pairs = self.dim // 2\n",
    "        along_row = torch.linspace(0, 1, self.grid_width).reshape(1, -1)\n",
    "        along_col = torch.linspace(0, 1, self.grid_width).reshape(-1, 1)\n",
    "#         self.Y = torch.stack([along_row+along_col*0, along_row*0+along_col])\n",
    "        self.Y = torch.stack([along_row*0+along_col, along_row+along_col*0])\n",
    "\n",
    "        self.Y = torch.repeat_interleave(self.Y.unsqueeze(0), self.num_pairs, dim=0)\n",
    "        self.Y = nn.Parameter(self.Y)\n",
    "        \n",
    "        self.pairW = torch.eye(2).unsqueeze(0).repeat_interleave(self.num_pairs, dim=0)\n",
    "        self.pairW = nn.Parameter(self.pairW)\n",
    "    \n",
    "#     @torch.jit.ignore\n",
    "#     def pairbl2x2(self, x, w):\n",
    "#         return BiLinear2x2Function.apply(x, w)\n",
    "    \n",
    "#     @torch.jit.ignore\n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        \n",
    "############# This block ########################\n",
    "        ### this block is significantly faster\n",
    "    \n",
    "#         x = x.view(bs, -1, 2).transpose(0,1)\n",
    "#         x = torch.bmm(x, self.pairW)\n",
    "#         x = x.transpose(1,0)#.reshape(-1, 2)\n",
    "        \n",
    "############# OR This block ########################\n",
    "        x = x.view(bs, -1, 2)\n",
    "        x = BMM2x2Function.apply(x, self.pairW)\n",
    "####################################################\n",
    "        x = x.view(bs, -1, 2)\n",
    "        x = BiLinear2x2Function.apply(x, self.Y)\n",
    "        x = x.view(bs, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbl2 = PairBilinear2(8, 3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_a = torch.randn(2, 8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pbl2(_a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1486, -0.1944,  1.0231, -0.1754, -0.6524, -0.3859, -0.1675,  0.6723],\n",
       "        [-0.3980, -0.1785,  0.8948, -0.8194,  0.5285,  0.0020,  1.1192,  1.1377]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1486, -0.1944,  1.0231, -0.1754, -0.6524, -0.3859, -0.1675,  0.6723],\n",
       "        [-0.3980, -0.1785,  0.8948, -0.8194,  0.5285,  0.0020,  1.1192,  1.1377]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.5000],\n",
       "         [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.0000, 0.5000, 1.0000],\n",
       "         [0.0000, 0.5000, 1.0000],\n",
       "         [0.0000, 0.5000, 1.0000]]], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbl2.Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasLayer(nn.Module):\n",
    "    def __init__(self, dim, init_val=0):\n",
    "        super().__init__()\n",
    "        self.bias = nn.Parameter(torch.ones(dim)*init_val)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x+self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizedPairBilinearSpline(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, grid_width, num_layers=None):\n",
    "        super().__init__()\n",
    "        assert input_dim%2 == 0, \"Input dim must be even number\"\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        if num_layers is None:\n",
    "            self.num_layers = int(np.ceil(np.log2(input_dim)))\n",
    "        else:\n",
    "            self.num_layers = num_layers\n",
    "            \n",
    "        self.facto_nets = []\n",
    "        self.idx_revidx = []\n",
    "        for i in range(self.num_layers):\n",
    "            idrid = self.get_pair(self.input_dim, i+1)\n",
    "            net = PairBilinear2(self.input_dim, grid_width)\n",
    "            self.facto_nets.append(net)\n",
    "            self.idx_revidx.append(idrid)\n",
    "        self.facto_nets = nn.ModuleList(self.facto_nets)\n",
    "            \n",
    "#     @torch.jit.ignore\n",
    "    def get_pair(self, inp_dim, step=1):\n",
    "        dim = 2**int(np.ceil(np.log2(inp_dim)))\n",
    "        assert isinstance(step, int), \"Step must be integer\"\n",
    "\n",
    "        blocks = (2**step)\n",
    "        range_ = dim//blocks\n",
    "        adder_ = torch.arange(0, range_)*blocks\n",
    "\n",
    "        pairs_ = torch.Tensor([0, blocks//2])\n",
    "        repeat_ = torch.arange(0, blocks//2).reshape(-1,1)\n",
    "        block_map = (pairs_+repeat_).reshape(-1)\n",
    "\n",
    "        reorder_for_pair = (block_map+adder_.reshape(-1,1)).reshape(-1)\n",
    "        indx = reorder_for_pair.type(torch.long)\n",
    "        indx = indx[indx<inp_dim]\n",
    "\n",
    "        rev_indx = torch.argsort(indx)\n",
    "        return indx, rev_indx\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## swap first and then forward and reverse-swap\n",
    "        y = x\n",
    "#         for i in range(len(self.facto_nets)):\n",
    "        for i, fn in enumerate(self.facto_nets):\n",
    "            idx, revidx = self.idx_revidx[i]\n",
    "            y = y[:, idx]\n",
    "            y = fn(y) \n",
    "            y = y[:, revidx]\n",
    "#         y = x + y ## this is residual addition... remove if only want feed forward\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfL = FactorizedPairBilinearSpline(784, 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1258, -1.1524, -0.2506,  ..., -1.5825, -0.5878, -0.1140],\n",
       "        [ 0.7014, -0.5556, -0.3817,  ...,  0.3989,  0.2578,  0.1990],\n",
       "        [-0.1584,  0.7390, -0.2506,  ...,  0.9209, -0.1103, -1.8729],\n",
       "        ...,\n",
       "        [ 1.5498, -0.9650, -0.5772,  ...,  1.1618, -0.3113,  0.3023],\n",
       "        [-0.3681, -0.8609,  1.1708,  ...,  0.2597,  0.0747,  0.3674],\n",
       "        [ 0.8068, -0.4530, -0.9414,  ..., -1.0015,  0.7598, -0.1231]],\n",
       "       device='cuda:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfL(torch.randn(100, 784).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorizedPairBilinearSpline(\n",
       "  (facto_nets): ModuleList(\n",
       "    (0): PairBilinear2()\n",
       "    (1): PairBilinear2()\n",
       "    (2): PairBilinear2()\n",
       "    (3): PairBilinear2()\n",
       "    (4): PairBilinear2()\n",
       "    (5): PairBilinear2()\n",
       "    (6): PairBilinear2()\n",
       "    (7): PairBilinear2()\n",
       "    (8): PairBilinear2()\n",
       "    (9): PairBilinear2()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799680"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_count = sum([torch.numel(p) for p in pfL.parameters()])\n",
    "param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bias = BiasLayer(784)\n",
    "        self.la1 = FactorizedPairBilinearSpline(784, grid_width=2)\n",
    "        self.bn1 = nn.BatchNorm1d(784)\n",
    "        self.fc = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bias(x)\n",
    "        x = self.la1(x)\n",
    "#         x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FactorNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.bias = BiasLayer(784)\n",
    "#         self.la1 = FactorizedPairBilinearSpline(784, grid_width=5)\n",
    "#         self.bn1 = nn.BatchNorm1d(784)\n",
    "#         self.la2 = FactorizedPairBilinearSpline(784, grid_width=5)\n",
    "#         self.bn2 = nn.BatchNorm1d(784)\n",
    "#         self.fc = nn.Linear(784, 10)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.bias(x)\n",
    "#         x = self.bn1(self.la1(x))\n",
    "#         x = torch.relu(x)\n",
    "#         x = self.bn2(self.la2(x))\n",
    "#         x = torch.relu(x)\n",
    "#         x = self.fc(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinaryNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.la1 = nn.Linear(784, 784, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(784)\n",
    "        self.la2 = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.la1(x))\n",
    "        x = torch.relu(x)\n",
    "        x = self.la2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabben.datasets import OpenTabularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arcene', 'covertype', 'higgs', 'poker', 'adult', 'parkinsons', 'musk', 'rossman', 'amazon', 'duolingo-original', 'duolingo-categorical', 'cifar10']\n"
     ]
    }
   ],
   "source": [
    "from tabben.datasets import list_datasets\n",
    "print(list_datasets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already available at `../../../../../_Datasets/arcene.npz`\n",
      "Data already available at `../../../../../_Datasets/arcene.json`\n",
      "Data already available at `../../../../../_Datasets/arcene.npz`\n",
      "Data already available at `../../../../../_Datasets/arcene.json`\n"
     ]
    }
   ],
   "source": [
    "# load the arcene dataset (default is train split) and\n",
    "# save the data to the current directory \n",
    "ds_train = OpenTabularDataset('./../../../../../_Datasets/', 'arcene', split='train')\n",
    "ds_test = OpenTabularDataset('./../../../../../_Datasets/', 'arcene', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), 100)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[0][0].shape, len(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = data.DataLoader(ds_train, batch_size=4)\n",
    "# test_loader = data.DataLoader(ds_test, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx, yy = iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx.shape, yy.shape, yy.dtype, yy.type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = ds_train[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.Tensor(xx).type(torch.float32)\n",
    "yy = torch.Tensor(yy.reshape(-1,1)).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xx.isnan()).type(torch.float32).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = xx/(xx.max(dim=0, keepdim=True)[0]+1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = xx.to(device)\n",
    "yy = yy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "_xx, _yy = ds_test[:]\n",
    "_xx = torch.Tensor(_xx).type(torch.float32)\n",
    "_yy = torch.Tensor(_yy.reshape(-1,1)).type(torch.float32)\n",
    "_xx = _xx/_xx.max(dim=0, keepdim=True)[0]\n",
    "_xx = _xx.to(device)\n",
    "_yy = _yy.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpbs = FactorizedPairBilinearSpline(10000, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "_a = torch.randn(2, 10000).to(device)\n",
    "_y = fpbs(_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.3257, 0.0000,  ..., 0.0000, 0.0000, 0.9831],\n",
       "        [0.0000, 0.1881, 0.3475,  ..., 0.0000, 0.7245, 0.7936],\n",
       "        [0.0000, 0.0000, 0.0042,  ..., 0.0000, 0.0867, 0.9531],\n",
       "        ...,\n",
       "        [0.0106, 0.0688, 0.2034,  ..., 0.0000, 0.0000, 0.8499],\n",
       "        [0.0426, 0.0000, 0.1610,  ..., 0.0000, 0.4821, 0.7561],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0255, 0.6848]],\n",
       "       device='cuda:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpbs(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xx.isnan()).type(torch.float32).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.bias = BiasLayer(10000)\n",
    "#         self.la1 = FactorizedPairBilinearSpline(10000, grid_width=2)\n",
    "#         self.la1 = FactorizedPairBilinearSpline(10000, grid_width=5, num_layers=6)\n",
    "#         self.bn1 = nn.BatchNorm1d(10000)\n",
    "        self.fc = nn.Linear(10000, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         x = self.bias(x)\n",
    "#         x = self.la1(x.contiguous())\n",
    "#         x = self.la2(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = torch.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorNet(\n",
       "  (fc): Linear(in_features=10000, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = FactorNet().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xx)[0].isnan().type(torch.float32).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  10001\n"
     ]
    }
   ],
   "source": [
    "print(\"number of params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10000*10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0,  Loss:0.6665022373199463\n",
      "Train Acc:65.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 100,  Loss:0.09022673219442368\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 200,  Loss:0.03565641865134239\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 300,  Loss:0.019870519638061523\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 400,  Loss:0.012948412448167801\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 500,  Loss:0.009222248569130898\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 600,  Loss:0.006955340970307589\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 700,  Loss:0.005459170322865248\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 800,  Loss:0.004412537906318903\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 900,  Loss:0.0036477106623351574\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 1000,  Loss:0.003069523023441434\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 1100,  Loss:0.00262039084918797\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 1200,  Loss:0.0022636621724814177\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 1300,  Loss:0.001975040649995208\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 1400,  Loss:0.0017378313932567835\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 1500,  Loss:0.0015402468852698803\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 1600,  Loss:0.001373750506900251\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 1700,  Loss:0.0012320226524025202\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 1800,  Loss:0.0011102897115051746\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 1900,  Loss:0.0010048943804576993\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 2000,  Loss:0.0009130110847763717\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 2100,  Loss:0.0008323927177116275\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 2200,  Loss:0.0007612528279423714\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 2300,  Loss:0.0006981365731917322\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 2400,  Loss:0.0006418885313905776\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 2500,  Loss:0.000591551885008812\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 2600,  Loss:0.0005463147535920143\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 2700,  Loss:0.0005055147921666503\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 2800,  Loss:0.0004686029860749841\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 2900,  Loss:0.0004350914095994085\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 3000,  Loss:0.00040460037416778505\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 3100,  Loss:0.000376762734958902\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 3200,  Loss:0.0003512983676046133\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 3300,  Loss:0.00032795427250675857\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 3400,  Loss:0.00030649470863863826\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 3500,  Loss:0.0002867441507987678\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 3600,  Loss:0.0002685265790205449\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 3700,  Loss:0.00025168739375658333\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 3800,  Loss:0.00023610852076672018\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 3900,  Loss:0.00022167414135765284\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 4000,  Loss:0.0002082649734802544\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 4100,  Loss:0.00019580282969400287\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 4200,  Loss:0.00018421554705128074\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 4300,  Loss:0.00017341363127343357\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 4400,  Loss:0.00016333624080289155\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 4500,  Loss:0.00015392912609968334\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 4600,  Loss:0.00014513616042677313\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 4700,  Loss:0.00013691026833839715\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 4800,  Loss:0.00012921146117150784\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 4900,  Loss:0.00012199624325148761\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 5000,  Loss:0.00011521924170665443\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 5100,  Loss:0.00010887331882258877\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 5200,  Loss:0.00010291791113559157\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 5300,  Loss:9.730473539093509e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 5400,  Loss:9.202902583638206e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 5500,  Loss:8.70704825501889e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 5600,  Loss:8.240168972406536e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 5700,  Loss:7.800175808370113e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 5800,  Loss:7.386058132397011e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 5900,  Loss:6.994832801865414e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 6000,  Loss:6.626202230108902e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 6100,  Loss:6.278675573412329e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 6200,  Loss:5.950225386186503e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 6300,  Loss:5.639003938995302e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 6400,  Loss:5.346799298422411e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 6500,  Loss:5.069080725661479e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 6600,  Loss:4.807457298738882e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 6700,  Loss:4.559664739645086e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 6800,  Loss:4.3254633055767044e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 6900,  Loss:4.1044953832169995e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 7000,  Loss:3.894555993610993e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 7100,  Loss:3.6959423596272245e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 7200,  Loss:3.507582368911244e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 7300,  Loss:3.329474930069409e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 7400,  Loss:3.1610245059709996e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 7500,  Loss:3.0010982300154865e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 7600,  Loss:2.849457632692065e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 7700,  Loss:2.7052086807088926e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 7800,  Loss:2.5701992854010314e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 7900,  Loss:2.44067377934698e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 8000,  Loss:2.317824328201823e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 8100,  Loss:2.2020683900336735e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 8200,  Loss:2.091975329676643e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 8300,  Loss:1.987008363357745e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 8400,  Loss:1.888181213871576e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 8500,  Loss:1.7941822079592384e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 8600,  Loss:1.7047135770553723e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 8700,  Loss:1.619655631657224e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 8800,  Loss:1.5389490727102384e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 8900,  Loss:1.4627727068727836e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 9000,  Loss:1.3903516446589492e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 9100,  Loss:1.321745276072761e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 9200,  Loss:1.2562982192321215e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 9300,  Loss:1.1940698641410563e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 9400,  Loss:1.1352986803103704e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 9500,  Loss:1.0786733582790475e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 9600,  Loss:1.0258630027237814e-05\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 9700,  Loss:9.744234375830274e-06\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 9800,  Loss:9.26500706555089e-06\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "Epoch: 9900,  Loss:8.804257959127426e-06\n",
      "Train Acc:100.00%, Test Acc:56.00%\n",
      "\n",
      "\t-> Train Acc 100.0 ; Test Acc 56.0\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "EPOCHS = 10000\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train_acc = 0\n",
    "\n",
    "    yout = model(xx)\n",
    "    loss = criterion(yout, yy)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        correct = ((yout>0.5).type(torch.float32) == yy).type(torch.float32)\n",
    "        train_acc = correct.mean()\n",
    "        train_accs.append(train_acc*100)\n",
    "        losses.append(float(loss))\n",
    "        \n",
    "        print(f'Epoch: {epoch},  Loss:{float(loss)}')\n",
    "\n",
    "        yout = model(_xx)\n",
    "        correct = ((yout>0.5).type(torch.float32) == yy).type(torch.float32)\n",
    "        train_acc = correct.mean()\n",
    "        test_accs.append(train_acc*100)\n",
    "        print(f'Train Acc:{train_accs[-1]:.2f}%, Test Acc:{test_accs[-1]:.2f}%')\n",
    "        print()\n",
    "\n",
    "### after each class index is finished training\n",
    "print(f'\\t-> Train Acc {max(train_accs)} ; Test Acc {max(test_accs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100%|██████████| 1200/1200 [00:52<00:00, 22.72it/s] using called pairlinear\n",
    "# 100%|██████████| 1200/1200 [00:10<00:00, 118.42it/s] using Ordinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stats: 20 epochs || Fact+BN+Linear ; lr0.0001 ##_with 3 bilinear layers\n",
    "### for factor-net: 5grid : 73706-> 100%|██████████| 1200/1200 [00:24<00:00, 48.44it/s]\n",
    "########### -> Train Acc 90.3367 ; Test Acc 88.06\n",
    "\n",
    "### for factor-net: 50grid : 5894906-> 100%|██████████| 1200/1200 [00:28<00:00, 42.74it/s]\n",
    "########### -> Train Acc 99.985 ; Test Acc 85.85\n",
    "\n",
    "### for factor-net: 10grid : 250106-> 100%|██████████| 1200/1200 [00:24<00:00, 48.11it/s]\n",
    "########### -> Train Acc 92.17167 ; Test Acc 88.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for factor-net: fact+bn+relu+linear : 5grid : lr 0.0003\n",
    "####### -> Train Acc 92.42833333333334 ; Test Acc 88.42\n",
    "\n",
    "### same : factor-net had default of 3 bilinear layers.. changed to log2(input dim)=10 to properly mix all.\n",
    "#######  -> 100%|██████████| 1200/1200 [00:50<00:00, 23.88it/s]\n",
    "### facto-net: fact+bn+relu+linear : 5grid  -> params=221882\n",
    "######## -> Train Acc 95.165 ; Test Acc 89.45\n",
    "\n",
    "### ordinary net || linear+BN+Linear : lr=0.0003 : params=624074  -> [579.83it/s]\n",
    "######## -> Train Acc 95.96166666666667 ; Test Acc 89.33\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
