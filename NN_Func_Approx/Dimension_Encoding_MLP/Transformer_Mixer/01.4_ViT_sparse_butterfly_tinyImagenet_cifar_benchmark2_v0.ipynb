{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6287145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os, sys, pathlib, random, time, pickle, copy, json\n",
    "# from tqdm.autonotebook import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0173cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dcaa3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_lib import TransformerBlock, \\\n",
    "        Mixer_TransformerBlock_Encoder, \\\n",
    "        PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "387412b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyImageNet_Preload(data.Dataset):\n",
    "#     https://gist.github.com/z-a-f/b862013c0dc2b540cf96a123a6766e54\n",
    "    \n",
    "    def __init__(self, root, mode='train', transform=None, preload=False):\n",
    "        super().__init__()\n",
    "        self.preload = preload\n",
    "        dataset = datasets.ImageFolder(\n",
    "            root=os.path.join(root, mode),\n",
    "            transform=None\n",
    "        )\n",
    "        self.transform = transform\n",
    "        self.images, self.labels = [], []\n",
    "        print(\"Dataset Size:\",len(dataset))\n",
    "        \n",
    "        if preload:\n",
    "            for i in tqdm(range(len(dataset))):\n",
    "                x, y = dataset[i]\n",
    "                self.images.append(x)\n",
    "                self.labels.append(y)\n",
    "                \n",
    "#         del dataset\n",
    "        self.dataset = dataset\n",
    "            \n",
    "    def _add_channels(img, total_channels=3):\n",
    "        while len(img.shape) < 3:  # third axis is the channels\n",
    "            img = np.expand_dims(img, axis=-1)\n",
    "        while(img.shape[-1]) < 3:\n",
    "            img = np.concatenate([img, img[:, :, -1:]], axis=-1)\n",
    "        return img\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.preload:\n",
    "            img, lbl = self.images[idx], self.labels[idx]\n",
    "        else:\n",
    "            img, lbl = self.dataset[idx]\n",
    "        return self.transform(img), lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2e11b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b45f6816",
   "metadata": {},
   "outputs": [],
   "source": [
    "### add randomize patches for clear benefit\n",
    "class Mixer_ViT_Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_dim:tuple, patch_size:tuple, hidden_channel:int, num_blocks:int, num_classes:int, block_seq_size:int, block_mlp_size:int, forward_expansion:float=2.0, pos_emb=True, dropout:float=0.0, randomize_patch:bool=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_dim = image_dim ### must contain (C, H, W) or (H, W)\n",
    "        \n",
    "        ### find patch dim\n",
    "        d0 = int(image_dim[-2]/patch_size[0])\n",
    "        d1 = int(image_dim[-1]/patch_size[1])\n",
    "        assert d0*patch_size[0]==image_dim[-2], \"Image must be divisible into patch size\"\n",
    "        assert d1*patch_size[1]==image_dim[-1], \"Image must be divisible into patch size\"\n",
    "        \n",
    "#         self.d0, self.d1 = d0, d1 ### number of patches in each axis\n",
    "        __patch_size = patch_size[0]*patch_size[1]*image_dim[0] ## number of channels in each patch\n",
    "    \n",
    "        ### find channel dim\n",
    "        channel_size = d0*d1 ## number of patches\n",
    "        \n",
    "        ### after the number of channels are changed\n",
    "        init_dim = __patch_size\n",
    "        final_dim = hidden_channel\n",
    "        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        #### rescale the patches (patch wise image non preserving transform, unlike bilinear interpolation)\n",
    "        self.channel_change = nn.Linear(init_dim, final_dim)\n",
    "        print(f\"ViT Mixer : Channels per patch -> Initial:{init_dim} Final:{final_dim}\")\n",
    "        \n",
    "        \n",
    "        self.channel_dim = final_dim\n",
    "        self.patch_dim = channel_size\n",
    "        \n",
    "        self.transformer_blocks = []\n",
    "        \n",
    "        f = self.get_factors(self.channel_dim)\n",
    "        print(f)\n",
    "        fi = np.abs(np.array(f) - np.sqrt(self.channel_dim)).argmin()\n",
    "        \n",
    "        _n_heads = f[fi]\n",
    "        \n",
    "        ## number of dims per channel -> channel_dim\n",
    "#         print('Num patches:', self.patch_dim)\n",
    "        print(f'Sequence len: {self.patch_dim} ; Block size: {block_seq_size}')\n",
    "        print('Channel dim:', self.channel_dim, 'num heads:',_n_heads)\n",
    "            \n",
    "        \n",
    "        if block_seq_size is None or block_seq_size<2:\n",
    "            ### Find the block size for sequence:\n",
    "            block_seq_size = int(2**np.ceil(np.log2(np.sqrt(self.patch_dim))))\n",
    "            \n",
    "        print(f'MLP dim: {self.channel_dim} ; Block size: {block_mlp_size}')\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            L = Mixer_TransformerBlock_Encoder(self.patch_dim, block_seq_size, self.channel_dim, _n_heads, dropout, forward_expansion, nn.GELU, block_mlp_size)\n",
    "            self.transformer_blocks.append(L)\n",
    "        self.transformer_blocks = nn.Sequential(*self.transformer_blocks)\n",
    "        \n",
    "        self.linear = nn.Linear(self.patch_dim*self.channel_dim, num_classes)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(self.channel_dim, dropout=0)\n",
    "        if not pos_emb:\n",
    "            self.positional_encoding = nn.Identity()\n",
    "            \n",
    "        self.randomize = None\n",
    "        if randomize_patch is not None:\n",
    "            self.randomize = torch.randperm(self.patch_dim)\n",
    "        \n",
    "        \n",
    "    def get_factors(self, n):\n",
    "        facts = []\n",
    "        for i in range(2, n+1):\n",
    "            if n%i == 0:\n",
    "                facts.append(i)\n",
    "        return facts\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.unfold(x).swapaxes(-1, -2)\n",
    "        x = self.channel_change(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        ## swap position of patches here\n",
    "        if self.randomize is not None:\n",
    "            x = x[..., self.randomize, :]\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.linear(x.view(bs, -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7094b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b0ac9f5-f1e2-4dba-a8e3-06bb4d5acac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a356db1-2fad-40ba-8186-7ca3c40ddd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3efe0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67e27199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT Mixer : Channels per patch -> Initial:12 Final:64\n",
      "[2, 4, 8, 16, 32, 64]\n",
      "Sequence len: 256 ; Block size: 16\n",
      "Channel dim: 64 num heads: 8\n",
      "MLP dim: 64 ; Block size: None\n"
     ]
    }
   ],
   "source": [
    "model = Mixer_ViT_Classifier([3, 32, 32], [2, 2], 64, num_blocks=2, num_classes=10, \n",
    "                            block_seq_size=16, block_mlp_size=None, pos_emb=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f741268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1017ac1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT Mixer : Channels per patch -> Initial:3 Final:64\n",
      "[2, 4, 8, 16, 32, 64]\n",
      "Sequence len: 1024 ; Block size: 32\n",
      "Channel dim: 64 num heads: 8\n",
      "MLP dim: 64 ; Block size: None\n"
     ]
    }
   ],
   "source": [
    "model = Mixer_ViT_Classifier([3, 32, 32], [1, 1], 64, num_blocks=2, num_classes=10, \n",
    "                            block_seq_size=32, block_mlp_size=None, pos_emb=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae575940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfold: 0\n",
      "channel_change: 256\n",
      "transformer_blocks: 133888\n",
      "linear: 655370\n",
      "positional_encoding: 0\n"
     ]
    }
   ],
   "source": [
    "# print(\"number of params: \", sum(p.numel() for p in model.parameters()))\n",
    "for name, m in model.named_children():\n",
    "#     print(name)\n",
    "    print(f\"{name}: {sum(p.numel() for p in m.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d642126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b258ea4-8037-457d-aea6-35e9e02d9be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  789514\n"
     ]
    }
   ],
   "source": [
    "print(\"number of params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e21965e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx = torch.randn(32, 3, 32, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "493d244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit model(xx).mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad3633e7-1a94-4f3e-a218-109083017442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit model(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8d7dd09f-bc43-4e55-9c57-a8ecdce48302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     %timeit model(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9707501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Mixer_ViT_Classifier([3, 32, 32], [1, 1], 64, num_blocks=4, num_classes=10, \n",
    "#                             block_seq_size=1024, block_mlp_size=None, pos_emb=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "652815b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c38b267c-a70b-43df-89aa-1548eab48e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"number of params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1cee9c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit model(xx).mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "78b2a832-f765-45db-a180-5b0a48074860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit model(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "669dc9ed-77a8-4615-8b16-d4dc13c4412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     %timeit model(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "044f8dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d123fa46-4abe-4533-82aa-21139c782670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_skip(model_name, ep):\n",
    "    \n",
    "    ## if file of benchmark is there and the training is done for full epochs\n",
    "    filename = f\"./output/benchmark/{model_name}_data.json\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename) as f:\n",
    "            data = json.load(f)\n",
    "        ## data consists of lists and dicts.\n",
    "        epochs = data['train_stat'][-1][0]\n",
    "        if epochs >= ep-1:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3c173ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(dataset:str, patch_size:int, num_layers:int, SEED:int, sparse_att:bool=False, sparse_mlp:bool=False, pos_emb:bool=False, cuda:int=0):\n",
    "    device = torch.device(f\"cuda:{cuda}\")\n",
    "    \n",
    "    if sparse_att:\n",
    "        assert num_layers%2 == 0, 'number of blocks on sparse transformer is (x2)/2 hence it must be even'\n",
    "        num_layers_ = num_layers//2\n",
    "    else:\n",
    "        num_layers_ = num_layers\n",
    "    \n",
    "    BS = 256\n",
    "    NC = -1\n",
    "    EPOCHS = 200\n",
    "    imsize = (3, 32, 32)\n",
    "    expansion_dict = {16:1024, 8:256, 4:128, 2:64, 1:64}\n",
    "    expansion = expansion_dict[patch_size]\n",
    "\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    ##### Data Transforms\n",
    "    if dataset == 'tiny':\n",
    "        NC = 200\n",
    "        EPOCHS = 400\n",
    "        imsize = (3, 64, 64)\n",
    "        tiny_train = transforms.Compose([\n",
    "        transforms.RandAugment(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.5]*3,\n",
    "            std=[0.2]*3,\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        tiny_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.5]*3,\n",
    "                std=[0.2]*3,\n",
    "            ),\n",
    "        ])\n",
    "        \n",
    "        train_dataset = TinyImageNet_Preload(root=\"../../../../../_Datasets/tiny-imagenet-200\",\n",
    "                                     mode='train', transform=tiny_train)\n",
    "        test_dataset = TinyImageNet_Preload(root=\"../../../../../_Datasets/tiny-imagenet-200\",\n",
    "                                     mode='val', transform=tiny_test)\n",
    "        \n",
    "    elif dataset == 'cifar10':\n",
    "        NC = 10\n",
    "        BS = 32\n",
    "        cifar_train = transforms.Compose([\n",
    "            transforms.RandomCrop(size=32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "                std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        cifar_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "                std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        train_dataset = datasets.CIFAR10(root=\"../../../../../_Datasets/cifar10/\", train=True, download=True, transform=cifar_train)\n",
    "        test_dataset = datasets.CIFAR10(root=\"../../../../../_Datasets/cifar10/\", train=False, download=True, transform=cifar_test)\n",
    "\n",
    "    elif dataset == 'cifar100':\n",
    "        NC = 100\n",
    "        BS = 128\n",
    "        cifar_train = transforms.Compose([\n",
    "            transforms.RandomCrop(size=32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.5071, 0.4865, 0.4409],\n",
    "                std=[0.2009, 0.1984, 0.2023],\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        cifar_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.5071, 0.4865, 0.4409],\n",
    "                std=[0.2009, 0.1984, 0.2023],\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        train_dataset = datasets.CIFAR100(root=\"../../../../../_Datasets/cifar100/\", train=True, download=True, transform=cifar_train)\n",
    "        test_dataset = datasets.CIFAR100(root=\"../../../../../_Datasets/cifar100/\", train=False, download=True, transform=cifar_test)\n",
    "        \n",
    "    ##### Now create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BS, shuffle=True, num_workers=4)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BS, shuffle=False, num_workers=4)\n",
    "    \n",
    "    ### Now create models\n",
    "    \n",
    "    seq_len = (imsize[-1]*imsize[-2])//(patch_size*patch_size)\n",
    "    mlp_dim = expansion\n",
    "    print(seq_len, mlp_dim)\n",
    "    \n",
    "    if sparse_att:\n",
    "        seq_len = int(2**np.ceil(np.log2(np.sqrt(seq_len))))\n",
    "    if sparse_mlp:\n",
    "        mlp_dim = int(2**np.ceil(np.log2(np.sqrt(expansion))))\n",
    "    \n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    model = Mixer_ViT_Classifier(imsize, \n",
    "                                 patch_size=[patch_size]*2, \n",
    "                                 hidden_channel=expansion, \n",
    "                                 num_blocks=num_layers_, \n",
    "                                 num_classes=NC, \n",
    "                                 block_seq_size=seq_len, \n",
    "                                 block_mlp_size=mlp_dim,\n",
    "                                 pos_emb=pos_emb, randomize_patch=True).to(device)\n",
    "    \n",
    "#     _x = torch.randn(BS, *imsize).to(device)\n",
    "#     print(\"Output: \",vit_mixer(_x).shape)\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"number of params: \", num_params)\n",
    "    \n",
    "    _a, _b, _c = 'att', 'mlp', 'nPE'\n",
    "    if sparse_att: _a = 'sAtt'\n",
    "    if sparse_mlp: _b = 'sMlp'\n",
    "    if pos_emb: _c = 'PE'\n",
    "    model_name = f'01.3_ViT_{_c}_{dataset}_patch{patch_size}_l{num_layers}_{_a}_{_b}_s{SEED}_rand'\n",
    "    print(f\"Model Name: {model_name}\")\n",
    "    \n",
    "    if experiment_skip(model_name, EPOCHS):\n",
    "        print(f'EXPERIMENT DONE... SKIPPING : {model_name}')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    STAT ={'train_stat':[], 'test_stat':[], 'params':num_params, }\n",
    "\n",
    "    ## Following is copied from \n",
    "    ### https://github.com/kuangliu/pytorch-cifar/blob/master/main.py\n",
    "\n",
    "    # Training\n",
    "    def train(epoch):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "#             break\n",
    "\n",
    "        STAT['train_stat'].append((epoch, train_loss/(batch_idx+1), 100.*correct/total)) ### (Epochs, Loss, Acc)\n",
    "        print(f\"[Train] {epoch} Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "        return\n",
    "\n",
    "    global best_acc\n",
    "    best_acc = -1\n",
    "    def test(epoch):\n",
    "        global best_acc\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        time_taken = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader)):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                start = time.time()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                start = time.time()-start\n",
    "                time_taken.append(start)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        STAT['test_stat'].append((epoch, test_loss/(batch_idx+1), 100.*correct/total, np.mean(time_taken))) ### (Epochs, Loss, Acc, time)\n",
    "        print(f\"[Test] {epoch} Loss: {test_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "\n",
    "        # Save checkpoint.\n",
    "        acc = 100.*correct/total\n",
    "        if acc > best_acc:\n",
    "            print('Saving..')\n",
    "            state = {\n",
    "                'model': model.state_dict(),\n",
    "                'acc': acc,\n",
    "                'epoch': epoch\n",
    "            }\n",
    "            if not os.path.isdir('models'):\n",
    "                os.mkdir('models')\n",
    "            torch.save(state, f'./models/benchmark/{model_name}.pth')\n",
    "            best_acc = acc\n",
    "\n",
    "        with open(f\"./output/benchmark/{model_name}_data.json\", 'w') as f:\n",
    "            json.dump(STAT, f, indent=0)\n",
    "\n",
    "    ### Train the whole damn thing\n",
    "#     EPOCHS = 1\n",
    "    for epoch in range(0, EPOCHS):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "        scheduler.step()\n",
    "        \n",
    "    \n",
    "    train_stat = np.array(STAT['train_stat'])\n",
    "    test_stat = np.array(STAT['test_stat'])\n",
    "\n",
    "    plt.plot(train_stat[:,1], label='train')\n",
    "    plt.plot(test_stat[:,1], label='test')\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"./output/benchmark/plots/{model_name}_loss.svg\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_stat[:,2], label='train')\n",
    "    plt.plot(test_stat[:,2], label='test')\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"./output/benchmark/plots/{model_name}_accs.svg\")\n",
    "    plt.show()\n",
    "    \n",
    "    del model, optimizer\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2e6ae4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "1024 64\n",
      "ViT Mixer : Channels per patch -> Initial:3 Final:64\n",
      "[2, 4, 8, 16, 32, 64]\n",
      "Sequence len: 1024 ; Block size: 32\n",
      "Channel dim: 64 num heads: 8\n",
      "MLP dim: 64 ; Block size: 64\n",
      "number of params:  789514\n",
      "Model Name: 01.3_ViT_nPE_cifar10_patch1_l4_sAtt_mlp_s147_rand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:12<00:00, 11.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 0 Loss: 1.987 | Acc: 33.006 16503/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:09<00:00, 34.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 0 Loss: 1.717 | Acc: 41.270 4127/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:12<00:00, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 1 Loss: 1.721 | Acc: 39.856 19928/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:09<00:00, 34.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 1 Loss: 1.542 | Acc: 46.060 4606/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:12<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 2 Loss: 1.603 | Acc: 43.174 21587/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:09<00:00, 34.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 2 Loss: 1.465 | Acc: 48.710 4871/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:13<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 3 Loss: 1.516 | Acc: 45.948 22974/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:09<00:00, 32.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 3 Loss: 1.382 | Acc: 51.200 5120/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▉                                                | 89/1563 [00:07<02:02, 12.00it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:12<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 19 Loss: 1.037 | Acc: 63.688 31844/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:08<00:00, 34.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 19 Loss: 1.046 | Acc: 63.280 6328/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:12<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 20 Loss: 1.025 | Acc: 63.910 31955/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:08<00:00, 34.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 20 Loss: 1.048 | Acc: 62.940 6294/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:12<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 21 Loss: 1.016 | Acc: 63.844 31922/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:09<00:00, 34.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 21 Loss: 1.043 | Acc: 62.900 6290/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|█████████████████████████████████████▋           | 1203/1563 [01:42<00:30, 11.86it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|███████████████████████████████████████████████████| 313/313 [00:09<00:00, 32.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 37 Loss: 0.970 | Acc: 66.620 6662/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:12<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 38 Loss: 0.868 | Acc: 69.268 34634/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:09<00:00, 34.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 38 Loss: 0.959 | Acc: 67.050 6705/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:12<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 39 Loss: 0.859 | Acc: 69.734 34867/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:09<00:00, 34.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 39 Loss: 0.957 | Acc: 67.000 6700/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:12<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 40 Loss: 0.857 | Acc: 69.922 34961/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:09<00:00, 34.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 40 Loss: 0.940 | Acc: 67.550 6755/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████████████████▋                               | 586/1563 [00:49<01:16, 12.83it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:12<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 54 Loss: 0.784 | Acc: 72.468 36234/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:09<00:00, 34.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 54 Loss: 0.919 | Acc: 68.690 6869/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:12<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 55 Loss: 0.782 | Acc: 72.650 36325/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:09<00:00, 34.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 55 Loss: 0.933 | Acc: 68.010 6801/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:04<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 56 Loss: 0.776 | Acc: 72.666 36333/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 56 Loss: 0.925 | Acc: 68.300 6830/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|███████████████████████████████████████████▏     | 1379/1563 [01:50<00:14, 12.62it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:05<00:00, 12.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 73 Loss: 0.717 | Acc: 74.734 37367/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 73 Loss: 0.899 | Acc: 69.600 6960/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:05<00:00, 12.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 74 Loss: 0.708 | Acc: 74.968 37484/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 74 Loss: 0.905 | Acc: 69.580 6958/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:05<00:00, 12.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 75 Loss: 0.705 | Acc: 75.132 37566/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 75 Loss: 0.901 | Acc: 69.870 6987/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████▋                | 1041/1563 [01:23<00:42, 12.27it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:05<00:00, 12.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 91 Loss: 0.658 | Acc: 76.768 38384/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:08<00:00, 34.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 91 Loss: 0.924 | Acc: 69.560 6956/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:05<00:00, 12.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 92 Loss: 0.659 | Acc: 76.706 38353/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:09<00:00, 34.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 92 Loss: 0.925 | Acc: 70.290 7029/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:05<00:00, 12.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 93 Loss: 0.660 | Acc: 76.648 38324/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 93 Loss: 0.916 | Acc: 69.530 6953/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|███████████████████████████▎                      | 853/1563 [01:08<00:57, 12.45it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:05<00:00, 12.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 109 Loss: 0.616 | Acc: 78.366 39183/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:08<00:00, 35.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 109 Loss: 0.928 | Acc: 70.340 7034/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:05<00:00, 12.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 110 Loss: 0.612 | Acc: 78.264 39132/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:08<00:00, 34.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 110 Loss: 0.944 | Acc: 69.860 6986/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:05<00:00, 12.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 111 Loss: 0.613 | Acc: 78.414 39207/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:09<00:00, 34.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 111 Loss: 0.935 | Acc: 70.530 7053/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:05<00:00, 12.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 112 Loss: 0.607 | Acc: 78.558 39279/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|█████████████████████████████████████████████▎     | 278/313 [00:08<00:01, 29.53it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|███████████████████████████████████████████████████| 313/313 [00:10<00:00, 29.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 123 Loss: 0.957 | Acc: 70.530 7053/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:42<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 124 Loss: 0.585 | Acc: 79.224 39612/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:10<00:00, 29.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 124 Loss: 0.958 | Acc: 70.540 7054/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:43<00:00,  9.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 125 Loss: 0.585 | Acc: 79.304 39652/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:10<00:00, 29.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 125 Loss: 0.954 | Acc: 70.390 7039/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████▌                                          | 235/1563 [00:24<02:16,  9.71it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:42<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 135 Loss: 0.564 | Acc: 80.052 40026/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:10<00:00, 30.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 135 Loss: 0.975 | Acc: 70.570 7057/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:42<00:00,  9.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 136 Loss: 0.564 | Acc: 80.108 40054/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:10<00:00, 30.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 136 Loss: 0.970 | Acc: 70.370 7037/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████▏    | 1408/1563 [02:26<00:16,  9.18it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:42<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 146 Loss: 0.550 | Acc: 80.632 40316/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:10<00:00, 29.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 146 Loss: 0.969 | Acc: 70.610 7061/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|███████████████████████▌                          | 738/1563 [01:17<01:28,  9.31it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:42<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 148 Loss: 0.547 | Acc: 80.770 40385/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:10<00:00, 30.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 148 Loss: 0.976 | Acc: 70.420 7042/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████▊                              | 618/1563 [01:04<01:36,  9.80it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:42<00:00,  9.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 157 Loss: 0.529 | Acc: 81.190 40595/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:10<00:00, 29.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 157 Loss: 0.981 | Acc: 70.490 7049/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:42<00:00,  9.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 158 Loss: 0.537 | Acc: 81.012 40506/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:10<00:00, 29.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 158 Loss: 0.979 | Acc: 70.340 7034/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████                                         | 285/1563 [00:29<02:10,  9.77it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:42<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 160 Loss: 0.537 | Acc: 81.090 40545/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:10<00:00, 29.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 160 Loss: 0.985 | Acc: 70.290 7029/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████▏                                            | 163/1563 [00:17<02:26,  9.53it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|███████████████████████████████████████████████████| 313/313 [00:10<00:00, 30.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 168 Loss: 0.985 | Acc: 70.700 7070/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:42<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 169 Loss: 0.527 | Acc: 81.306 40653/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:10<00:00, 30.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 169 Loss: 0.984 | Acc: 70.480 7048/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:42<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 170 Loss: 0.526 | Acc: 81.494 40747/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:10<00:00, 30.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 170 Loss: 0.990 | Acc: 70.640 7064/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:42<00:00,  9.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 171 Loss: 0.519 | Acc: 81.624 40812/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:10<00:00, 29.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 171 Loss: 0.989 | Acc: 70.710 7071/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|████████████████████████████████████████████████▎| 1542/1563 [02:40<00:02,  9.53it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:42<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 182 Loss: 0.519 | Acc: 81.848 40924/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:10<00:00, 29.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 182 Loss: 0.989 | Acc: 70.750 7075/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [02:42<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 183 Loss: 0.518 | Acc: 81.710 40855/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:10<00:00, 30.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 183 Loss: 0.991 | Acc: 70.730 7073/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████████████████████████████████████████▍   | 1450/1563 [02:30<00:11,  9.43it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████████████████| 1563/1563 [01:29<00:00, 17.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 194 Loss: 0.516 | Acc: 81.800 40900/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:06<00:00, 49.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 194 Loss: 0.993 | Acc: 70.770 7077/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [01:29<00:00, 17.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 195 Loss: 0.516 | Acc: 81.864 40932/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:06<00:00, 49.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 195 Loss: 0.993 | Acc: 70.730 7073/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [01:29<00:00, 17.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 196 Loss: 0.511 | Acc: 81.988 40994/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:06<00:00, 49.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 196 Loss: 0.993 | Acc: 70.720 7072/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1563/1563 [01:29<00:00, 17.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 197 Loss: 0.518 | Acc: 81.788 40894/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 313/313 [00:06<00:00, 49.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 197 Loss: 0.993 | Acc: 70.720 7072/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|███████████▋                                      | 364/1563 [00:20<01:10, 17.09it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(dataset='cifar10', \n",
    "          patch_size=1, \n",
    "          num_layers=4, \n",
    "          SEED=147, \n",
    "          sparse_att=True, sparse_mlp=False, \n",
    "          pos_emb=False,\n",
    "          cuda=1,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0d03c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark(dataset='tiny', \n",
    "#           patch_size=4, num_layers=10, SEED=123, sparse_att=True, sparse_mlp=True, cuda=0\n",
    "#          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "717e715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Automate the benchmark\n",
    "# ###### for tiny\n",
    "# cuda_idx = 0\n",
    "# # for seed in [147, 258, 369]:\n",
    "# for seed in [147]:\n",
    "#     for patch_size in [16, 8, 4]:\n",
    "#         for sparse_attention in [False, True]:\n",
    "#             for sparse_mlp in [False, True]:\n",
    "# #                 for nlayers in [6, 10, 14]:\n",
    "#                 for nlayers in [6]:\n",
    "#                     print(f'''\n",
    "#                         Experimenting on Tiny Dataset \n",
    "#                         patch:{patch_size},\n",
    "#                         sparse_att: {sparse_attention},\n",
    "#                         sparse_mlp: {sparse_mlp},\n",
    "#                         num_layers : {nlayers},\n",
    "#                         seed: {seed}\n",
    "#                     ''')\n",
    "            \n",
    "#                     benchmark(dataset='tiny', \n",
    "#                               patch_size=patch_size, \n",
    "#                               num_layers=nlayers, \n",
    "#                               SEED=seed, \n",
    "#                               sparse_att=sparse_attention, sparse_mlp=sparse_mlp, \n",
    "#                               cuda=cuda_idx\n",
    "#                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af01a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Automate the benchmark\n",
    "# ###### for tiny\n",
    "\n",
    "# not_working = [\n",
    "#     (4, False, True, 6),\n",
    "#     (4, False, False, 10),\n",
    "#     (4, False, False, 14),\n",
    "#     (4, False, True, 10),\n",
    "#     (4, False, True, 14),\n",
    "#     (4, True, True, 14),\n",
    "# ]\n",
    "\n",
    "# cuda_idx = 0\n",
    "# # for seed in [147, 258, 369]:\n",
    "# for seed in [147]:\n",
    "#     for patch_size in [16, 8, 4]:\n",
    "#         for sparse_attention in [False, True]:\n",
    "#             for sparse_mlp in [False, True]:\n",
    "#                 for nlayers in [6, 10, 14]:\n",
    "\n",
    "#                     print(f'''\n",
    "#                         Experimenting on Tiny Dataset \n",
    "#                         patch:{patch_size},\n",
    "#                         sparse_att: {sparse_attention},\n",
    "#                         sparse_mlp: {sparse_mlp},\n",
    "#                         num_layers : {nlayers},\n",
    "#                         seed: {seed}\n",
    "#                     ''')\n",
    "            \n",
    "#                 ### check if config is in not_working case\n",
    "#                     exit = False\n",
    "#                     for nw in not_working:\n",
    "#                         if patch_size==nw[0] and \\\n",
    "#                             sparse_attention==nw[1] and \\\n",
    "#                             sparse_mlp==nw[2] and\\\n",
    "#                             nlayers==nw[3]:\n",
    "                            \n",
    "#                             exit=True\n",
    "#                             break\n",
    "#                     if exit:\n",
    "#                         print(f'Exiting as the config is in NOT WORKING')\n",
    "#                         continue\n",
    "\n",
    "\n",
    "#                     benchmark(dataset='tiny', \n",
    "#                               patch_size=patch_size, \n",
    "#                               num_layers=nlayers, \n",
    "#                               SEED=seed, \n",
    "#                               sparse_att=sparse_attention, sparse_mlp=sparse_mlp, \n",
    "#                               cuda=cuda_idx\n",
    "#                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa4e109c-de63-40cf-b46e-084834eacc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Automate the benchmark\n",
    "# ###### for c10\n",
    "\n",
    "# not_working = [\n",
    "# ]\n",
    "\n",
    "# cuda_idx = 0\n",
    "# for dataset in ['cifar10']:\n",
    "#     for seed in [147]:\n",
    "#         for nlayers in [12]:\n",
    "#             for patch_size in [2]:\n",
    "#                 for sparse_attention in [True]:\n",
    "#                     for sparse_mlp in [False]:\n",
    "#                         for PE in [False]:\n",
    "\n",
    "#                             print(f'''\n",
    "#                                 Experimenting on {dataset} Dataset \n",
    "#                                 patch:{patch_size},\n",
    "#                                 sparse_att: {sparse_attention},\n",
    "#                                 sparse_mlp: {sparse_mlp},\n",
    "#                                 num_layers: {nlayers},\n",
    "#                                 pos_embed: {PE},\n",
    "#                                 seed: {seed}\n",
    "#                             ''')\n",
    "\n",
    "#                         ### check if config is in not_working case\n",
    "#                             exit = False\n",
    "#                             for nw in not_working:\n",
    "#                                 if patch_size==nw[0] and \\\n",
    "#                                     sparse_attention==nw[1] and \\\n",
    "#                                     sparse_mlp==nw[2] and\\\n",
    "#                                     nlayers==nw[3]:\n",
    "\n",
    "#                                     exit=True\n",
    "#                                     break\n",
    "#                             if exit:\n",
    "#                                 print(f'Exiting as the config is in NOT WORKING')\n",
    "#                                 continue\n",
    "\n",
    "\n",
    "#                             benchmark(dataset=dataset, \n",
    "#                                       patch_size=patch_size, \n",
    "#                                       num_layers=nlayers, \n",
    "#                                       SEED=seed, \n",
    "#                                       sparse_att=sparse_attention, sparse_mlp=sparse_mlp, \n",
    "#                                       pos_emb=PE,\n",
    "#                                       cuda=cuda_idx\n",
    "#                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59373f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asfdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b3d9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Automate the benchmark\n",
    "# ###### for c100\n",
    "\n",
    "# not_working = [\n",
    "#     (1, False, False, 4),\n",
    "#     (1, False, False, 8),\n",
    "#     (1, False, False, 12),\n",
    "#     (1, False, True, 4),\n",
    "#     (1, False, True, 8),\n",
    "#     (1, False, True, 12),\n",
    "# ]\n",
    "\n",
    "# cuda_idx = 0\n",
    "# dataset = 'cifar100'\n",
    "# # for seed in [147, 258, 369]:\n",
    "# for seed in [147]:\n",
    "#     for patch_size in [1, 2, 4, 8]:\n",
    "#         for sparse_attention in [False, True]:\n",
    "#             for sparse_mlp in [False, True]:\n",
    "#                 for nlayers in [4, 8, 12]:\n",
    "\n",
    "#                     print(f'''\n",
    "#                         Experimenting on {dataset} Dataset \n",
    "#                         patch:{patch_size},\n",
    "#                         sparse_att: {sparse_attention},\n",
    "#                         sparse_mlp: {sparse_mlp},\n",
    "#                         num_layers : {nlayers},\n",
    "#                         seed: {seed}\n",
    "#                     ''')\n",
    "            \n",
    "#                 ### check if config is in not_working case\n",
    "#                     exit = False\n",
    "#                     for nw in not_working:\n",
    "#                         if patch_size==nw[0] and \\\n",
    "#                             sparse_attention==nw[1] and \\\n",
    "#                             sparse_mlp==nw[2] and\\\n",
    "#                             nlayers==nw[3]:\n",
    "                            \n",
    "#                             exit=True\n",
    "#                             break\n",
    "#                     if exit:\n",
    "#                         print(f'Exiting as the config is in NOT WORKING')\n",
    "#                         continue\n",
    "\n",
    "\n",
    "#                     benchmark(dataset=dataset, \n",
    "#                               patch_size=patch_size, \n",
    "#                               num_layers=nlayers, \n",
    "#                               SEED=seed, \n",
    "#                               sparse_att=sparse_attention, sparse_mlp=sparse_mlp, \n",
    "#                               cuda=cuda_idx\n",
    "#                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2f64de7",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'train_dataset' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_423930/2732260213.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mSEED\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m147\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0msparse_att\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_mlp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m           \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m          )\n",
      "\u001b[0;32m/tmp/ipykernel_423930/1396521468.py\u001b[0m in \u001b[0;36mbenchmark\u001b[0;34m(dataset, patch_size, num_layers, SEED, sparse_att, sparse_mlp, pos_emb, cuda)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m##### Now create data loaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'train_dataset' referenced before assignment"
     ]
    }
   ],
   "source": [
    "### Automate the benchmark\n",
    "###### for c100\n",
    "\n",
    "benchmark(dataset='c100', \n",
    "          patch_size=1, \n",
    "          num_layers=4, \n",
    "          SEED=147, \n",
    "          sparse_att=True, sparse_mlp=False, \n",
    "          cuda=1,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c0465-3702-4a9b-b3eb-3b642926dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## c10 works on all patch sizes on 3090 using 64 batch size\n",
    "## c100 doesn't work on patch 1 for dense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63adc707-1309-4719-9764-eca72cdd1f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdsad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca20622e",
   "metadata": {},
   "source": [
    "## Benchmark Memory and Time CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faebb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvidia_smi\n",
    "\n",
    "MB = 1024*1024\n",
    "def get_memory_used():\n",
    "    nvidia_smi.nvmlInit()\n",
    "    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(1)\n",
    "    info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "    val = info.used/MB\n",
    "    nvidia_smi.nvmlShutdown()\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1812db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_memory_used()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7fa91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "def get_memory_used():\n",
    "    command = \"nvidia-smi --query-gpu=memory.used --format=csv\"\n",
    "    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    return memory_free_values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30adc9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_memory_used()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d9cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asdfsadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7429fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_memory(dataset:str, patch_size:int, num_layers:int, SEED:int, sparse_att:bool=False, sparse_mlp:bool=False, pos_emb:bool=False, cuda:int=0):\n",
    "    device = torch.device(f\"cuda:{cuda}\")\n",
    "    \n",
    "    if sparse_att:\n",
    "        assert num_layers%2 == 0, 'number of blocks on sparse transformer is (x2)/2 hence it must be even'\n",
    "        num_layers_ = num_layers//2\n",
    "    else:\n",
    "        num_layers_ = num_layers\n",
    "    \n",
    "    BS = 32\n",
    "    NC = -1\n",
    "    EPOCHS = 1\n",
    "    imsize = (3, 32, 32)\n",
    "    expansion_dict = {16:1024, 8:256, 4:128, 2:64, 1:64}\n",
    "    expansion = expansion_dict[patch_size]\n",
    "\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    ##### Data Transforms\n",
    "        \n",
    "    if dataset == 'cifar10':\n",
    "        NC = 10\n",
    "    elif dataset == 'cifar100':\n",
    "        NC = 100\n",
    "\n",
    "    ### Now create models\n",
    "    \n",
    "    seq_len = (imsize[-1]*imsize[-2])//(patch_size*patch_size)\n",
    "    mlp_dim = expansion\n",
    "    print(seq_len, mlp_dim)\n",
    "    \n",
    "    if sparse_att:\n",
    "        seq_len = int(2**np.ceil(np.log2(np.sqrt(seq_len))))\n",
    "    if sparse_mlp:\n",
    "        mlp_dim = int(2**np.ceil(np.log2(np.sqrt(expansion))))\n",
    "    \n",
    "    mem_begin = get_memory_used()\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    model = Mixer_ViT_Classifier(imsize, \n",
    "                                 patch_size=[patch_size]*2, \n",
    "                                 hidden_channel=expansion, \n",
    "                                 num_blocks=num_layers_, \n",
    "                                 num_classes=NC, \n",
    "                                 block_seq_size=seq_len, \n",
    "                                 block_mlp_size=mlp_dim,\n",
    "                                 pos_emb=pos_emb).to(device)\n",
    "    \n",
    "    _x = torch.randn(BS, *imsize)#.to(device)\n",
    "    _y = torch.randint(10, (BS,))\n",
    "#     print(\"Output: \",vit_mixer(_x).shape)\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"number of params: \", num_params)\n",
    "    \n",
    "    _a, _b, _c = 'att', 'mlp', 'nPE'\n",
    "    if sparse_att: _a = 'sAtt'\n",
    "    if sparse_mlp: _b = 'sMlp'\n",
    "    if pos_emb: _c = 'PE'\n",
    "    model_name = f'01.3_ViT_{_c}_{dataset}_patch{patch_size}_l{num_layers}_{_a}_{_b}_s{SEED}'\n",
    "    print(f\"Model Name: {model_name}\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    inputs, targets = _x.to(device), _y.to(device)\n",
    "    ### test time taken for multiple iterations\n",
    "    time_taken = []\n",
    "    for i in range(50):\n",
    "        start = time.time()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        start = time.time()-start\n",
    "        time_taken.append(start)\n",
    "    train_time = np.mean(time_taken)\n",
    "        \n",
    "    mem_end = get_memory_used()\n",
    "    print(f\"mem begin: {mem_begin}  end: {mem_end}\")\n",
    "\n",
    "    model.eval()\n",
    "    time_taken = []\n",
    "    for i in range(50):\n",
    "        with torch.no_grad():\n",
    "            start = time.time()\n",
    "            outputs = model(inputs)\n",
    "            start = time.time()-start\n",
    "            time_taken.append(start)\n",
    "            \n",
    "    test_time = np.mean(time_taken)\n",
    "    \n",
    "    filename = f\"./output/benchmark_memory_data.json\"\n",
    "    if not os.path.exists(filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump({}, f, indent=0)\n",
    "            \n",
    "    with open(filename, 'r+') as f:\n",
    "#         with open(filename,'r+') as f:\n",
    "        file_data = json.load(f)\n",
    "        file_data[f\"{model_name}\"] = {'memory':mem_end-mem_begin, 'time_train':train_time, 'time_test':test_time}\n",
    "        f.seek(0)\n",
    "        json.dump(file_data, f, indent = 0)\n",
    "\n",
    "    del model, optimizer\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8dbad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Automate the benchmark\n",
    "# ###### for c10\n",
    "\n",
    "# not_working = [\n",
    "# ]\n",
    "\n",
    "# cuda_idx = 0\n",
    "# seed = 147\n",
    "# PE = False\n",
    "# for dataset in ['cifar10']:\n",
    "#     for nlayers in [8, 4]:\n",
    "#         for patch_size in [2, 4]:\n",
    "#             for sparse_attention in [False, True]:\n",
    "#                 for sparse_mlp in [False]:\n",
    "\n",
    "#                     print(f'''\n",
    "#                         Experimenting on {dataset} Dataset \n",
    "#                         patch:{patch_size},\n",
    "#                         sparse_att: {sparse_attention},\n",
    "#                         sparse_mlp: {sparse_mlp},\n",
    "#                         num_layers: {nlayers},\n",
    "#                         pos_embed: {PE},\n",
    "#                         seed: {seed}\n",
    "#                     ''')\n",
    "\n",
    "#                     benchmark_memory(dataset=dataset, \n",
    "#                               patch_size=patch_size, \n",
    "#                               num_layers=nlayers, \n",
    "#                               SEED=seed, \n",
    "#                               sparse_att=sparse_attention, sparse_mlp=sparse_mlp, \n",
    "#                               pos_emb=PE,\n",
    "#                               cuda=cuda_idx\n",
    "#                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0e54bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_memory(dataset='cifar100', \n",
    "                  patch_size=2, \n",
    "                  num_layers=4, \n",
    "                  SEED=147, \n",
    "                  sparse_att=True, \n",
    "                  sparse_mlp=False, \n",
    "                  pos_emb=False,\n",
    "                  cuda=0\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675693c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
