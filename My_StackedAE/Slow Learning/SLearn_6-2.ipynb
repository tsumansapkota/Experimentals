{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-b22782e32495>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE parameters\n",
    "num_label = 10\n",
    "label_hid1 = 5\n",
    "label_hid2 = 3\n",
    "label_hid3 = 2\n",
    "label_hid4 = 1\n",
    "\n",
    "num_image = 784 # 28*28\n",
    "image_hid1 = 392\n",
    "image_hid2 = 196\n",
    "image_hid3 = 98\n",
    "image_hid4 = 49\n",
    "image_hid5 = 25\n",
    "image_hid6 = 12\n",
    "image_hid7 = 6\n",
    "image_hid8 = 3\n",
    "image_hid9 = 2\n",
    "image_hid10 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tie_weight = tf.transpose\n",
    "act_func = lambda X:X\n",
    "initializer = tf.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLACEHOLDER DEFINATION\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_image])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, num_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Encoded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMAGE COMPRESSION\n",
    "wi1 = tf.Variable(np.load('../save/weights/w1.npy'))\n",
    "wi2 = tf.Variable(np.load('../save/weights/w2.npy'))\n",
    "wi3 = tf.Variable(np.load('../save/weights/w3.npy'))\n",
    "wi4 = tf.Variable(np.load('../save/weights/w4.npy'))\n",
    "wi5 = tf.Variable(np.load('../save/weights/w5.npy'))\n",
    "wi6 = tf.Variable(np.load('../save/weights/w6.npy'))\n",
    "wi7 = tf.Variable(np.load('../save/weights/w7.npy'))\n",
    "wi8 = tf.Variable(np.load('../save/weights/w8.npy'))\n",
    "wi9 = tf.Variable(np.load('../save/weights/w9.npy'))\n",
    "wi10 = tf.Variable(np.load('../save/weights/w10.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_layer1 = act_func(tf.matmul(X, wi1))\n",
    "img_layer2 = act_func(tf.matmul(img_layer1, wi2))\n",
    "img_layer3 = act_func(tf.matmul(img_layer2, wi3))\n",
    "img_layer4 = act_func(tf.matmul(img_layer3, wi4))\n",
    "img_layer5 = act_func(tf.matmul(img_layer4, wi5))\n",
    "img_layer6 = act_func(tf.matmul(img_layer5, wi6))\n",
    "img_layer7 = act_func(tf.matmul(img_layer6, wi7))\n",
    "img_layer8 = act_func(tf.matmul(img_layer7, wi8))\n",
    "img_layer9 = act_func(tf.matmul(img_layer8, wi9))\n",
    "img_layer10 = act_func(tf.matmul(img_layer9, wi10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LABEL DECOMPRESSION\n",
    "wl1 = tf.Variable(np.load('../save/weights/lw1.npy'))\n",
    "wl2 = tf.Variable(np.load('../save/weights/lw2.npy'))\n",
    "wl3 = tf.Variable(np.load('../save/weights/lw3.npy'))\n",
    "wl4 = tf.Variable(np.load('../save/weights/lw4.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_layer1 = act_func(tf.matmul(Y, wl1))\n",
    "lbl_layer2 = act_func(tf.matmul(lbl_layer1, wl2))\n",
    "lbl_layer3 = act_func(tf.matmul(lbl_layer2, wl3))\n",
    "lbl_layer4 = act_func(tf.matmul(lbl_layer3, wl4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Parameters for activation learning\n",
    "initializer = tf.random_uniform_initializer()\n",
    "\n",
    "# input_no = image_hid8\n",
    "# output_no = label_hid3\n",
    "\n",
    "inpt = img_layer7\n",
    "yval = lbl_layer3\n",
    "\n",
    "i=3\n",
    "j=2\n",
    "\n",
    "pW1 = tf.Variable(np.load('../save/weights/slearn_parms0_{}-{}.npy'.format(i, j)))\n",
    "W1 = tf.Variable(tf.matmul(wi8,pW1))\n",
    "W2 = tf.Variable(np.load('../save/weights/slearn_parms1_{}-{}.npy'.format(i, j)))\n",
    "W3 = tf.Variable(np.load('../save/weights/slearn_parms2_{}-{}.npy'.format(i, j)))\n",
    "# W3 = tf.Variable(tf.matmul(pW3, tf.transpose(wl4)))\n",
    "\n",
    "b1 = tf.Variable(np.load('../save/weights/slearn_parms3_{}-{}.npy'.format(i, j)))\n",
    "b2 = tf.Variable(np.load('../save/weights/slearn_parms4_{}-{}.npy'.format(i, j)))\n",
    "b3 = tf.Variable(np.load('../save/weights/slearn_parms5_{}-{}.npy'.format(i, j)))\n",
    "# b3 = tf.Variable(tf.matmul(pb3,tf.transpose(wl4)))\n",
    "\n",
    "a1 = tf.nn.relu(tf.matmul(inpt, W1) + b1)\n",
    "a2 = tf.nn.relu(tf.matmul(a1, W2) + b2)\n",
    "a3 = tf.matmul(a2, W3) + b3\n",
    "\n",
    "output = a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_43:0' shape=(6, 155) dtype=float32_ref>\n",
      "Tensor(\"add_14:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(W1)\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Encode-Decode Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_layer1 = tf.matmul(output,tf.transpose(wl4))\n",
    "out_layer2 = tf.matmul(output,tf.transpose(wl3))\n",
    "out_layer3 = tf.matmul(out_layer2,tf.transpose(wl2))\n",
    "out_layer4 = tf.matmul(out_layer3,tf.transpose(wl1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTIONS\n",
    "loss = tf.reduce_mean(tf.square(output - yval))\n",
    "# loss = tf.reduce_mean(tf.abs(output_layer - X))\n",
    "# cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y,logits=yout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss, var_list=[W1, W2, W3, b1, b2, b3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARATION\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Complete. Training Loss: 14.824981689453125\n",
      "TEST ACCURACY: \n",
      "0.4207\n",
      "TRAIN ACCURACY: \n",
      "0.41703635\n",
      "\n",
      "Epoch 1 Complete. Training Loss: 15.448415756225586\n",
      "TEST ACCURACY: \n",
      "0.4304\n",
      "\n",
      "Epoch 2 Complete. Training Loss: 10.972615242004395\n",
      "TEST ACCURACY: \n",
      "0.4453\n",
      "\n",
      "Epoch 3 Complete. Training Loss: 11.025583267211914\n",
      "TEST ACCURACY: \n",
      "0.4847\n",
      "\n",
      "Epoch 4 Complete. Training Loss: 14.187256813049316\n",
      "TEST ACCURACY: \n",
      "0.5007\n",
      "\n",
      "Epoch 5 Complete. Training Loss: 12.967081069946289\n",
      "TEST ACCURACY: \n",
      "0.5297\n",
      "TRAIN ACCURACY: \n",
      "0.5236909\n",
      "\n",
      "Epoch 6 Complete. Training Loss: 15.003914833068848\n",
      "TEST ACCURACY: \n",
      "0.5354\n",
      "\n",
      "Epoch 7 Complete. Training Loss: 14.381705284118652\n",
      "TEST ACCURACY: \n",
      "0.555\n",
      "\n",
      "Epoch 8 Complete. Training Loss: 7.482082843780518\n",
      "TEST ACCURACY: \n",
      "0.5615\n",
      "\n",
      "Epoch 9 Complete. Training Loss: 10.604217529296875\n",
      "TEST ACCURACY: \n",
      "0.5665\n",
      "\n",
      "Epoch 10 Complete. Training Loss: 11.52511215209961\n",
      "TEST ACCURACY: \n",
      "0.5723\n",
      "TRAIN ACCURACY: \n",
      "0.5681818\n",
      "\n",
      "Epoch 11 Complete. Training Loss: 11.272902488708496\n",
      "TEST ACCURACY: \n",
      "0.5806\n",
      "\n",
      "Epoch 12 Complete. Training Loss: 12.182841300964355\n",
      "TEST ACCURACY: \n",
      "0.5827\n",
      "\n",
      "Epoch 13 Complete. Training Loss: 10.863724708557129\n",
      "TEST ACCURACY: \n",
      "0.5904\n",
      "\n",
      "Epoch 14 Complete. Training Loss: 8.723493576049805\n",
      "TEST ACCURACY: \n",
      "0.5926\n",
      "\n",
      "Epoch 15 Complete. Training Loss: 10.210739135742188\n",
      "TEST ACCURACY: \n",
      "0.5903\n",
      "TRAIN ACCURACY: \n",
      "0.58794546\n",
      "\n",
      "Epoch 16 Complete. Training Loss: 7.768589496612549\n",
      "TEST ACCURACY: \n",
      "0.5969\n",
      "\n",
      "Epoch 17 Complete. Training Loss: 10.39490032196045\n",
      "TEST ACCURACY: \n",
      "0.5989\n",
      "\n",
      "Epoch 18 Complete. Training Loss: 9.979032516479492\n",
      "TEST ACCURACY: \n",
      "0.6023\n",
      "\n",
      "Epoch 19 Complete. Training Loss: 9.521987915039062\n",
      "TEST ACCURACY: \n",
      "0.6081\n",
      "\n",
      "Epoch 20 Complete. Training Loss: 13.193077087402344\n",
      "TEST ACCURACY: \n",
      "0.6089\n",
      "TRAIN ACCURACY: \n",
      "0.60803634\n",
      "\n",
      "Epoch 21 Complete. Training Loss: 9.536751747131348\n",
      "TEST ACCURACY: \n",
      "0.6094\n",
      "\n",
      "Epoch 22 Complete. Training Loss: 9.15234375\n",
      "TEST ACCURACY: \n",
      "0.6108\n",
      "\n",
      "Epoch 23 Complete. Training Loss: 9.173871994018555\n",
      "TEST ACCURACY: \n",
      "0.6097\n",
      "\n",
      "Epoch 24 Complete. Training Loss: 10.756711959838867\n",
      "TEST ACCURACY: \n",
      "0.6149\n",
      "\n",
      "Epoch 25 Complete. Training Loss: 10.826855659484863\n",
      "TEST ACCURACY: \n",
      "0.6179\n",
      "TRAIN ACCURACY: \n",
      "0.61727273\n",
      "\n",
      "Epoch 26 Complete. Training Loss: 7.279977798461914\n",
      "TEST ACCURACY: \n",
      "0.6152\n",
      "\n",
      "Epoch 27 Complete. Training Loss: 10.256762504577637\n",
      "TEST ACCURACY: \n",
      "0.6249\n",
      "\n",
      "Epoch 28 Complete. Training Loss: 8.519247055053711\n",
      "TEST ACCURACY: \n",
      "0.6236\n",
      "\n",
      "Epoch 29 Complete. Training Loss: 8.550884246826172\n",
      "TEST ACCURACY: \n",
      "0.6302\n",
      "\n",
      "Epoch 30 Complete. Training Loss: 8.323158264160156\n",
      "TEST ACCURACY: \n",
      "0.631\n",
      "TRAIN ACCURACY: \n",
      "0.6286727\n",
      "\n",
      "Epoch 31 Complete. Training Loss: 10.313407897949219\n",
      "TEST ACCURACY: \n",
      "0.6232\n",
      "\n",
      "Epoch 32 Complete. Training Loss: 11.217009544372559\n",
      "TEST ACCURACY: \n",
      "0.6286\n",
      "\n",
      "Epoch 33 Complete. Training Loss: 7.945006847381592\n",
      "TEST ACCURACY: \n",
      "0.6274\n",
      "\n",
      "Epoch 34 Complete. Training Loss: 9.376397132873535\n",
      "TEST ACCURACY: \n",
      "0.6414\n",
      "\n",
      "Epoch 35 Complete. Training Loss: 9.15791130065918\n",
      "TEST ACCURACY: \n",
      "0.6403\n",
      "TRAIN ACCURACY: \n",
      "0.63067275\n",
      "\n",
      "Epoch 36 Complete. Training Loss: 9.672998428344727\n",
      "TEST ACCURACY: \n",
      "0.6437\n",
      "\n",
      "Epoch 37 Complete. Training Loss: 9.931547164916992\n",
      "TEST ACCURACY: \n",
      "0.6439\n",
      "\n",
      "Epoch 38 Complete. Training Loss: 7.961172103881836\n",
      "TEST ACCURACY: \n",
      "0.6475\n",
      "\n",
      "Epoch 39 Complete. Training Loss: 8.706562995910645\n",
      "TEST ACCURACY: \n",
      "0.649\n",
      "\n",
      "Epoch 40 Complete. Training Loss: 8.886590957641602\n",
      "TEST ACCURACY: \n",
      "0.6426\n",
      "TRAIN ACCURACY: \n",
      "0.6361455\n",
      "\n",
      "Epoch 41 Complete. Training Loss: 9.330698013305664\n",
      "TEST ACCURACY: \n",
      "0.6546\n",
      "\n",
      "Epoch 42 Complete. Training Loss: 8.440754890441895\n",
      "TEST ACCURACY: \n",
      "0.651\n",
      "\n",
      "Epoch 43 Complete. Training Loss: 7.615724563598633\n",
      "TEST ACCURACY: \n",
      "0.6548\n",
      "\n",
      "Epoch 44 Complete. Training Loss: 8.958610534667969\n",
      "TEST ACCURACY: \n",
      "0.6569\n",
      "\n",
      "Epoch 45 Complete. Training Loss: 7.673171520233154\n",
      "TEST ACCURACY: \n",
      "0.6529\n",
      "TRAIN ACCURACY: \n",
      "0.6451091\n",
      "\n",
      "Epoch 46 Complete. Training Loss: 10.170663833618164\n",
      "TEST ACCURACY: \n",
      "0.6616\n",
      "\n",
      "Epoch 47 Complete. Training Loss: 7.213371753692627\n",
      "TEST ACCURACY: \n",
      "0.6642\n",
      "\n",
      "Epoch 48 Complete. Training Loss: 6.4638776779174805\n",
      "TEST ACCURACY: \n",
      "0.6652\n",
      "\n",
      "Epoch 49 Complete. Training Loss: 9.745004653930664\n",
      "TEST ACCURACY: \n",
      "0.6673\n",
      "\n",
      "Epoch 50 Complete. Training Loss: 9.764904022216797\n",
      "TEST ACCURACY: \n",
      "0.6655\n",
      "TRAIN ACCURACY: \n",
      "0.6574909\n",
      "\n",
      "Epoch 51 Complete. Training Loss: 8.512968063354492\n",
      "TEST ACCURACY: \n",
      "0.6659\n",
      "\n",
      "Epoch 52 Complete. Training Loss: 5.763404846191406\n",
      "TEST ACCURACY: \n",
      "0.6679\n",
      "\n",
      "Epoch 53 Complete. Training Loss: 7.851551055908203\n",
      "TEST ACCURACY: \n",
      "0.6715\n",
      "\n",
      "Epoch 54 Complete. Training Loss: 7.95668363571167\n",
      "TEST ACCURACY: \n",
      "0.6677\n",
      "\n",
      "Epoch 55 Complete. Training Loss: 8.453468322753906\n",
      "TEST ACCURACY: \n",
      "0.667\n",
      "TRAIN ACCURACY: \n",
      "0.6577455\n",
      "\n",
      "Epoch 56 Complete. Training Loss: 7.139350891113281\n",
      "TEST ACCURACY: \n",
      "0.666\n",
      "\n",
      "Epoch 57 Complete. Training Loss: 8.902565002441406\n",
      "TEST ACCURACY: \n",
      "0.676\n",
      "\n",
      "Epoch 58 Complete. Training Loss: 9.335067749023438\n",
      "TEST ACCURACY: \n",
      "0.6771\n",
      "\n",
      "Epoch 59 Complete. Training Loss: 8.553238868713379\n",
      "TEST ACCURACY: \n",
      "0.6786\n",
      "\n",
      "Epoch 60 Complete. Training Loss: 9.206583976745605\n",
      "TEST ACCURACY: \n",
      "0.6774\n",
      "TRAIN ACCURACY: \n",
      "0.67103636\n",
      "\n",
      "Epoch 61 Complete. Training Loss: 6.329354763031006\n",
      "TEST ACCURACY: \n",
      "0.6758\n",
      "\n",
      "Epoch 62 Complete. Training Loss: 8.518924713134766\n",
      "TEST ACCURACY: \n",
      "0.6778\n",
      "\n",
      "Epoch 63 Complete. Training Loss: 7.490777969360352\n",
      "TEST ACCURACY: \n",
      "0.682\n",
      "\n",
      "Epoch 64 Complete. Training Loss: 7.516285419464111\n",
      "TEST ACCURACY: \n",
      "0.6841\n",
      "\n",
      "Epoch 65 Complete. Training Loss: 8.677010536193848\n",
      "TEST ACCURACY: \n",
      "0.6755\n",
      "TRAIN ACCURACY: \n",
      "0.67294544\n",
      "\n",
      "Epoch 66 Complete. Training Loss: 10.771870613098145\n",
      "TEST ACCURACY: \n",
      "0.6825\n",
      "\n",
      "Epoch 67 Complete. Training Loss: 8.241874694824219\n",
      "TEST ACCURACY: \n",
      "0.684\n",
      "\n",
      "Epoch 68 Complete. Training Loss: 7.809128761291504\n",
      "TEST ACCURACY: \n",
      "0.6847\n",
      "\n",
      "Epoch 69 Complete. Training Loss: 8.412288665771484\n",
      "TEST ACCURACY: \n",
      "0.6815\n",
      "\n",
      "Epoch 70 Complete. Training Loss: 7.710587978363037\n",
      "TEST ACCURACY: \n",
      "0.6852\n",
      "TRAIN ACCURACY: \n",
      "0.6798364\n",
      "\n",
      "Epoch 71 Complete. Training Loss: 7.901886940002441\n",
      "TEST ACCURACY: \n",
      "0.6885\n",
      "\n",
      "Epoch 72 Complete. Training Loss: 6.634998321533203\n",
      "TEST ACCURACY: \n",
      "0.6877\n",
      "\n",
      "Epoch 73 Complete. Training Loss: 8.32214641571045\n",
      "TEST ACCURACY: \n",
      "0.6871\n",
      "\n",
      "Epoch 74 Complete. Training Loss: 7.2339091300964355\n",
      "TEST ACCURACY: \n",
      "0.6893\n",
      "\n",
      "Epoch 75 Complete. Training Loss: 8.1581449508667\n",
      "TEST ACCURACY: \n",
      "0.6869\n",
      "TRAIN ACCURACY: \n",
      "0.68363637\n",
      "\n",
      "Epoch 76 Complete. Training Loss: 8.301440238952637\n",
      "TEST ACCURACY: \n",
      "0.6915\n",
      "\n",
      "Epoch 77 Complete. Training Loss: 8.506123542785645\n",
      "TEST ACCURACY: \n",
      "0.6894\n",
      "\n",
      "Epoch 78 Complete. Training Loss: 9.901288986206055\n",
      "TEST ACCURACY: \n",
      "0.6878\n",
      "\n",
      "Epoch 79 Complete. Training Loss: 8.248976707458496\n",
      "TEST ACCURACY: \n",
      "0.6872\n",
      "\n",
      "Epoch 80 Complete. Training Loss: 6.980287075042725\n",
      "TEST ACCURACY: \n",
      "0.6915\n",
      "TRAIN ACCURACY: \n",
      "0.68972725\n",
      "\n",
      "Epoch 81 Complete. Training Loss: 8.008212089538574\n",
      "TEST ACCURACY: \n",
      "0.6928\n",
      "\n",
      "Epoch 82 Complete. Training Loss: 9.559974670410156\n",
      "TEST ACCURACY: \n",
      "0.6895\n",
      "\n",
      "Epoch 83 Complete. Training Loss: 7.889449119567871\n",
      "TEST ACCURACY: \n",
      "0.689\n",
      "\n",
      "Epoch 84 Complete. Training Loss: 6.978959560394287\n",
      "TEST ACCURACY: \n",
      "0.6922\n",
      "\n",
      "Epoch 85 Complete. Training Loss: 7.772233486175537\n",
      "TEST ACCURACY: \n",
      "0.6874\n",
      "TRAIN ACCURACY: \n",
      "0.68578184\n",
      "\n",
      "Epoch 86 Complete. Training Loss: 7.36335563659668\n",
      "TEST ACCURACY: \n",
      "0.6904\n",
      "\n",
      "Epoch 87 Complete. Training Loss: 8.145203590393066\n",
      "TEST ACCURACY: \n",
      "0.6928\n",
      "\n",
      "Epoch 88 Complete. Training Loss: 6.8158769607543945\n",
      "TEST ACCURACY: \n",
      "0.6932\n",
      "\n",
      "Epoch 89 Complete. Training Loss: 8.51733112335205\n",
      "TEST ACCURACY: \n",
      "0.6934\n",
      "\n",
      "Epoch 90 Complete. Training Loss: 8.963891983032227\n",
      "TEST ACCURACY: \n",
      "0.6944\n",
      "TRAIN ACCURACY: \n",
      "0.6912182\n",
      "\n",
      "Epoch 91 Complete. Training Loss: 7.550092697143555\n",
      "TEST ACCURACY: \n",
      "0.6938\n",
      "\n",
      "Epoch 92 Complete. Training Loss: 7.294692039489746\n",
      "TEST ACCURACY: \n",
      "0.692\n",
      "\n",
      "Epoch 93 Complete. Training Loss: 6.647825241088867\n",
      "TEST ACCURACY: \n",
      "0.6976\n",
      "\n",
      "Epoch 94 Complete. Training Loss: 7.954516410827637\n",
      "TEST ACCURACY: \n",
      "0.6961\n",
      "\n",
      "Epoch 95 Complete. Training Loss: 7.867325305938721\n",
      "TEST ACCURACY: \n",
      "0.6975\n",
      "TRAIN ACCURACY: \n",
      "0.69325453\n",
      "\n",
      "Epoch 96 Complete. Training Loss: 7.564276218414307\n",
      "TEST ACCURACY: \n",
      "0.6967\n",
      "\n",
      "Epoch 97 Complete. Training Loss: 9.725399017333984\n",
      "TEST ACCURACY: \n",
      "0.6973\n",
      "\n",
      "Epoch 98 Complete. Training Loss: 7.450494766235352\n",
      "TEST ACCURACY: \n",
      "0.7004\n",
      "\n",
      "Epoch 99 Complete. Training Loss: 7.650641918182373\n",
      "TEST ACCURACY: \n",
      "0.6984\n",
      "\n",
      "Finished Training the Model\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "try:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        # Epoch == Entire Training Set\n",
    "        for epoch in range(num_epochs):\n",
    "            num_batches = mnist.train.num_examples // batch_size\n",
    "\n",
    "            # 150 batch size\n",
    "            for iteration in range(num_batches):\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_size+3*epoch)\n",
    "                sess.run(train, feed_dict={X: X_batch, Y: y_batch})\n",
    "\n",
    "            training_loss = loss.eval(feed_dict={X: X_batch, Y: y_batch})   \n",
    "            print(\"Epoch {} Complete. Training Loss: {}\".format(epoch,training_loss))\n",
    "            saver.save(sess, \"../save/slearn_6-2.ckpt\")\n",
    "            \n",
    "            matches = tf.equal(tf.argmax(out_layer4, 1), tf.argmax(Y,1))\n",
    "            acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "\n",
    "            print('TEST ACCURACY: ')\n",
    "            print (sess.run(acc, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))\n",
    "            if epoch % 5 == 0:\n",
    "                print('TRAIN ACCURACY: ')\n",
    "                print (sess.run(acc, feed_dict={X:mnist.train.images, Y:mnist.train.labels}))\n",
    "            print()\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print('Keyboard Interrupted')\n",
    "finally:\n",
    "    print('Finished Training the Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../save/slearn_6-2.ckpt\n",
      "TRAIN ACCURACY: \n",
      "0.69394547\n",
      "TEST ACCURACY: \n",
      "0.6984\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_test_labels = 10\n",
    "start_point = 15\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "#     sess.run(init)\n",
    "    saver.restore(sess,\"../save/slearn_6-2.ckpt\")\n",
    "    \n",
    "    params = sess.run([W1, W2, W3, b1, b2, b3])\n",
    "    \n",
    "    matches = tf.equal(tf.argmax(out_layer4, 1), tf.argmax(Y,1))\n",
    "    acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "    \n",
    "    print('TRAIN ACCURACY: ')\n",
    "    print (sess.run(acc, feed_dict={X:mnist.train.images, Y:mnist.train.labels}))\n",
    "\n",
    "    print('TEST ACCURACY: ')\n",
    "    print (sess.run(acc, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 6.27279431e-02,  6.97199553e-02,  3.36577222e-02,\n",
       "          2.14120522e-01,  5.89018799e-02,  3.29146802e-01,\n",
       "          1.98967636e-01,  6.93862885e-03,  4.93403524e-04,\n",
       "          1.40144870e-01,  1.01631790e-01,  4.69973534e-02,\n",
       "          1.36196688e-02,  6.91744015e-02,  2.37899385e-02,\n",
       "          7.34490976e-02,  2.81018186e-02,  1.51729584e-02,\n",
       "          2.37421647e-01,  1.83578078e-02,  5.27659431e-02,\n",
       "          4.13591042e-02,  5.25669716e-02,  7.68910721e-02,\n",
       "          4.76556011e-02,  4.19994816e-02, -6.85101867e-01,\n",
       "         -7.48948634e-01,  8.76460597e-02,  5.73289115e-03,\n",
       "          6.75919205e-02,  7.41538629e-02,  3.54237109e-02,\n",
       "          4.46166769e-02,  3.15570869e-02,  3.59759084e-03,\n",
       "          2.35630628e-02,  3.30958784e-01,  7.37297237e-02,\n",
       "          2.04122126e-01,  1.38268946e-02,  4.51459177e-02,\n",
       "          2.33768675e-04,  2.25564465e-02,  2.37490516e-02,\n",
       "          7.18264803e-02,  1.04620177e-02,  2.48926431e-02,\n",
       "          4.41824980e-02,  6.30536899e-02,  3.85236405e-02,\n",
       "          6.84295148e-02,  5.64012229e-01, -1.72495842e-01,\n",
       "          3.04017421e-02,  1.56584978e-02,  3.75987142e-02,\n",
       "          4.14332449e-02,  6.70411065e-02, -8.88004363e-01,\n",
       "          5.10849282e-02, -8.13399434e-01,  4.40853350e-02,\n",
       "          5.21532409e-02,  5.24915829e-02,  6.15046173e-02,\n",
       "          7.45566115e-02,  3.03038627e-01,  6.93668500e-02,\n",
       "          6.81728721e-02,  1.78238243e-01,  7.64758065e-02,\n",
       "          5.28183505e-02,  4.13193889e-02,  2.00935565e-02,\n",
       "          4.02266486e-03,  4.17002812e-02, -6.05646491e-01,\n",
       "          5.39184697e-02, -3.19513798e-01,  4.26802039e-02,\n",
       "          2.93477457e-02,  3.53849322e-01,  4.20311093e-02,\n",
       "          5.65715060e-02,  7.18106776e-02,  5.30468449e-02,\n",
       "          3.68554667e-02,  4.15683314e-02,  5.47224544e-02,\n",
       "          4.04368853e-03,  7.07960576e-02,  5.60822710e-02,\n",
       "          2.42605135e-02, -1.85263664e-01,  3.83245237e-02,\n",
       "          3.81909020e-04,  3.22549753e-02,  7.01455921e-02,\n",
       "         -9.77701664e-01,  6.67187721e-02,  2.55024843e-02,\n",
       "          2.38055110e-01,  1.19464807e-01,  3.75244617e-02,\n",
       "         -1.01421326e-02,  3.51758987e-01,  5.77659570e-02,\n",
       "          2.28680149e-02,  1.34557057e-02, -3.49592179e-01,\n",
       "          2.83572674e-02,  3.77425849e-02,  2.99139172e-02,\n",
       "          1.91986859e-01, -6.84086323e-01, -1.54503718e-01,\n",
       "          4.95748043e-01, -4.01750728e-02,  4.12734374e-02,\n",
       "          5.73534258e-02,  4.81274389e-02,  1.95133284e-01,\n",
       "         -6.40193447e-02,  3.87509987e-02,  8.68878141e-03,\n",
       "          2.14283854e-01,  2.41496675e-02,  4.66756150e-02,\n",
       "          1.84660748e-01,  3.41823697e-02,  3.05936076e-02,\n",
       "          9.58733037e-02,  1.83236971e-02,  1.06277987e-02,\n",
       "          2.04411790e-01,  3.73646319e-02,  2.84971073e-02,\n",
       "         -1.36319563e-01,  6.74672127e-02,  4.44693156e-02,\n",
       "          4.43600491e-02,  4.04446423e-02,  1.69686988e-01,\n",
       "          3.34634371e-02,  6.82806820e-02,  4.90549505e-02,\n",
       "          4.85288724e-02, -1.97570011e-01,  6.98383898e-02,\n",
       "          1.89063158e-02,  7.48453569e-03, -6.25682399e-02,\n",
       "          2.19816267e-02,  7.14364499e-02],\n",
       "        [ 9.45488065e-02,  1.05087742e-01,  5.07317334e-02,\n",
       "          4.65521187e-01,  8.87818336e-02, -1.44181773e-01,\n",
       "         -1.88408226e-01,  1.04584815e-02,  7.43698911e-04,\n",
       "         -4.24811617e-02,  3.28007847e-01,  7.08383471e-02,\n",
       "          2.05287039e-02,  1.04265444e-01,  3.58581841e-02,\n",
       "          2.40934361e-03,  4.23574112e-02,  2.28699520e-02,\n",
       "          3.21066916e-01,  2.76704244e-02,  7.95332417e-02,\n",
       "          6.23399019e-02,  7.92333186e-02,  1.15896650e-01,\n",
       "          7.18305036e-02,  6.33051246e-02,  7.41325498e-01,\n",
       "          3.87056231e-01,  2.57869720e-01,  8.64109211e-03,\n",
       "          2.92248338e-01,  1.11770906e-01,  5.33935800e-02,\n",
       "          6.72499835e-02,  4.75654751e-02,  5.42258937e-03,\n",
       "          3.55162136e-02,  3.40909064e-01,  1.11131594e-01,\n",
       "          8.09610665e-01,  2.08410490e-02,  6.80476874e-02,\n",
       "          3.52355681e-04,  3.39989625e-02,  3.57965529e-02,\n",
       "          1.08262867e-01,  1.57692265e-02,  3.75202708e-02,\n",
       "          6.65955544e-02,  9.50397849e-02,  5.80660515e-02,\n",
       "          1.03142671e-01,  2.51176924e-01,  1.21319495e-01,\n",
       "          4.58240472e-02,  2.36017965e-02,  5.66719286e-02,\n",
       "          6.24516532e-02,  1.01049960e-01,  3.85179132e-01,\n",
       "          7.69994706e-02,  4.62470502e-01,  6.64491057e-02,\n",
       "          7.86097273e-02,  7.91197121e-02,  9.27048922e-02,\n",
       "          1.12377964e-01,  4.12471313e-03,  1.04555503e-01,\n",
       "          1.02755845e-01,  2.93362886e-01,  1.15270719e-01,\n",
       "          7.96122402e-02,  6.22800365e-02,  3.02866921e-02,\n",
       "          6.06329553e-03,  6.28541559e-02,  2.11888060e-01,\n",
       "          3.52397323e-01,  4.66112122e-02,  6.43311739e-02,\n",
       "          4.42353711e-02,  3.93976510e-01,  6.33528009e-02,\n",
       "          8.52693021e-02,  1.08239055e-01,  7.99566358e-02,\n",
       "          5.55516407e-02,  6.26552626e-02,  8.24822634e-02,\n",
       "          6.09498471e-03,  1.06709719e-01,  8.45318809e-02,\n",
       "          3.65674719e-02,  8.13708425e-01,  5.77659309e-02,\n",
       "          5.75645186e-04,  4.86174040e-02,  1.05729304e-01,\n",
       "         -1.41988918e-01,  4.15295541e-01,  3.84394750e-02,\n",
       "          3.78602266e-01, -5.60533941e-01,  5.65599948e-02,\n",
       "          6.11173034e-01,  1.05967629e+00,  8.70696753e-02,\n",
       "          3.44685838e-02,  2.02815663e-02, -2.31225044e-01,\n",
       "          4.27424386e-02,  5.68887740e-02,  4.50887531e-02,\n",
       "          3.51675808e-01,  6.13601029e-01,  6.98836744e-01,\n",
       "          3.00938040e-01, -2.72201747e-01,  6.22107685e-02,\n",
       "          8.64478797e-02,  7.25416988e-02,  1.30367219e+00,\n",
       "          3.60503048e-01,  5.84087484e-02,  1.30964583e-02,\n",
       "          3.40278685e-01,  3.64004001e-02,  7.03533888e-02,\n",
       "          3.35324079e-01,  5.15225194e-02,  4.61132377e-02,\n",
       "          2.51040339e-01,  2.76190080e-02,  1.60191059e-02,\n",
       "          4.77115601e-01,  5.63190952e-02,  4.29532193e-02,\n",
       "         -1.69593051e-01,  1.01692222e-01,  6.70278594e-02,\n",
       "          6.68631718e-02,  6.09615445e-02,  3.02716076e-01,\n",
       "          5.04388846e-02,  1.02918357e-01,  7.39397258e-02,\n",
       "          7.31467605e-02, -3.90546858e-01,  1.05266273e-01,\n",
       "          2.84971762e-02,  1.12813180e-02, -1.91703305e-01,\n",
       "          3.31325457e-02,  1.07674994e-01],\n",
       "        [ 1.13779828e-01,  1.26462370e-01,  6.10504597e-02,\n",
       "         -2.83154696e-01,  1.06839873e-01,  2.67716259e-01,\n",
       "          2.69554138e-01,  1.25857135e-02,  8.94965779e-04,\n",
       "         -1.72689691e-01,  3.03254157e-01,  8.52467120e-02,\n",
       "          2.47041993e-02,  1.25472799e-01,  4.31516618e-02,\n",
       "         -2.38974914e-01,  5.09728156e-02,  2.75216494e-02,\n",
       "          2.68800586e-01,  3.32985260e-02,  9.57101285e-02,\n",
       "          7.50197023e-02,  9.53492224e-02,  1.39469787e-01,\n",
       "          8.64406750e-02,  7.61812478e-02, -3.72130126e-01,\n",
       "         -5.05058430e-02,  3.26296806e-01,  1.03986738e-02,\n",
       "         -1.51791185e-01,  1.34504870e-01,  6.42537251e-02,\n",
       "          8.09284821e-02,  5.72401956e-02,  6.52553281e-03,\n",
       "          4.27401438e-02,  2.38238409e-01,  1.33735538e-01,\n",
       "         -5.26409984e-01,  2.50800774e-02,  8.18884596e-02,\n",
       "          4.24024067e-04,  4.09142748e-02,  4.30774987e-02,\n",
       "          1.30283296e-01,  1.89766549e-02,  4.51518223e-02,\n",
       "          8.01409483e-02,  1.14370674e-01,  6.98765665e-02,\n",
       "          1.24121688e-01, -1.07528687e-01, -3.67859840e-01,\n",
       "          5.51445596e-02,  2.84023546e-02,  6.81988746e-02,\n",
       "          7.51541853e-02,  1.21603303e-01, -2.57141799e-01,\n",
       "          9.26610082e-02, -5.71291089e-01,  7.99647123e-02,\n",
       "          9.45987701e-02,  9.52124745e-02,  1.11560874e-01,\n",
       "          1.35235399e-01, -6.36606812e-01,  1.25821888e-01,\n",
       "          1.23656169e-01,  2.77887553e-01,  1.38716549e-01,\n",
       "          9.58051980e-02,  7.49476627e-02,  3.64469364e-02,\n",
       "          7.29655754e-03,  7.56385475e-02,  1.82511434e-01,\n",
       "          4.43902612e-02,  1.76347438e-02,  7.74159953e-02,\n",
       "          5.32327592e-02,  1.62658811e-01,  7.62386173e-02,\n",
       "          1.02612890e-01,  1.30254656e-01,  9.62196440e-02,\n",
       "          6.68507218e-02,  7.53992125e-02,  9.92589593e-02,\n",
       "          7.33469194e-03,  1.28414273e-01,  1.01725481e-01,\n",
       "          4.40052226e-02,  6.60273552e-01,  6.95153996e-02,\n",
       "          6.92730187e-04,  5.85060716e-02,  1.27234414e-01,\n",
       "         -2.50354618e-01,  1.81933790e-01,  4.62579839e-02,\n",
       "          1.37445971e-01, -2.45192945e-01,  6.80641904e-02,\n",
       "          4.01378185e-01,  2.23551765e-01,  1.04779474e-01,\n",
       "          4.14794162e-02,  2.44067907e-02, -2.76056796e-01,\n",
       "          5.14361709e-02,  6.84598386e-02,  5.42597175e-02,\n",
       "          2.84202069e-01,  9.76286421e-04,  1.10946846e+00,\n",
       "         -7.99782157e-01, -4.09031719e-01,  7.48643130e-02,\n",
       "          1.04031190e-01,  8.72965232e-02, -1.86418742e-01,\n",
       "          7.22707272e-01,  7.02889562e-02,  1.57602485e-02,\n",
       "          1.72155350e-01,  4.38041613e-02,  8.46631154e-02,\n",
       "          3.06307495e-01,  6.20021001e-02,  5.54925762e-02,\n",
       "          3.07769030e-01,  3.32366526e-02,  1.92773584e-02,\n",
       "          1.65495187e-01,  6.77742809e-02,  5.16898148e-02,\n",
       "          1.14045358e+00,  1.22376204e-01,  8.06611925e-02,\n",
       "          8.04629996e-02,  7.33609945e-02,  2.04130918e-01,\n",
       "          6.06980510e-02,  1.23851731e-01,  8.89789015e-02,\n",
       "          8.80246684e-02, -4.05776978e-01,  1.26677200e-01,\n",
       "          3.42934430e-02,  1.35759134e-02,  6.36099204e-02,\n",
       "          3.98716331e-02,  1.29575863e-01],\n",
       "        [-3.08001548e-01, -3.42333108e-01, -1.65263355e-01,\n",
       "         -3.50350678e-01, -2.89215118e-01,  5.44648767e-01,\n",
       "         -3.31702799e-01, -3.40694785e-02, -2.42266897e-03,\n",
       "         -2.86013752e-01, -5.26138186e-01, -2.30762497e-01,\n",
       "         -6.68741688e-02, -3.39654386e-01, -1.16811380e-01,\n",
       "         -4.49420124e-01, -1.37983218e-01, -7.45010003e-02,\n",
       "         -6.57822907e-01, -9.01389867e-02, -2.59086937e-01,\n",
       "         -2.03078046e-01, -2.58109957e-01, -3.77544165e-01,\n",
       "         -2.33994588e-01, -2.06222355e-01,  2.66137421e-01,\n",
       "         -5.18728256e-01, -4.17186141e-01, -2.81491671e-02,\n",
       "         -1.18518841e+00, -3.64104182e-01, -1.73934579e-01,\n",
       "         -2.19073102e-01, -1.54948980e-01, -1.76645909e-02,\n",
       "         -1.15697399e-01, -7.22927570e-01, -3.62021536e-01,\n",
       "         -4.31455165e-01, -6.78916797e-02, -2.21671730e-01,\n",
       "         -1.14783156e-03, -1.10754788e-01, -1.16610624e-01,\n",
       "         -3.52676392e-01, -5.13697267e-02, -1.22225799e-01,\n",
       "         -2.16941237e-01, -3.09600979e-01, -1.89155594e-01,\n",
       "         -3.35996926e-01,  1.17511950e-01,  4.87443000e-01,\n",
       "         -1.49276108e-01, -7.68850595e-02, -1.84614107e-01,\n",
       "         -2.03442082e-01, -3.29179674e-01, -5.76226413e-01,\n",
       "         -2.50832975e-01, -2.79474288e-01, -2.16464147e-01,\n",
       "         -2.56078511e-01, -2.57739812e-01, -3.01994860e-01,\n",
       "         -3.66081715e-01,  3.16297054e-01, -3.40599328e-01,\n",
       "         -3.34736735e-01, -6.01781785e-01, -3.75505179e-01,\n",
       "         -2.59344310e-01, -2.02883020e-01, -9.86617208e-02,\n",
       "         -1.97517537e-02, -2.04753250e-01, -6.25201548e-03,\n",
       "         -3.73709261e-01, -2.30549932e-01, -2.09564805e-01,\n",
       "         -1.44100860e-01, -4.69276071e-01, -2.06377655e-01,\n",
       "         -2.77772695e-01, -3.52598846e-01, -2.60466188e-01,\n",
       "         -1.80964649e-01, -2.04105362e-01, -2.68693656e-01,\n",
       "         -1.98549815e-02, -3.47616911e-01, -2.75370479e-01,\n",
       "         -1.19121961e-01, -1.45166671e+00, -1.88177913e-01,\n",
       "         -1.87521789e-03, -1.58375710e-01, -3.44423056e-01,\n",
       "         -1.49207637e-02, -5.11101663e-01, -1.25220165e-01,\n",
       "         -5.11648417e-01, -9.01321352e-01, -1.84249505e-01,\n",
       "          5.45877099e-01,  1.04117587e-01, -2.83637583e-01,\n",
       "         -1.12284623e-01, -6.60690963e-02, -7.98034847e-01,\n",
       "         -1.39237493e-01, -1.85320511e-01, -1.46880835e-01,\n",
       "         -7.79061317e-01, -5.26800454e-01,  1.95858888e-02,\n",
       "         -1.14984822e+00, -5.65493882e-01, -2.02657402e-01,\n",
       "         -2.81612009e-01, -2.36311331e-01, -8.05617809e-01,\n",
       "         -8.16392124e-01, -1.90271929e-01, -4.26629335e-02,\n",
       "         -6.21752501e-01, -1.18577704e-01, -2.29182735e-01,\n",
       "         -6.77171946e-01, -1.67839438e-01, -1.50218189e-01,\n",
       "         -5.02541006e-01, -8.99714902e-02, -5.21837324e-02,\n",
       "         -5.90184808e-01, -1.83464721e-01, -1.39924124e-01,\n",
       "         -4.52912241e-01, -3.31271946e-01, -2.18349531e-01,\n",
       "         -2.17813030e-01, -1.98587939e-01, -4.69862729e-01,\n",
       "         -1.64309397e-01, -3.35266173e-01, -2.40865543e-01,\n",
       "         -2.38282442e-01, -4.57233667e-01, -3.42914671e-01,\n",
       "         -9.28322226e-02, -3.67499515e-02, -6.59611046e-01,\n",
       "         -1.07932359e-01, -3.50761324e-01],\n",
       "        [-1.10541619e-01, -1.22863196e-01, -5.93129471e-02,\n",
       "         -1.56599805e-01, -1.03799179e-01,  3.46732736e-02,\n",
       "         -5.51612496e-01, -1.22275222e-02, -8.69494805e-04,\n",
       "         -3.18434954e-01, -1.31836250e-01, -8.28205645e-02,\n",
       "         -2.40011103e-02, -1.21901818e-01, -4.19235565e-02,\n",
       "         -1.71753719e-01, -4.95221168e-02, -2.67383792e-02,\n",
       "         -2.24724337e-01, -3.23508456e-02, -9.29861963e-02,\n",
       "         -7.28846267e-02, -9.26355645e-02, -1.35500431e-01,\n",
       "         -8.39805529e-02, -7.40131140e-02, -2.95220375e-01,\n",
       "          1.26209304e-01, -1.55986786e-01, -1.01027247e-02,\n",
       "         -1.47345796e-01, -1.30676821e-01, -6.24250472e-02,\n",
       "         -7.86252394e-02, -5.56111224e-02, -6.33981451e-03,\n",
       "         -4.15237434e-02, -2.44130701e-01, -1.29929394e-01,\n",
       "          7.07102008e-03, -2.43662931e-02, -7.95578882e-02,\n",
       "         -4.11956280e-04, -3.97498459e-02, -4.18515019e-02,\n",
       "         -1.26575410e-01, -1.84365716e-02, -4.38667871e-02,\n",
       "         -7.78601170e-02, -1.11115657e-01, -6.78878576e-02,\n",
       "         -1.20589152e-01,  4.39256057e-02,  1.72686964e-01,\n",
       "         -5.35751320e-02, -2.75940169e-02, -6.62579238e-02,\n",
       "         -7.30152801e-02, -1.18142448e-01, -4.07464474e-01,\n",
       "         -9.00238454e-02, -5.05302191e-01, -7.76888952e-02,\n",
       "         -9.19064656e-02, -9.25027132e-02, -1.08385824e-01,\n",
       "         -1.31386563e-01, -1.29257023e+00, -1.22240953e-01,\n",
       "         -1.20136879e-01, -1.98459744e-01, -1.34768635e-01,\n",
       "         -9.30785537e-02, -7.28146359e-02, -3.54096517e-02,\n",
       "         -7.08889589e-03, -7.34858513e-02,  5.28387204e-02,\n",
       "         -1.38761953e-01,  1.27784163e-01, -7.52127245e-02,\n",
       "         -5.17177396e-02,  8.33930194e-01, -7.40688592e-02,\n",
       "         -9.96925086e-02, -1.26547590e-01, -9.34812129e-02,\n",
       "         -6.49481416e-02, -7.32533336e-02, -9.64340419e-02,\n",
       "         -7.12594483e-03, -1.24759555e-01, -9.88303572e-02,\n",
       "         -4.27528210e-02, -6.36598021e-02, -6.75369799e-02,\n",
       "         -6.73014903e-04, -5.68409786e-02, -1.23613305e-01,\n",
       "         -4.63167913e-02, -1.81537166e-01, -4.49414663e-02,\n",
       "         -2.72682279e-01, -1.25399381e-01, -6.61270618e-02,\n",
       "         -1.68570913e-02,  7.86425173e-01, -1.01797417e-01,\n",
       "         -4.02989052e-02, -2.37121712e-02, -3.90087187e-01,\n",
       "         -4.99722809e-02, -6.65114447e-02, -5.27154729e-02,\n",
       "         -2.89344877e-01, -1.81081325e-01, -7.12375402e-01,\n",
       "          1.73515067e-01,  3.73980165e-01, -7.27336556e-02,\n",
       "         -1.01070441e-01, -8.48120376e-02, -2.53602356e-01,\n",
       "         -9.03470993e-01, -6.82885200e-02, -1.53117087e-02,\n",
       "         -2.00477809e-01, -4.25574854e-02, -8.22535828e-02,\n",
       "         -2.17757195e-01, -6.02375083e-02, -5.39132431e-02,\n",
       "         -1.10996567e-01, -3.22907344e-02, -1.87287200e-02,\n",
       "         -2.34628141e-01, -6.58454075e-02, -5.02187088e-02,\n",
       "         -5.47080874e-01, -1.18893355e-01, -7.83655643e-02,\n",
       "         -7.81730041e-02, -7.12731257e-02, -1.39576018e-01,\n",
       "         -5.89705743e-02, -1.20326884e-01, -8.64465460e-02,\n",
       "         -8.55194703e-02, -1.44458890e-01, -1.23071939e-01,\n",
       "         -3.33174430e-02, -1.31895402e-02, -7.65307844e-01,\n",
       "         -3.87368761e-02, -1.25888094e-01],\n",
       "        [ 7.14536458e-02,  7.94182718e-02,  3.83396409e-02,\n",
       "         -8.50692689e-01,  6.70953616e-02,  6.18971407e-01,\n",
       "         -4.01624888e-01,  7.90381804e-03,  5.62037807e-04,\n",
       "         -2.59409308e-01,  2.61731774e-01,  5.35348654e-02,\n",
       "          1.55142173e-02,  7.87968338e-02,  2.70992070e-02,\n",
       "         -1.69457763e-01,  3.20108905e-02,  1.72835756e-02,\n",
       "          2.59304017e-01,  2.09114477e-02,  6.01058900e-02,\n",
       "          4.71123159e-02,  5.98792359e-02,  8.75869244e-02,\n",
       "          5.42846769e-02,  4.78417724e-02, -1.96491882e-01,\n",
       "          2.93143168e-02,  1.80820733e-01,  6.53035752e-03,\n",
       "         -1.50179982e-01,  8.44689533e-02,  4.03512865e-02,\n",
       "          5.08230291e-02,  3.59467976e-02,  4.09802934e-03,\n",
       "          2.68407743e-02,  2.72754818e-01,  8.39858279e-02,\n",
       "         -1.65706143e-01,  1.57502647e-02,  5.14258817e-02,\n",
       "          2.66286806e-04,  2.56941337e-02,  2.70526335e-02,\n",
       "          8.18178207e-02,  1.19173247e-02,  2.83553042e-02,\n",
       "          5.03284521e-02,  7.18246996e-02,  4.38824296e-02,\n",
       "          7.79483169e-02, -3.46656926e-02,  4.29878235e-01,\n",
       "          3.46307456e-02,  1.78366527e-02,  4.28288393e-02,\n",
       "          4.71967645e-02,  7.63667822e-02,  2.98785955e-01,\n",
       "          5.81910312e-02,  3.28306615e-01,  5.02177700e-02,\n",
       "          5.94079532e-02,  5.97933680e-02,  7.00601339e-02,\n",
       "          8.49277154e-02,  2.82655269e-01,  7.90160447e-02,\n",
       "          7.76559785e-02,  2.58220196e-01,  8.71138871e-02,\n",
       "          6.01655878e-02,  4.70670760e-02,  2.28886493e-02,\n",
       "          4.58223280e-03,  4.75009456e-02, -9.30627212e-02,\n",
       "          2.47964263e-01,  1.20085090e-01,  4.86171842e-02,\n",
       "          3.34301293e-02, -3.64640385e-01,  4.78778109e-02,\n",
       "          6.44408166e-02,  8.17998052e-02,  6.04258701e-02,\n",
       "          4.19822074e-02,  4.73506451e-02,  6.23345524e-02,\n",
       "          4.60618129e-03,  8.06440637e-02,  6.38835281e-02,\n",
       "          2.76352465e-02, -1.51525289e-01,  4.36556116e-02,\n",
       "          4.35034075e-04,  3.67417671e-02,  7.99031109e-02,\n",
       "          8.61219108e-01,  1.96862847e-01,  2.90499814e-02,\n",
       "          1.86915994e-01,  2.71079212e-01,  4.27442603e-02,\n",
       "         -6.64993823e-02, -1.23455264e-01,  6.58014268e-02,\n",
       "          2.60490440e-02,  1.53274462e-02,  7.86404684e-02,\n",
       "          3.23018655e-02,  4.29927185e-02,  3.40750553e-02,\n",
       "          3.06367517e-01, -9.12944674e-02, -2.87704319e-01,\n",
       "          1.52716577e-01,  3.70244890e-01,  4.70147282e-02,\n",
       "          6.53315037e-02,  5.48221543e-02, -3.07628989e-01,\n",
       "          2.53842175e-01,  4.41414118e-02,  9.89742577e-03,\n",
       "          2.63389587e-01,  2.75089759e-02,  5.31683639e-02,\n",
       "          2.07852185e-01,  3.89372669e-02,  3.48492898e-02,\n",
       "          2.49003351e-01,  2.08725948e-02,  1.21061662e-02,\n",
       "          1.89863920e-01,  4.25621942e-02,  3.24611664e-02,\n",
       "          3.35273445e-01,  7.68521652e-02,  5.06551638e-02,\n",
       "          5.05307019e-02,  4.60706428e-02,  2.02302769e-01,\n",
       "          3.81183252e-02,  7.77787790e-02,  5.58786802e-02,\n",
       "          5.52794188e-02,  6.00966036e-01,  7.95531794e-02,\n",
       "          2.15362571e-02,  8.52566306e-03, -3.82250220e-01,\n",
       "          2.50393562e-02,  8.13735276e-02]], dtype=float32),\n",
       " array([[ 0.12928236,  0.00709345,  0.2270143 , ..., -0.11943059,\n",
       "          0.70338863,  0.65438473],\n",
       "        [ 0.5483187 ,  0.02092948,  0.75045013, ...,  0.6519301 ,\n",
       "          0.2481015 , -0.07473479],\n",
       "        [ 0.6927793 ,  0.39143902,  0.849975  , ..., -0.02917881,\n",
       "          0.7804612 ,  0.8315323 ],\n",
       "        ...,\n",
       "        [ 1.3140947 , -0.45753518,  0.0665556 , ...,  0.33966336,\n",
       "          0.18813947, -0.14644662],\n",
       "        [ 0.40146548,  0.82552207,  0.22120453, ...,  0.37094074,\n",
       "          0.6960912 ,  0.29183385],\n",
       "        [ 0.7315277 ,  0.3760425 ,  0.20362827, ...,  0.39146245,\n",
       "          0.6885634 ,  0.46912655]], dtype=float32),\n",
       " array([[ 0.9708115 , -0.3309298 ],\n",
       "        [ 1.3039603 ,  1.3371193 ],\n",
       "        [ 0.98141766,  0.18867482],\n",
       "        [ 1.3461785 , -0.34453517],\n",
       "        [-1.2347285 ,  0.35852358],\n",
       "        [-2.3279512 , -1.2246487 ],\n",
       "        [-1.1549656 , -0.63763064],\n",
       "        [-0.20642047, -3.2606552 ],\n",
       "        [-2.220081  ,  0.8199499 ],\n",
       "        [-1.2909098 ,  1.4883667 ],\n",
       "        [ 0.4982706 ,  0.4866576 ],\n",
       "        [ 1.1368098 , -0.058157  ],\n",
       "        [ 0.7944016 ,  0.1643444 ],\n",
       "        [ 1.1349216 ,  0.42144048],\n",
       "        [ 0.97918946,  0.6415223 ],\n",
       "        [ 0.39369217,  0.5492121 ],\n",
       "        [ 3.000935  , -1.4299917 ],\n",
       "        [ 0.5363048 ,  0.06976717],\n",
       "        [ 1.4205241 ,  0.46125373],\n",
       "        [ 0.27979648,  0.96578944],\n",
       "        [-2.0099235 , -0.3510035 ],\n",
       "        [ 0.8610555 ,  2.2938848 ],\n",
       "        [-2.057006  , -0.03213193],\n",
       "        [-1.189904  , -0.77032423],\n",
       "        [ 0.618128  ,  0.03546301],\n",
       "        [ 1.0950874 , -0.37824896],\n",
       "        [-1.8723288 , -0.5274496 ],\n",
       "        [ 0.7599564 ,  0.41595116],\n",
       "        [ 0.2019101 ,  1.339313  ],\n",
       "        [-0.99787664,  1.1880351 ],\n",
       "        [ 0.44627306,  0.3564782 ]], dtype=float32),\n",
       " array([-0.28383029, -0.31458557, -0.15186486,  0.41508207, -0.26639026,\n",
       "         0.6201169 ,  0.33647546, -0.03149702, -0.00305842, -0.34932426,\n",
       "         0.13541721, -0.21263544, -0.06160916, -0.31217527, -0.10744675,\n",
       "         0.7965779 , -0.12664579, -0.06842119, -0.04330331, -0.08283355,\n",
       "        -0.23805155, -0.1870908 , -0.23751526, -0.3471313 , -0.21489719,\n",
       "        -0.18942702, -0.7418141 , -0.55674785,  0.05194964, -0.02596493,\n",
       "        -0.21929169, -0.33435127, -0.15995553, -0.20170794, -0.14258547,\n",
       "        -0.01627403, -0.10622326, -0.08091658, -0.3327444 ,  0.00287841,\n",
       "        -0.06249094, -0.20345554, -0.00187809, -0.10168617, -0.10713416,\n",
       "        -0.32413107, -0.04729197, -0.11250894, -0.19976912, -0.28495467,\n",
       "        -0.17469911, -0.30920935,  1.0555378 ,  0.6093084 , -0.13741975,\n",
       "        -0.07063854, -0.16984136, -0.18727449, -0.30248228, -0.570955  ,\n",
       "        -0.23112178,  0.02355189, -0.19898465, -0.23572895, -0.23653843,\n",
       "        -0.27749056, -0.33645573, -0.5533304 , -0.31371927, -0.30730048,\n",
       "         0.03483503, -0.34579587, -0.23848681, -0.1865959 , -0.09067887,\n",
       "        -0.01844228, -0.18869911, -0.6417174 ,  0.29200387,  0.95964336,\n",
       "        -0.19297187, -0.13239871, -0.84725064, -0.19007044, -0.25604025,\n",
       "        -0.32444695, -0.2393859 , -0.16607864, -0.18790606, -0.24759218,\n",
       "        -0.01826485, -0.31940347, -0.25278816, -0.10948997,  0.12419037,\n",
       "        -0.17285003, -0.00264572, -0.14541171, -0.31624797, -0.31811666,\n",
       "        -0.00314215, -0.11497195, -0.06430664, -0.5438882 , -0.17002307,\n",
       "         0.5083839 ,  0.04175209, -0.26094735, -0.10315148, -0.06065961,\n",
       "        -0.92349344, -0.12804289, -0.17049542, -0.13507186, -0.13124591,\n",
       "         0.0136297 ,  0.24480306, -0.21864651, -0.67782176, -0.18614395,\n",
       "        -0.25848284, -0.21713065, -0.4318761 ,  1.1131146 , -0.17476822,\n",
       "        -0.0395139 ,  0.04845734, -0.1089573 , -0.21143371, -0.23421483,\n",
       "        -0.15427087, -0.13839707,  0.13148999, -0.08271152, -0.04803753,\n",
       "        -0.02969478, -0.16844337, -0.12841488,  0.49219307, -0.30410913,\n",
       "        -0.20056294, -0.20006253, -0.18251912,  0.11210488, -0.15121455,\n",
       "        -0.30812535, -0.22134608, -0.21894099, -0.1469192 , -0.31525296,\n",
       "        -0.08532681, -0.03395198,  1.0817326 , -0.09930573, -0.32260868],\n",
       "       dtype=float32),\n",
       " array([ 0.39138705,  1.1002902 , -0.75354606,  0.69841635,  1.1794804 ,\n",
       "        -0.53908145,  2.0798452 , -1.8548064 ,  0.12978671, -1.9527441 ,\n",
       "        -0.2887079 , -0.22140636, -0.4877299 ,  0.20309873, -0.67571145,\n",
       "         0.12151922, -2.0811496 , -0.3362277 , -1.1085688 ,  0.04150168,\n",
       "        -1.5946794 , -0.15697241, -2.4613414 ,  1.969222  , -0.28648257,\n",
       "         0.54264486, -0.7806928 , -0.3322365 ,  1.1098017 , -1.3309399 ,\n",
       "        -0.17421073], dtype=float32),\n",
       " array([[-1.9626482, -0.4831743]], dtype=float32)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(params)):\n",
    "    np.save('../save/weights/slearn_parms{}_6-2.npy'.format(i),params[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
