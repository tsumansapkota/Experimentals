{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6287145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os, sys, pathlib, random, time, pickle, copy, json\n",
    "# from tqdm.autonotebook import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0173cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcaa3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_lib import TransformerBlock, \\\n",
    "        Mixer_TransformerBlock_Encoder, \\\n",
    "        PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "387412b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyImageNet_Preload(data.Dataset):\n",
    "#     https://gist.github.com/z-a-f/b862013c0dc2b540cf96a123a6766e54\n",
    "    \n",
    "    def __init__(self, root, mode='train', transform=None, preload=False):\n",
    "        super().__init__()\n",
    "        self.preload = preload\n",
    "        dataset = datasets.ImageFolder(\n",
    "            root=os.path.join(root, mode),\n",
    "            transform=None\n",
    "        )\n",
    "        self.transform = transform\n",
    "        self.images, self.labels = [], []\n",
    "        print(\"Dataset Size:\",len(dataset))\n",
    "        \n",
    "        if preload:\n",
    "            for i in tqdm(range(len(dataset))):\n",
    "                x, y = dataset[i]\n",
    "                self.images.append(x)\n",
    "                self.labels.append(y)\n",
    "                \n",
    "#         del dataset\n",
    "        self.dataset = dataset\n",
    "            \n",
    "    def _add_channels(img, total_channels=3):\n",
    "        while len(img.shape) < 3:  # third axis is the channels\n",
    "            img = np.expand_dims(img, axis=-1)\n",
    "        while(img.shape[-1]) < 3:\n",
    "            img = np.concatenate([img, img[:, :, -1:]], axis=-1)\n",
    "        return img\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.preload:\n",
    "            img, lbl = self.images[idx], self.labels[idx]\n",
    "        else:\n",
    "            img, lbl = self.dataset[idx]\n",
    "        return self.transform(img), lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2e11b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b45f6816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixer_ViT_Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_dim:tuple, patch_size:tuple, hidden_channel:int, num_blocks:int, num_classes:int, block_seq_size:int, block_mlp_size:int, forward_expansion:float=2.0, pos_emb=True, dropout:float=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_dim = image_dim ### must contain (C, H, W) or (H, W)\n",
    "        \n",
    "        ### find patch dim\n",
    "        d0 = int(image_dim[-2]/patch_size[0])\n",
    "        d1 = int(image_dim[-1]/patch_size[1])\n",
    "        assert d0*patch_size[0]==image_dim[-2], \"Image must be divisible into patch size\"\n",
    "        assert d1*patch_size[1]==image_dim[-1], \"Image must be divisible into patch size\"\n",
    "        \n",
    "#         self.d0, self.d1 = d0, d1 ### number of patches in each axis\n",
    "        __patch_size = patch_size[0]*patch_size[1]*image_dim[0] ## number of channels in each patch\n",
    "    \n",
    "        ### find channel dim\n",
    "        channel_size = d0*d1 ## number of patches\n",
    "        \n",
    "        ### after the number of channels are changed\n",
    "        init_dim = __patch_size\n",
    "        final_dim = hidden_channel\n",
    "        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        #### rescale the patches (patch wise image non preserving transform, unlike bilinear interpolation)\n",
    "        self.channel_change = nn.Linear(init_dim, final_dim)\n",
    "        print(f\"ViT Mixer : Channels per patch -> Initial:{init_dim} Final:{final_dim}\")\n",
    "        \n",
    "        \n",
    "        self.channel_dim = final_dim\n",
    "        self.patch_dim = channel_size\n",
    "        \n",
    "        self.transformer_blocks = []\n",
    "        \n",
    "        f = self.get_factors(self.channel_dim)\n",
    "        print(f)\n",
    "        fi = np.abs(np.array(f) - np.sqrt(self.channel_dim)).argmin()\n",
    "        \n",
    "        _n_heads = f[fi]\n",
    "        \n",
    "        ## number of dims per channel -> channel_dim\n",
    "#         print('Num patches:', self.patch_dim)\n",
    "        print(f'Sequence len: {self.patch_dim} ; Block size: {block_seq_size}')\n",
    "        print('Channel dim:', self.channel_dim, 'num heads:',_n_heads)\n",
    "        \n",
    "        if block_seq_size is None or block_seq_size<2:\n",
    "            ### Find the block size for sequence:\n",
    "            block_seq_size = int(2**np.ceil(np.log2(np.sqrt(self.patch_dim))))\n",
    "            \n",
    "        print(f'MLP dim: {self.channel_dim} ; Block size: {block_mlp_size}')\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            L = Mixer_TransformerBlock_Encoder(self.patch_dim, block_seq_size, self.channel_dim, _n_heads, dropout, forward_expansion, nn.GELU, block_mlp_size)\n",
    "            self.transformer_blocks.append(L)\n",
    "        self.transformer_blocks = nn.Sequential(*self.transformer_blocks)\n",
    "        \n",
    "        self.linear = nn.Linear(self.patch_dim*self.channel_dim, num_classes)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(self.channel_dim, dropout=0)\n",
    "        if not pos_emb:\n",
    "            self.positional_encoding = nn.Identity()\n",
    "        \n",
    "        \n",
    "    def get_factors(self, n):\n",
    "        facts = []\n",
    "        for i in range(2, n+1):\n",
    "            if n%i == 0:\n",
    "                facts.append(i)\n",
    "        return facts\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.unfold(x).swapaxes(-1, -2)\n",
    "        x = self.channel_change(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.linear(x.view(bs, -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7094b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67e27199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT Mixer : Channels per patch -> Initial:3 Final:64\n",
      "[2, 4, 8, 16, 32, 64]\n",
      "Sequence len: 1024 ; Block size: 32\n",
      "Channel dim: 64 num heads: 8\n",
      "MLP dim: 64 ; Block size: None\n"
     ]
    }
   ],
   "source": [
    "model = Mixer_ViT_Classifier([3, 32, 32], [1, 1], 64, num_blocks=2, num_classes=10, \n",
    "                            block_seq_size=32, block_mlp_size=None, pos_emb=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d642126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mixer_ViT_Classifier(\n",
       "  (unfold): Unfold(kernel_size=[1, 1], dilation=1, padding=0, stride=[1, 1])\n",
       "  (channel_change): Linear(in_features=3, out_features=64, bias=True)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): Mixer_TransformerBlock_Encoder(\n",
       "      (sparse_transformers): ModuleList(\n",
       "        (0): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention_Sparse(\n",
       "            (values): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (keys): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (queries): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (fc_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): ResMlpBlock(\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (1): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention_Sparse(\n",
       "            (values): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (keys): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (queries): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (fc_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): ResMlpBlock(\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Mixer_TransformerBlock_Encoder(\n",
       "      (sparse_transformers): ModuleList(\n",
       "        (0): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention_Sparse(\n",
       "            (values): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (keys): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (queries): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (fc_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): ResMlpBlock(\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (1): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention_Sparse(\n",
       "            (values): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (keys): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (queries): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (fc_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): ResMlpBlock(\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=65536, out_features=10, bias=True)\n",
       "  (positional_encoding): Identity()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "493d244f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 ms ± 12.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model(torch.randn(10, 3, 32, 32).to(device)).mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9707501d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT Mixer : Channels per patch -> Initial:3 Final:64\n",
      "[2, 4, 8, 16, 32, 64]\n",
      "Sequence len: 1024 ; Block size: 1024\n",
      "Channel dim: 64 num heads: 8\n",
      "MLP dim: 64 ; Block size: None\n"
     ]
    }
   ],
   "source": [
    "model = Mixer_ViT_Classifier([3, 32, 32], [1, 1], 64, num_blocks=4, num_classes=10, \n",
    "                            block_seq_size=1024, block_mlp_size=None, pos_emb=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "652815b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mixer_ViT_Classifier(\n",
       "  (unfold): Unfold(kernel_size=[1, 1], dilation=1, padding=0, stride=[1, 1])\n",
       "  (channel_change): Linear(in_features=3, out_features=64, bias=True)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): Mixer_TransformerBlock_Encoder(\n",
       "      (sparse_transformers): ModuleList(\n",
       "        (0): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention_Sparse(\n",
       "            (values): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (keys): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (queries): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (fc_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): ResMlpBlock(\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Mixer_TransformerBlock_Encoder(\n",
       "      (sparse_transformers): ModuleList(\n",
       "        (0): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention_Sparse(\n",
       "            (values): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (keys): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (queries): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (fc_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): ResMlpBlock(\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Mixer_TransformerBlock_Encoder(\n",
       "      (sparse_transformers): ModuleList(\n",
       "        (0): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention_Sparse(\n",
       "            (values): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (keys): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (queries): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (fc_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): ResMlpBlock(\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Mixer_TransformerBlock_Encoder(\n",
       "      (sparse_transformers): ModuleList(\n",
       "        (0): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention_Sparse(\n",
       "            (values): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (keys): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (queries): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (fc_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): ResMlpBlock(\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=65536, out_features=10, bias=True)\n",
       "  (positional_encoding): Identity()\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee9c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit model(torch.randn(10, 3, 32, 32).to(device)).mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2667f95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "facef435",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 1024, 64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fb80bcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14 µs ± 13.9 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "s = x.shape\n",
    "%timeit x.view(s[0], -1, 32, 1, 64).transpose(2,3).contiguous().view(*s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82ee5195",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mixer_TransformerBlock_Encoder(seq_length=1024, block_size=32, embed_size=64, \n",
    "                                       heads=8, dropout=0, forward_expansion=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b463737d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mixer_TransformerBlock_Encoder(\n",
       "  (sparse_transformers): ModuleList(\n",
       "    (0): Sparse_TransformerBlock(\n",
       "      (attention): SelfAttention_Sparse(\n",
       "        (values): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (keys): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (queries): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): ResMlpBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (1): Sparse_TransformerBlock(\n",
       "      (attention): SelfAttention_Sparse(\n",
       "        (values): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (keys): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (queries): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): ResMlpBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0fa348e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.08 s ± 51.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "757e6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mixer_TransformerBlock_Encoder(seq_length=1024, block_size=1024, embed_size=64, \n",
    "                                       heads=8, dropout=0, forward_expansion=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f31c405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mixer_TransformerBlock_Encoder(\n",
       "  (sparse_transformers): ModuleList(\n",
       "    (0): Sparse_TransformerBlock(\n",
       "      (attention): SelfAttention_Sparse(\n",
       "        (values): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (keys): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (queries): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc_out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): ResMlpBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d5ca4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590 ms ± 12.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "044f8dfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43masas\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'asas' is not defined"
     ]
    }
   ],
   "source": [
    "asas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c173ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(dataset:str, patch_size:int, num_layers:int, SEED:int, sparse_att:bool=False, sparse_mlp:bool=False, cuda:int=0):\n",
    "#     device = torch.device(f\"cuda:{cuda}\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    \n",
    "    BS = 256\n",
    "    NC = -1\n",
    "    EPOCHS = 300\n",
    "    imsize = (3, 32, 32)\n",
    "    expansion_dict = {16:1024, 8:256, 4:128, 2:64, 1:64}\n",
    "    expansion = expansion_dict[patch_size]\n",
    "\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    ##### Data Transforms\n",
    "    if dataset == 'tiny':\n",
    "        NC = 200\n",
    "        EPOCHS = 400\n",
    "        imsize = (3, 64, 64)\n",
    "        tiny_train = transforms.Compose([\n",
    "        transforms.RandAugment(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.5]*3,\n",
    "            std=[0.2]*3,\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        tiny_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.5]*3,\n",
    "                std=[0.2]*3,\n",
    "            ),\n",
    "        ])\n",
    "        \n",
    "        train_dataset = TinyImageNet_Preload(root=\"../../../../../_Datasets/tiny-imagenet-200\",\n",
    "                                     mode='train', transform=tiny_train)\n",
    "        test_dataset = TinyImageNet_Preload(root=\"../../../../../_Datasets/tiny-imagenet-200\",\n",
    "                                     mode='val', transform=tiny_test)\n",
    "        \n",
    "    elif dataset == 'cifar10':\n",
    "        NC = 10\n",
    "        BS = 32\n",
    "        cifar_train = transforms.Compose([\n",
    "            transforms.RandomCrop(size=32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "                std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        cifar_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "                std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        train_dataset = datasets.CIFAR10(root=\"../../../../../_Datasets/cifar10/\", train=True, download=True, transform=cifar_train)\n",
    "        test_dataset = datasets.CIFAR10(root=\"../../../../../_Datasets/cifar10/\", train=False, download=True, transform=cifar_test)\n",
    "\n",
    "    elif dataset == 'cifar100':\n",
    "        NC = 100\n",
    "        BS = 128\n",
    "        cifar_train = transforms.Compose([\n",
    "            transforms.RandomCrop(size=32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.5071, 0.4865, 0.4409],\n",
    "                std=[0.2009, 0.1984, 0.2023],\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        cifar_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.5071, 0.4865, 0.4409],\n",
    "                std=[0.2009, 0.1984, 0.2023],\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        train_dataset = datasets.CIFAR100(root=\"../../../../../_Datasets/cifar100/\", train=True, download=True, transform=cifar_train)\n",
    "        test_dataset = datasets.CIFAR100(root=\"../../../../../_Datasets/cifar100/\", train=False, download=True, transform=cifar_test)\n",
    "        \n",
    "    ##### Now create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BS, shuffle=True, num_workers=4)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BS, shuffle=False, num_workers=4)\n",
    "    \n",
    "    ### Now create models\n",
    "    \n",
    "    seq_len = (imsize[-1]*imsize[-2])//(patch_size*patch_size)\n",
    "    mlp_dim = expansion\n",
    "    print(seq_len, mlp_dim)\n",
    "    \n",
    "    if sparse_att:\n",
    "        seq_len = int(2**np.ceil(np.log2(np.sqrt(seq_len))))\n",
    "        assert num_layers%2 == 0, 'number of blocks on sparse transformer is (x2)/2 hence it must be even'\n",
    "        num_layers_ = num_layers//2\n",
    "    else:\n",
    "        num_layers_ = num_layers\n",
    "    \n",
    "    if sparse_mlp:\n",
    "        mlp_dim = int(2**np.ceil(np.log2(np.sqrt(expansion))))\n",
    "    \n",
    "        \n",
    "    torch.manual_seed(SEED)\n",
    "    model = Mixer_ViT_Classifier(imsize, \n",
    "                                 patch_size=[patch_size]*2, \n",
    "                                 hidden_channel=expansion, \n",
    "                                 num_blocks=num_layers_, \n",
    "                                 num_classes=NC, \n",
    "                                 block_seq_size=seq_len, \n",
    "                                 block_mlp_size=mlp_dim)#.to(device)\n",
    "    print(model)\n",
    "    print('\\n\\n')\n",
    "    \n",
    "#     _x = torch.randn(BS, *imsize).to(device)\n",
    "#     print(\"Output: \",vit_mixer(_x).shape)\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"number of params: \", num_params)\n",
    "    \n",
    "    _a, _b = 'att', 'mlp'\n",
    "    if sparse_att: _a = 'sAtt'\n",
    "    if sparse_mlp: _b = 'sMlp'\n",
    "    model_name = f'01.3_ViT_{dataset}_patch{patch_size}_l{num_layers}_{_a}_{_b}_s{SEED}'\n",
    "    print(f\"Model Name: {model_name}\")\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    STAT ={'train_stat':[], 'test_stat':[], 'params':num_params, }\n",
    "\n",
    "    ## Following is copied from \n",
    "    ### https://github.com/kuangliu/pytorch-cifar/blob/master/main.py\n",
    "\n",
    "    # Training\n",
    "    def train(epoch):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            break\n",
    "\n",
    "        STAT['train_stat'].append((epoch, train_loss/(batch_idx+1), 100.*correct/total)) ### (Epochs, Loss, Acc)\n",
    "        print(f\"[Train] {epoch} Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "        return\n",
    "\n",
    "    global best_acc\n",
    "    best_acc = -1\n",
    "    def test(epoch):\n",
    "        global best_acc\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        time_taken = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader)):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                start = time.time()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                start = time.time()-start\n",
    "                time_taken.append(start)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        STAT['test_stat'].append((epoch, test_loss/(batch_idx+1), 100.*correct/total, np.mean(time_taken))) ### (Epochs, Loss, Acc, time)\n",
    "        print(f\"[Test] {epoch} Loss: {test_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "\n",
    "        # Save checkpoint.\n",
    "        acc = 100.*correct/total\n",
    "        if acc > best_acc:\n",
    "            print('Saving..')\n",
    "            state = {\n",
    "                'model': model.state_dict(),\n",
    "                'acc': acc,\n",
    "                'epoch': epoch\n",
    "            }\n",
    "            if not os.path.isdir('models'):\n",
    "                os.mkdir('models')\n",
    "            torch.save(state, f'./models/benchmark/{model_name}.pth')\n",
    "            best_acc = acc\n",
    "\n",
    "        with open(f\"./output/benchmark/{model_name}_data.json\", 'w') as f:\n",
    "            json.dump(STAT, f, indent=0)\n",
    "\n",
    "    ### Train the whole damn thing\n",
    "    EPOCHS = 1\n",
    "    for epoch in range(0, EPOCHS):\n",
    "        train(epoch)\n",
    "#         test(epoch)\n",
    "#         scheduler.step()\n",
    "    return\n",
    "    \n",
    "    train_stat = np.array(STAT['train_stat'])\n",
    "    test_stat = np.array(STAT['test_stat'])\n",
    "\n",
    "    plt.plot(train_stat[:,1], label='train')\n",
    "    plt.plot(test_stat[:,1], label='test')\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"./output/benchmark/plots/{model_name}_loss.svg\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_stat[:,2], label='train')\n",
    "    plt.plot(test_stat[:,2], label='test')\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"./output/benchmark/plots/{model_name}_accs.svg\")\n",
    "    plt.show()\n",
    "    \n",
    "    del model, optimizer\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d03c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark(dataset='tiny', \n",
    "#           patch_size=4, num_layers=10, SEED=123, sparse_att=True, sparse_mlp=True, cuda=0\n",
    "#          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717e715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Automate the benchmark\n",
    "# ###### for tiny\n",
    "# cuda_idx = 0\n",
    "# # for seed in [147, 258, 369]:\n",
    "# for seed in [147]:\n",
    "#     for patch_size in [16, 8, 4]:\n",
    "#         for sparse_attention in [False, True]:\n",
    "#             for sparse_mlp in [False, True]:\n",
    "# #                 for nlayers in [6, 10, 14]:\n",
    "#                 for nlayers in [6]:\n",
    "#                     print(f'''\n",
    "#                         Experimenting on Tiny Dataset \n",
    "#                         patch:{patch_size},\n",
    "#                         sparse_att: {sparse_attention},\n",
    "#                         sparse_mlp: {sparse_mlp},\n",
    "#                         num_layers : {nlayers},\n",
    "#                         seed: {seed}\n",
    "#                     ''')\n",
    "            \n",
    "#                     benchmark(dataset='tiny', \n",
    "#                               patch_size=patch_size, \n",
    "#                               num_layers=nlayers, \n",
    "#                               SEED=seed, \n",
    "#                               sparse_att=sparse_attention, sparse_mlp=sparse_mlp, \n",
    "#                               cuda=cuda_idx\n",
    "#                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af01a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Automate the benchmark\n",
    "# ###### for tiny\n",
    "\n",
    "# not_working = [\n",
    "#     (4, False, True, 6),\n",
    "#     (4, False, False, 10),\n",
    "#     (4, False, False, 14),\n",
    "#     (4, False, True, 10),\n",
    "#     (4, False, True, 14),\n",
    "#     (4, True, True, 14),\n",
    "# ]\n",
    "\n",
    "# cuda_idx = 0\n",
    "# # for seed in [147, 258, 369]:\n",
    "# for seed in [147]:\n",
    "#     for patch_size in [16, 8, 4]:\n",
    "#         for sparse_attention in [False, True]:\n",
    "#             for sparse_mlp in [False, True]:\n",
    "#                 for nlayers in [6, 10, 14]:\n",
    "\n",
    "#                     print(f'''\n",
    "#                         Experimenting on Tiny Dataset \n",
    "#                         patch:{patch_size},\n",
    "#                         sparse_att: {sparse_attention},\n",
    "#                         sparse_mlp: {sparse_mlp},\n",
    "#                         num_layers : {nlayers},\n",
    "#                         seed: {seed}\n",
    "#                     ''')\n",
    "            \n",
    "#                 ### check if config is in not_working case\n",
    "#                     exit = False\n",
    "#                     for nw in not_working:\n",
    "#                         if patch_size==nw[0] and \\\n",
    "#                             sparse_attention==nw[1] and \\\n",
    "#                             sparse_mlp==nw[2] and\\\n",
    "#                             nlayers==nw[3]:\n",
    "                            \n",
    "#                             exit=True\n",
    "#                             break\n",
    "#                     if exit:\n",
    "#                         print(f'Exiting as the config is in NOT WORKING')\n",
    "#                         continue\n",
    "\n",
    "\n",
    "#                     benchmark(dataset='tiny', \n",
    "#                               patch_size=patch_size, \n",
    "#                               num_layers=nlayers, \n",
    "#                               SEED=seed, \n",
    "#                               sparse_att=sparse_attention, sparse_mlp=sparse_mlp, \n",
    "#                               cuda=cuda_idx\n",
    "#                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3d9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Automate the benchmark\n",
    "###### for c10 / c100\n",
    "\n",
    "not_working = [\n",
    "#     (1, False, False, 4),\n",
    "#     (1, False, False, 8),\n",
    "#     (1, False, False, 12),\n",
    "#     (1, False, True, 4),\n",
    "#     (1, False, True, 8),\n",
    "#     (1, False, True, 12),\n",
    "\n",
    "#     (1, True, False, 12),\n",
    "#     (1, True, True, 12),\n",
    "]\n",
    "\n",
    "cuda_idx = 0\n",
    "# for seed in [147, 258, 369]:\n",
    "for seed in [147]:\n",
    "    for patch_size in [1]:\n",
    "        for sparse_attention in [False, True]:\n",
    "            for sparse_mlp in [False, True]:\n",
    "                for nlayers in [4]:\n",
    "\n",
    "                    print(f'''\n",
    "                        Experimenting on Cifar100 Dataset \n",
    "                        patch:{patch_size},\n",
    "                        sparse_att: {sparse_attention},\n",
    "                        sparse_mlp: {sparse_mlp},\n",
    "                        num_layers : {nlayers},\n",
    "                        seed: {seed}\n",
    "                    ''')\n",
    "            \n",
    "                ### check if config is in not_working case\n",
    "                    exit = False\n",
    "                    for nw in not_working:\n",
    "                        if patch_size==nw[0] and \\\n",
    "                            sparse_attention==nw[1] and \\\n",
    "                            sparse_mlp==nw[2] and\\\n",
    "                            nlayers==nw[3]:\n",
    "                            \n",
    "                            exit=True\n",
    "                            break\n",
    "                    if exit:\n",
    "                        print(f'Exiting as the config is in NOT WORKING')\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    benchmark(dataset='cifar100', \n",
    "                              patch_size=patch_size, \n",
    "                              num_layers=nlayers, \n",
    "                              SEED=seed, \n",
    "                              sparse_att=sparse_attention, sparse_mlp=sparse_mlp, \n",
    "                              cuda=cuda_idx\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f4b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdasdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use mixing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d174ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Experiments\n",
    "'''\n",
    "1. Sparse Attention\n",
    "2. Sparse MLP\n",
    "3. Sparse Att + MLP\n",
    "\n",
    "Datasets:\n",
    "A. Cifar-10/100 -> 4x4 vs 2x2 vs 1x1 patch\n",
    "B. Tiny-Imagenet -> 16x16 vs 4x4 vs 2x2 patch\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317f7e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "## Cifar 10/100\n",
    "\n",
    "vit_mixer = Mixer_ViT_Classifier((3, 32, 32), patch_size=[8, 8], hidden_channel=256, num_blocks=6, num_classes=200, block_seq_size=4, block_mlp_size=16)\n",
    "\n",
    "vit_mixer = Mixer_ViT_Classifier((3, 32, 32), patch_size=[4, 4], hidden_channel=128, num_blocks=3, num_classes=10, block_seq_size=8, block_mlp_size=128)\n",
    "\n",
    "vit_mixer = Mixer_ViT_Classifier((3, 32, 32), patch_size=[2, 2], hidden_channel=64, num_blocks=3, num_classes=10, block_seq_size=16, block_mlp_size=8)\n",
    "\n",
    "vit_mixer = Mixer_ViT_Classifier((3, 32, 32), patch_size=[1, 1], hidden_channel=64, num_blocks=3, num_classes=10, block_seq_size=32, block_mlp_size=8)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5dcc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "vit_mixer = Mixer_ViT_Classifier((3, 64, 64), patch_size=[4, 4], hidden_channel=128, num_blocks=9, num_classes=200, block_seq_size=16, block_mlp_size=16)\n",
    "Sequence len: 256 ; Block size: 16\n",
    "Channel dim: 128 num heads: 8\n",
    "MLP dim: 128 ; Block size: 16\n",
    "\n",
    "\n",
    "vit_mixer = Mixer_ViT_Classifier((3, 64, 64), patch_size=[8, 8], hidden_channel=256, num_blocks=9, num_classes=200, block_seq_size=8, block_mlp_size=16)\n",
    "Sequence len: 64 ; Block size: 16\n",
    "Channel dim: 256 num heads: 16\n",
    "MLP dim: 256 ; Block size: 16\n",
    "\n",
    "vit_mixer = Mixer_ViT_Classifier((3, 64, 64), patch_size=[16, 16], hidden_channel=1024, num_blocks=9, num_classes=200, block_seq_size=4, block_mlp_size=32)\n",
    "Sequence len: 16 ; Block size: 4\n",
    "Channel dim: 1024 num heads: 32\n",
    "MLP dim: 1024 ; Block size: 32\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b22c327",
   "metadata": {},
   "source": [
    "### Automate the model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = 64\n",
    "patch_size = [16, 8, 4, 2, 1]\n",
    "expansion = [1024, 256, 128, 64, 64]\n",
    "\n",
    "for i in range(len(patch_size)):\n",
    "    ps, exp = patch_size[i], expansion[i]\n",
    "    print(i, ps, exp)\n",
    "    seq_len = (img//ps)**2\n",
    "    print('seq:', seq_len)\n",
    "    \n",
    "    fact_seq = int(2**np.ceil(np.log2(np.sqrt(seq_len))))\n",
    "    print(f'fact_seq: {fact_seq}')\n",
    "    \n",
    "    \n",
    "    fact_mlp = int(2**np.ceil(np.log2(np.sqrt(exp))))\n",
    "    print(f'fact_mlp: {fact_mlp}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
