{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsuman/Program_Files/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-b22782e32495>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/tsuman/Program_Files/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/tsuman/Program_Files/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/tsuman/Program_Files/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/tsuman/Program_Files/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/tsuman/Program_Files/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE parameters\n",
    "num_inputs = 10\n",
    "neurons_hid1 = 5\n",
    "neurons_hid2 = 3\n",
    "neurons_hid3 = 1\n",
    "\n",
    "learning_rate = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PINV(PSEUDO-INVERSE) function\n",
    "def pinv(a, rcond=1e-15):\n",
    "    s, u, v = tf.svd(a)\n",
    "    # Ignore singular values close to zero to prevent numerical overflow\n",
    "    limit = rcond * tf.reduce_max(s)\n",
    "    non_zero = tf.greater(s, limit)\n",
    "\n",
    "    reciprocal = tf.where(non_zero, tf.reciprocal(s), tf.zeros(s.shape))\n",
    "    lhs = tf.matmul(v, tf.diag(reciprocal))\n",
    "    return tf.matmul(lhs, u, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSPOSE OR PINV\n",
    "tie_weight = tf.transpose #pinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLACEHOLDER DEFINATION\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WEIGHTS DEFINATION\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "w1 = tf.Variable(initializer([num_inputs, neurons_hid1]), dtype=tf.float32)\n",
    "w1_ = tie_weight(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n## WEIGHTS DEFINATION\\ninitializer = tf.variance_scaling_initializer()\\n\\nw1 = tf.Variable(initializer([num_inputs, neurons_hid1]), dtype=tf.float32)\\n# w1 = tf.Variable(np.load('./save/weights/lw1.npy'))\\nw1_ = tie_weight(w1)\\n\\nw2 = tf.Variable(initializer([neurons_hid1, neurons_hid2]), dtype=tf.float32)\\nw2_ = tie_weight(w2)\\n\\n# LAYER MODELING OF :NN\\nhid_layer1 = act_func(tf.matmul(X, w1))\\n\\n# hid_layer2 = act_func(tf.matmul(hid_layer1, w2))\\n\\n# hid_layer1_= act_func(tf.matmul(hid_layer2, w2_))\\noutput_layer = (tf.matmul(hid_layer1, w1_))\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## WEIGHTS DEFINATION\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "w1 = tf.Variable(initializer([num_inputs, neurons_hid1]), dtype=tf.float32)\n",
    "# w1 = tf.Variable(np.load('./save/weights/lw1.npy'))\n",
    "w1_ = tie_weight(w1)\n",
    "\n",
    "w2 = tf.Variable(initializer([neurons_hid1, neurons_hid2]), dtype=tf.float32)\n",
    "w2_ = tie_weight(w2)\n",
    "\n",
    "# LAYER MODELING OF :NN\n",
    "hid_layer1 = act_func(tf.matmul(X, w1))\n",
    "\n",
    "# hid_layer2 = act_func(tf.matmul(hid_layer1, w2))\n",
    "\n",
    "# hid_layer1_= act_func(tf.matmul(hid_layer2, w2_))\n",
    "output_layer = (tf.matmul(hid_layer1, w1_))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTIVATION FUNCTION  [ lambda X:X  <OR>  tf.nn.relu  ]\n",
    "act_func = lambda X:X #tf.nn.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYER MODELING OF :NN\n",
    "hid_layer1 = act_func(tf.matmul(X, w1))\n",
    "output_layer = tf.nn.softmax(tf.matmul(hid_layer1, w1_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTIONS\n",
    "# loss = tf.reduce_mean(tf.square(output_layer - X))\n",
    "# loss = tf.reduce_mean(tf.abs(output_layer - X))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=X,logits=output_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss, var_list=[w1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARATION\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Complete. Training Loss: 1.4862391948699951\n",
      "Epoch 1 Complete. Training Loss: 1.4661521911621094\n",
      "Epoch 2 Complete. Training Loss: 1.463252067565918\n",
      "Epoch 3 Complete. Training Loss: 1.4623050689697266\n",
      "Epoch 4 Complete. Training Loss: 1.4618703126907349\n",
      "Epoch 5 Complete. Training Loss: 1.4616217613220215\n",
      "Epoch 6 Complete. Training Loss: 1.4614803791046143\n",
      "Epoch 7 Complete. Training Loss: 1.4613920450210571\n",
      "Epoch 8 Complete. Training Loss: 1.4613291025161743\n",
      "Epoch 9 Complete. Training Loss: 1.461286187171936\n",
      "Epoch 10 Complete. Training Loss: 1.4612547159194946\n",
      "Epoch 11 Complete. Training Loss: 1.4612308740615845\n",
      "Epoch 12 Complete. Training Loss: 1.461213231086731\n",
      "Epoch 13 Complete. Training Loss: 1.4611997604370117\n",
      "Epoch 14 Complete. Training Loss: 1.4611907005310059\n",
      "Epoch 15 Complete. Training Loss: 1.461181402206421\n",
      "Epoch 16 Complete. Training Loss: 1.4611760377883911\n",
      "Epoch 17 Complete. Training Loss: 1.4611706733703613\n",
      "Epoch 18 Complete. Training Loss: 1.461166501045227\n",
      "Epoch 19 Complete. Training Loss: 1.4611632823944092\n",
      "Epoch 20 Complete. Training Loss: 1.4611608982086182\n",
      "Epoch 21 Complete. Training Loss: 1.4611588716506958\n",
      "Epoch 22 Complete. Training Loss: 1.461157202720642\n",
      "Epoch 23 Complete. Training Loss: 1.4611557722091675\n",
      "Epoch 24 Complete. Training Loss: 1.4611549377441406\n",
      "Epoch 25 Complete. Training Loss: 1.4611539840698242\n",
      "Epoch 26 Complete. Training Loss: 1.461153268814087\n",
      "Epoch 27 Complete. Training Loss: 1.4611526727676392\n",
      "Epoch 28 Complete. Training Loss: 1.4611523151397705\n",
      "Epoch 29 Complete. Training Loss: 1.4611518383026123\n",
      "Epoch 30 Complete. Training Loss: 1.4611514806747437\n",
      "Epoch 31 Complete. Training Loss: 1.461151361465454\n",
      "Epoch 32 Complete. Training Loss: 1.461151123046875\n",
      "Epoch 33 Complete. Training Loss: 1.461150884628296\n",
      "Epoch 34 Complete. Training Loss: 1.461150884628296\n",
      "Epoch 35 Complete. Training Loss: 1.4611507654190063\n",
      "Epoch 36 Complete. Training Loss: 1.4611506462097168\n",
      "Epoch 37 Complete. Training Loss: 1.4611506462097168\n",
      "Epoch 38 Complete. Training Loss: 1.4611506462097168\n",
      "Epoch 39 Complete. Training Loss: 1.4611506462097168\n",
      "Epoch 40 Complete. Training Loss: 1.4611504077911377\n",
      "Epoch 41 Complete. Training Loss: 1.4611504077911377\n",
      "Epoch 42 Complete. Training Loss: 1.4611504077911377\n",
      "Epoch 43 Complete. Training Loss: 1.4611504077911377\n",
      "Epoch 44 Complete. Training Loss: 1.4611504077911377\n",
      "Epoch 45 Complete. Training Loss: 1.4611504077911377\n",
      "Epoch 46 Complete. Training Loss: 1.4611501693725586\n",
      "Epoch 47 Complete. Training Loss: 1.4611501693725586\n",
      "Epoch 48 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 49 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 50 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 51 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 52 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 53 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 54 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 55 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 56 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 57 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 58 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 59 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 60 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 61 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 62 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 63 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 64 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 65 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 66 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 67 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 68 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 69 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 70 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 71 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 72 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 73 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 74 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 75 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 76 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 77 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 78 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 79 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 80 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 81 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 82 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 83 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 84 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 85 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 86 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 87 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 88 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 89 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 90 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 91 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 92 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 93 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 94 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 95 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 96 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 97 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 98 Complete. Training Loss: 1.461150050163269\n",
      "Epoch 99 Complete. Training Loss: 1.461150050163269\n",
      "Finished Training the Model\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 150\n",
    "\n",
    "try:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        # Epoch == Entire Training Set\n",
    "        for epoch in range(num_epochs):\n",
    "            num_batches = mnist.train.num_examples // batch_size\n",
    "\n",
    "            # 150 batch size\n",
    "            for iteration in range(num_batches):\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "                sess.run(train, feed_dict={X: y_batch})\n",
    "\n",
    "            training_loss = loss.eval(feed_dict={X: y_batch})   \n",
    "            print(\"Epoch {} Complete. Training Loss: {}\".format(epoch,training_loss))\n",
    "            saver.save(sess, \"./save/label_ae_3.ckpt\")\n",
    "            \n",
    "            \n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print('Keyboard Interrupted')\n",
    "finally:\n",
    "    print('Finished Training the Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save/label_ae_3.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_test_labels = 10\n",
    "start_point = 15\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver.restore(sess,\"./save/label_ae_3.ckpt\")\n",
    "    \n",
    "    results,compressed = sess.run([output_layer, hid_layer1],\n",
    "                                  feed_dict={X:mnist.test.labels[start_point:num_test_labels+start_point]})\n",
    "    weight1 = w1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ORIGINAL VS RECONSTRUCTED\n",
    "f, a = plt.subplots(2, num_test_labels, figsize=(20, 4))\n",
    "for i in range(start_point,num_test_labels+start_point):\n",
    "    j = i-start_point\n",
    "    a[0][j].imshow(np.reshape(mnist.test.labels[i], (10, 1)))\n",
    "    a[1][j].imshow(np.reshape(results[j], (10, 1)))\n",
    "    print(mnist.test.labels[i])\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "    print(results[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.6009, -2.3916, -0.7849,  2.6548,  2.2748],\n",
       "       [ 2.2848,  1.2325,  1.6521,  2.5497, -1.7147],\n",
       "       [-0.597 ,  2.3952,  0.7842, -2.6543, -2.2742],\n",
       "       [-2.2769, -1.2249, -1.6487, -2.5467,  1.7102],\n",
       "       [ 2.5635, -1.9363,  1.7633, -2.1952,  0.4541],\n",
       "       [ 0.9431, -1.7221, -2.7217, -0.1498, -2.829 ],\n",
       "       [-2.5615,  1.942 , -1.7637,  2.195 , -0.4556],\n",
       "       [ 2.3994,  2.2183, -2.264 , -0.6344,  1.6576],\n",
       "       [-2.3946, -2.2115,  2.2628,  0.6323, -1.6581],\n",
       "       [-0.9423,  1.733 ,  2.7275,  0.1485,  2.8345]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "print(weight1.shape)\n",
    "np.save('./save/weights/lw1.npy',weight1)\n",
    "#wt1 = np.load('./save/weights/w1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
