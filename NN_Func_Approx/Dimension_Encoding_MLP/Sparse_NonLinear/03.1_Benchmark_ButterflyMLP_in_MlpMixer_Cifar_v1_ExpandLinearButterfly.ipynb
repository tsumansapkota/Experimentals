{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os, sys, pathlib, random, time, pickle, copy, json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3090'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED = 147\n",
    "# SEED = 258\n",
    "SEED = 369\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_train = transforms.Compose([\n",
    "    transforms.RandomCrop(size=32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "        std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "    ),\n",
    "])\n",
    "\n",
    "cifar_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "        std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"../../../../../_Datasets/cifar10/\", train=True, download=True, transform=cifar_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"../../../../../_Datasets/cifar10/\", train=False, download=True, transform=cifar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar_train = transforms.Compose([\n",
    "#     transforms.RandomCrop(size=32, padding=4),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(\n",
    "#         mean=[0.5071, 0.4865, 0.4409],\n",
    "#         std=[0.2009, 0.1984, 0.2023],\n",
    "#     ),\n",
    "# ])\n",
    "\n",
    "# cifar_test = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(\n",
    "#         mean=[0.5071, 0.4865, 0.4409],\n",
    "#         std=[0.2009, 0.1984, 0.2023],\n",
    "#     ),\n",
    "# ])\n",
    "\n",
    "# train_dataset = datasets.CIFAR100(root=\"../../../../../_Datasets/cifar100/\", train=True, download=True, transform=cifar_train)\n",
    "# test_dataset = datasets.CIFAR100(root=\"../../../../../_Datasets/cifar100/\", train=False, download=True, transform=cifar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpBLock(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_layers_ratio=[2], actf=nn.GELU):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        #### convert hidden layers ratio to list if integer is inputted\n",
    "        if isinstance(hidden_layers_ratio, int):\n",
    "            hidden_layers_ratio = [hidden_layers_ratio]\n",
    "            \n",
    "        self.hlr = [1]+hidden_layers_ratio+[1]\n",
    "        \n",
    "        self.mlp = []\n",
    "        ### for 1 hidden layer, we iterate 2 times\n",
    "        for h in range(len(self.hlr)-1):\n",
    "            i, o = int(self.hlr[h]*self.input_dim),\\\n",
    "                    int(self.hlr[h+1]*self.input_dim)\n",
    "            self.mlp.append(nn.Linear(i, o))\n",
    "            self.mlp.append(actf())\n",
    "        self.mlp = self.mlp[:-1]\n",
    "        \n",
    "        self.mlp = nn.Sequential(*self.mlp)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MlpBLock(\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=6, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=6, out_features=8, bias=True)\n",
       "    (3): GELU(approximate='none')\n",
       "    (4): Linear(in_features=8, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MlpBLock(2, [3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparse_nonlinear_lib_minimal as snl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlockMLP_MixerBlock(\n",
       "  (facto_nets): ModuleList(\n",
       "    (0-1): 2 x BlockMLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): BlockLinear: [16, 16, 16]\n",
       "        (1): ELU(alpha=1.0)\n",
       "        (2): BlockLinear: [16, 16, 16]\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snl.BlockMLP_MixerBlock(256, 16, hidden_layers_ratio=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snl.BlockMLP_MixerBlock(256, 16, hidden_layers_ratio=[1])(torch.randn(1, 256)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlockLinear_MixerBlock(\n",
       "  (facto_nets): ModuleList(\n",
       "    (0-1): 2 x BlockWeight: [16, 16, 16]\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snl.BlockLinear_MixerBlock(256, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snl.BlockLinear_MixerBlock(256, 16)(torch.randn(1, 256)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseResMlp(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, block_dim, hidden_expansion=2, actf=nn.GELU):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hex = hidden_expansion\n",
    "            \n",
    "        self.layers1s = [snl.BlockLinear_MixerBlock(input_dim, block_dim, bias=True) for _ in range(hidden_expansion)]\n",
    "        self.layers2s = [snl.BlockLinear_MixerBlock(input_dim, block_dim, bias=False) for _ in range(hidden_expansion)]\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.zeros(1, input_dim))\n",
    "        \n",
    "        self.layers1s = nn.ModuleList(self.layers1s)\n",
    "        self.actf = actf()\n",
    "        self.layers2s = nn.ModuleList(self.layers2s)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = 0\n",
    "        for i in range(self.hex):\n",
    "            h = self.layers1s[i](x)\n",
    "            h = self.actf(h)\n",
    "            h = self.layers2s[i](h)\n",
    "            y += h#*(1/self.hex)\n",
    "        return y+self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseResMlp(\n",
       "  (layers1s): ModuleList(\n",
       "    (0-1): 2 x BlockLinear_MixerBlock(\n",
       "      (facto_nets): ModuleList(\n",
       "        (0-1): 2 x BlockWeight: [16, 16, 16]\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (actf): GELU(approximate='none')\n",
       "  (layers2s): ModuleList(\n",
       "    (0-1): 2 x BlockLinear_MixerBlock(\n",
       "      (facto_nets): ModuleList(\n",
       "        (0-1): 2 x BlockWeight: [16, 16, 16]\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SparseResMlp(256, 16)\n",
    "# model.layers1s[0].bias\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparseResMlp(256, 16)(torch.randn(1, 256)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.9978e-26, grad_fn=<LinalgDetBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = snl.BlockLinear_MixerBlock(16, 4, bias=False)\n",
    "torch.det(lin(torch.eye(16)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Linear with Hidden Expansion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlockLinear: [4, 2, 4]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snl.BlockLinear(4, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_SparseLinear_Monarch_Deform(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, block_dim, hidden_expansion=2, actf=nn.ELU):\n",
    "        super().__init__()\n",
    "        assert input_dim%block_dim == 0, \"Input dim must be divisible by block dim\"\n",
    "        assert np.sqrt(input_dim) == block_dim, \"Input dim must be square of block dim\"\n",
    "        assert hidden_expansion >= 1\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.block_dim = block_dim\n",
    "        self.hidden_expansion = hidden_expansion\n",
    "        \n",
    "        self.hidden_dim = input_dim*hidden_expansion\n",
    "        \n",
    "        \n",
    "        def log_base(a, base):\n",
    "            return np.log(a) / np.log(base)\n",
    "        \n",
    "        num_layers = int(np.ceil(log_base(input_dim, base=block_dim)))\n",
    "        assert num_layers == 2, \"Num layers > 2 does not contribute to monarch\"\n",
    "        \n",
    "        self.linear0_0 = snl.BlockLinear(self.input_dim//block_dim, block_dim, block_dim*hidden_expansion, bias=False)\n",
    "        self.linear0_1 = snl.BlockLinear(self.hidden_dim//block_dim, block_dim, block_dim, bias=True)\n",
    "        self.stride0 = block_dim*hidden_expansion\n",
    "        \n",
    "        self.actf = actf()\n",
    "        self.linear1_0 = snl.BlockLinear(self.hidden_dim//block_dim, block_dim, block_dim, bias=False)\n",
    "        self.linear1_1 = snl.BlockLinear(self.input_dim//block_dim, block_dim*hidden_expansion, block_dim, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## Say shape of x is [BS, 121] > hidden expansion 2\n",
    "        \n",
    "        bs = x.shape[0] ## BS, input_dim\n",
    "        y = x\n",
    "        \n",
    "        y = y.view(bs, -1, self.block_dim) ## BS, num_blocks, block_dim ; [bs, 11, 11]\n",
    "        y = y.transpose(0,1).contiguous()  ## num_blocks, BS, block_dim ; [11, bs, 11]\n",
    "        y = self.linear0_0(y) ## num_blocks, BS, block_dim*hidden_expansion ; [11, bs, 22]\n",
    "        y = y.transpose(0,1).contiguous() ## BS, num_blocks, block_dim*hidden_expansion ; [bs, 11, 22]\n",
    "        y = y.view(bs, -1)  ## BS, hidden_dim ; [bs, 242]\n",
    "        \n",
    "        y = y.view(bs, self.block_dim, self.stride0).permute(2,0,1).contiguous() ## num_blocks, BS, block_dim; [22, bs, 11]\n",
    "        y = self.linear0_1(y) ## num_blocks, BS, block_dim ; [22, bs, 11]\n",
    "        y = y.transpose(0,1).contiguous() ## BS, num_blocks, block_dim ; [bs, 22, 11]\n",
    "        y = y.view(bs, -1)  ## BS, hidden_dim ; [bs, 242]\n",
    "        \n",
    "        ### First linear complete\n",
    "        y = self.actf(y)\n",
    "        \n",
    "        \n",
    "        y = y.view(bs, -1, self.block_dim) ## BS, num_blocks, block_dim ; [bs, 22, 11]\n",
    "        y = y.transpose(0,1).contiguous()  ## num_blocks, BS, block_dim ; [22, bs, 11]\n",
    "        y = self.linear1_0(y) ## num_blocks, BS, block_dim*hidden_expansion ; [22, bs, 11]\n",
    "        y = y.transpose(0,1).contiguous() ## BS, num_blocks, block_dim*hidden_expansion ; [bs, 22, 11]\n",
    "        y = y.view(bs, -1)  ## BS, hidden_dim ; [bs, 242]\n",
    "        \n",
    "        y = y.view(bs, self.stride0, self.block_dim).permute(2,0,1).contiguous() ## num_blocks, BS, block_dim; [11, bs, 22]\n",
    "        y = self.linear1_1(y) ## num_blocks, BS, block_dim ; [11, bs, 11]\n",
    "        y = y.transpose(0,1).contiguous() ## BS, num_blocks, block_dim ; [bs, 11, 11]\n",
    "        y = y.view(bs, -1)  ## BS, hidden_dim ; [bs, 121]\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP_SparseLinear_Monarch_Deform(\n",
       "  (linear0_0): BlockLinear: [2, 2, 2]\n",
       "  (linear0_1): BlockLinear: [2, 2, 2]\n",
       "  (actf): ELU(alpha=1.0)\n",
       "  (linear1_0): BlockLinear: [2, 2, 2]\n",
       "  (linear1_1): BlockLinear: [2, 2, 2]\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monarch = MLP_SparseLinear_Monarch_Deform(4, 2, hidden_expansion=1)\n",
    "monarch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6140e-09, grad_fn=<LinalgDetBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.det(monarch(torch.eye(4))) ## remove residual before testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5158, -0.0804,  0.3653,  0.1062],\n",
       "        [ 0.5260, -0.0833,  0.3730,  0.1205],\n",
       "        [ 0.5209, -0.0811,  0.3733,  0.1179],\n",
       "        [ 0.5216, -0.0814,  0.3731,  0.1170]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monarch(torch.eye(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asdasd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block MLP - without res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockMLP(nn.Module):\n",
    "    def __init__(self, input_dim, layer_dims, actf=nn.GELU):\n",
    "        super().__init__()\n",
    "        self.block_dim = layer_dims[0]\n",
    "        \n",
    "        assert input_dim%self.block_dim == 0, \"Input dim must be even number\"\n",
    "        ### Create a block MLP\n",
    "        self.mlp = []\n",
    "        n_blocks = input_dim//layer_dims[0]\n",
    "        for i in range(len(layer_dims)-1):\n",
    "            l = snl.BlockLinear(n_blocks, layer_dims[i], layer_dims[i+1])\n",
    "            a = actf()\n",
    "            self.mlp.append(l)\n",
    "            self.mlp.append(a)\n",
    "        self.mlp = self.mlp[:-1]\n",
    "        self.mlp = nn.Sequential(*self.mlp)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, dim = x.shape[0], x.shape[1]\n",
    "        x = x.view(bs, -1, self.block_dim).transpose(0,1)\n",
    "        x = self.mlp(x)\n",
    "        x = x.transpose(1,0).reshape(bs, -1)\n",
    "        return x\n",
    "    \n",
    "############################################################################\n",
    "############################################################################\n",
    "\n",
    "class BlockMLP_MixerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, block_dim, hidden_layers_ratio=[2], actf=nn.GELU):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert input_dim%block_dim == 0, \"Input dim must be even number\"\n",
    "        self.input_dim = input_dim\n",
    "        self.block_dim = block_dim\n",
    "        \n",
    "        def log_base(a, base):\n",
    "            return np.log(a) / np.log(base)\n",
    "        \n",
    "        num_layers = int(np.ceil(log_base(input_dim, base=block_dim)))\n",
    "        hidden_layers_ratio = [1] + hidden_layers_ratio + [1]\n",
    "        \n",
    "        block_layer_dims = [int(a*block_dim) for a in hidden_layers_ratio]\n",
    "        self.facto_nets = []\n",
    "        for i in range(num_layers):\n",
    "            net = BlockMLP(self.input_dim, block_layer_dims, actf)\n",
    "            self.facto_nets.append(net)\n",
    "            \n",
    "        self.facto_nets = nn.ModuleList(self.facto_nets)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        y = x\n",
    "        for i, fn in enumerate(self.facto_nets):\n",
    "            y = y.view(-1, self.block_dim, self.block_dim**i).permute(0, 2, 1).contiguous().view(bs, -1)\n",
    "            y = fn(y)\n",
    "            y = y.view(-1, self.block_dim**i, self.block_dim).permute(0, 2, 1).contiguous()\n",
    "\n",
    "        y = y.view(bs, -1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP-Mixer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, patch_dim, channel_dim, patch_mixing=\"dense\", channel_mixing=\"dense\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.valid_functions = [\"dense\", \"sparse_linear\", \"sparse_mlp\"]\n",
    "        assert patch_mixing in self.valid_functions\n",
    "        assert channel_mixing in self.valid_functions\n",
    "        \n",
    "        self.patch_dim = patch_dim\n",
    "        self.channel_dim = channel_dim\n",
    "        \n",
    "        self.ln0 = nn.LayerNorm(channel_dim)\n",
    "        self.mlp_patch = self.get_mlp(patch_dim, patch_mixing)    \n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(channel_dim)\n",
    "        self.mlp_channel = self.get_mlp(channel_dim, channel_mixing)\n",
    "    \n",
    "    def get_mlp(self, dim, mixing_function, actf=nn.GELU):\n",
    "        block_dim = int(np.sqrt(dim))\n",
    "        assert block_dim**2 == dim, \"Sparsifying dimension must be a square number\"\n",
    "        \n",
    "        HIDDEN_EXPANSION = 2 #2\n",
    "        if mixing_function == self.valid_functions[0]:\n",
    "            mlp = MlpBLock(dim, [HIDDEN_EXPANSION], actf)\n",
    "        elif mixing_function == self.valid_functions[1]:\n",
    "#             mlp = SparseResMlp(dim, block_dim, HIDDEN_EXPANSION, actf)\n",
    "            mlp = MLP_SparseLinear_Monarch_Deform(dim, block_dim, HIDDEN_EXPANSION, actf)\n",
    "\n",
    "        elif mixing_function == self.valid_functions[2]:\n",
    "            mlp = BlockMLP_MixerBlock(dim, block_dim, [HIDDEN_EXPANSION], actf)\n",
    "        return mlp\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## x has shape-> N, nP, nC/hidden_dims; C=Channel, P=Patch\n",
    "        \n",
    "        ######## !!!! Can use same mixer on shape of -> N, C, P;\n",
    "        \n",
    "        #### mix per patch\n",
    "        y = self.ln0(x) ### per channel layer normalization ?? \n",
    "        y = torch.swapaxes(y, -1, -2).contiguous()\n",
    "        \n",
    "        y = y.view(-1, self.patch_dim)\n",
    "        y = self.mlp_patch(y)\n",
    "        y = y.view(-1, self.channel_dim, self.patch_dim)\n",
    "        \n",
    "        y = torch.swapaxes(y, -1, -2)\n",
    "        x = x+y\n",
    "        \n",
    "        #### mix per channel \n",
    "        y = self.ln1(x)\n",
    "        y = y.view(-1, self.channel_dim)\n",
    "        y = self.mlp_channel(y)\n",
    "        y = y.view(-1, self.patch_dim, self.channel_dim)\n",
    "        \n",
    "        x = x+y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MixerBlock(nn.Module):\n",
    "    \n",
    "#     def __init__(self, patch_dim, channel_dim):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.ln0 = nn.LayerNorm(channel_dim)\n",
    "#         self.mlp_patch = MlpBLock(patch_dim, [2])\n",
    "#         self.ln1 = nn.LayerNorm(channel_dim)\n",
    "#         self.mlp_channel = MlpBLock(channel_dim, [2])\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         ## x has shape-> N, nP, nC/hidden_dims; C=Channel, P=Patch\n",
    "        \n",
    "#         ######## !!!! Can use same mixer on shape of -> N, C, P;\n",
    "        \n",
    "#         #### mix per patch\n",
    "#         y = self.ln0(x) ### per channel layer normalization ?? \n",
    "#         y = torch.swapaxes(y, -1, -2)\n",
    "#         y = self.mlp_patch(y)\n",
    "#         y = torch.swapaxes(y, -1, -2)\n",
    "#         x = x+y\n",
    "        \n",
    "#         #### mix per channel \n",
    "#         y = self.ln1(x)\n",
    "#         y = self.mlp_channel(y)\n",
    "#         x = x+y\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixerBlock(\n",
       "  (ln0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp_patch): MLP_SparseLinear_Monarch_Deform(\n",
       "    (linear0_0): BlockLinear: [4, 4, 8]\n",
       "    (linear0_1): BlockLinear: [8, 4, 4]\n",
       "    (actf): GELU(approximate='none')\n",
       "    (linear1_0): BlockLinear: [8, 4, 4]\n",
       "    (linear1_1): BlockLinear: [4, 8, 4]\n",
       "  )\n",
       "  (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp_channel): MLP_SparseLinear_Monarch_Deform(\n",
       "    (linear0_0): BlockLinear: [16, 16, 32]\n",
       "    (linear0_1): BlockLinear: [32, 16, 16]\n",
       "    (actf): GELU(approximate='none')\n",
       "    (linear1_0): BlockLinear: [32, 16, 16]\n",
       "    (linear1_1): BlockLinear: [16, 32, 16]\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MixerBlock(2*2*4, 16*16, channel_mixing=\"sparse_linear\", patch_mixing=\"sparse_linear\")\n",
    "# model = MixerBlock(2*2*4, 16*16)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 256])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(1, 2*2*4, 16*16)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpMixer(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_dim:tuple, patch_size:tuple, hidden_expansion:float, num_blocks:int, num_classes:int,\n",
    "                patch_mixing:str, channel_mixing:str):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_dim = image_dim ### must contain (C, H, W) or (H, W)\n",
    "        self.scaler = nn.UpsamplingBilinear2d(size=(self.img_dim[-2], self.img_dim[-1]))\n",
    "        \n",
    "        ### find patch dim\n",
    "        d0 = int(image_dim[-2]/patch_size[0])\n",
    "        d1 = int(image_dim[-1]/patch_size[1])\n",
    "        assert d0*patch_size[0]==image_dim[-2], \"Image must be divisible into patch size\"\n",
    "        assert d1*patch_size[1]==image_dim[-1], \"Image must be divisible into patch size\"\n",
    "#         self.d0, self.d1 = d0, d1 ### number of patches in each axis\n",
    "        __patch_size = patch_size[0]*patch_size[1]*image_dim[0] ## number of channels in each patch\n",
    "    \n",
    "        ### find channel dim\n",
    "        channel_size = d0*d1 ## number of patches\n",
    "        \n",
    "        ### after the number of channels are changed\n",
    "        init_dim = __patch_size\n",
    "#         final_dim = int(patch_size[0]*patch_size[1]*hidden_expansion)\n",
    "        final_dim = int(init_dim*hidden_expansion)\n",
    "\n",
    "        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        #### rescale the patches (patch wise image non preserving transform, unlike bilinear interpolation)\n",
    "        self.channel_change = nn.Linear(init_dim, final_dim)\n",
    "        print(f\"MLP Mixer : Channes per patch -> Initial:{init_dim} Final:{final_dim}\")\n",
    "        \n",
    "        \n",
    "        self.channel_dim = final_dim\n",
    "        self.patch_dim = channel_size\n",
    "        \n",
    "        self.mixer_blocks = []\n",
    "        for i in range(num_blocks):\n",
    "            self.mixer_blocks.append(MixerBlock(self.patch_dim, self.channel_dim, patch_mixing, channel_mixing))\n",
    "        self.mixer_blocks = nn.Sequential(*self.mixer_blocks)\n",
    "        \n",
    "        self.linear = nn.Linear(self.patch_dim*self.channel_dim, num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.scaler(x)\n",
    "        x = self.unfold(x).swapaxes(-1, -2)\n",
    "        x = self.channel_change(x)\n",
    "        x = self.mixer_blocks(x)\n",
    "        x = self.linear(x.view(bs, -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Mixer : Channes per patch -> Initial:48 Final:121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MlpMixer(\n",
       "  (scaler): UpsamplingBilinear2d(size=(32, 32), mode='bilinear')\n",
       "  (unfold): Unfold(kernel_size=(4, 4), dilation=1, padding=0, stride=(4, 4))\n",
       "  (channel_change): Linear(in_features=48, out_features=121, bias=True)\n",
       "  (mixer_blocks): Sequential(\n",
       "    (0): MixerBlock(\n",
       "      (ln0): LayerNorm((121,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [8, 8, 16]\n",
       "        (linear0_1): BlockLinear: [16, 8, 8]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [16, 8, 8]\n",
       "        (linear1_1): BlockLinear: [8, 16, 8]\n",
       "      )\n",
       "      (ln1): LayerNorm((121,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [11, 11, 22]\n",
       "        (linear0_1): BlockLinear: [22, 11, 11]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [22, 11, 11]\n",
       "        (linear1_1): BlockLinear: [11, 22, 11]\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=7744, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixer = MlpMixer((3, 32, 32), (4, 4), hidden_expansion=2.53, num_blocks=1, num_classes=10, patch_mixing=\"sparse_linear\", channel_mixing=\"sparse_linear\")\n",
    "mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  99162\n"
     ]
    }
   ],
   "source": [
    "print(\"number of params: \", sum(p.numel() for p in mixer.parameters())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0244, -0.6296,  0.3240, -0.0677, -0.0714,  0.2028,  0.2985, -0.1748,\n",
       "         -0.0279, -0.3832]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixer(torch.randn(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Mixer : Channes per patch -> Initial:48 Final:144\n"
     ]
    }
   ],
   "source": [
    "model = MlpMixer((3, 4*9, 4*9), (4, 4), hidden_expansion=3.0, num_blocks=10, num_classes=10,\n",
    "                patch_mixing=\"sparse_linear\", channel_mixing=\"sparse_linear\")\n",
    "#                 patch_mixing=\"sparse_mlp\", channel_mixing=\"sparse_mlp\")\n",
    "#                 patch_mixing=\"dense\", channel_mixing=\"dense\")\n",
    "                 \n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MlpMixer(\n",
       "  (scaler): UpsamplingBilinear2d(size=(36, 36), mode='bilinear')\n",
       "  (unfold): Unfold(kernel_size=(4, 4), dilation=1, padding=0, stride=(4, 4))\n",
       "  (channel_change): Linear(in_features=48, out_features=144, bias=True)\n",
       "  (mixer_blocks): Sequential(\n",
       "    (0): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [9, 9, 18]\n",
       "        (linear0_1): BlockLinear: [18, 9, 9]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [18, 9, 9]\n",
       "        (linear1_1): BlockLinear: [9, 18, 9]\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [12, 12, 24]\n",
       "        (linear0_1): BlockLinear: [24, 12, 12]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [24, 12, 12]\n",
       "        (linear1_1): BlockLinear: [12, 24, 12]\n",
       "      )\n",
       "    )\n",
       "    (1): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [9, 9, 18]\n",
       "        (linear0_1): BlockLinear: [18, 9, 9]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [18, 9, 9]\n",
       "        (linear1_1): BlockLinear: [9, 18, 9]\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [12, 12, 24]\n",
       "        (linear0_1): BlockLinear: [24, 12, 12]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [24, 12, 12]\n",
       "        (linear1_1): BlockLinear: [12, 24, 12]\n",
       "      )\n",
       "    )\n",
       "    (2): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [9, 9, 18]\n",
       "        (linear0_1): BlockLinear: [18, 9, 9]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [18, 9, 9]\n",
       "        (linear1_1): BlockLinear: [9, 18, 9]\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [12, 12, 24]\n",
       "        (linear0_1): BlockLinear: [24, 12, 12]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [24, 12, 12]\n",
       "        (linear1_1): BlockLinear: [12, 24, 12]\n",
       "      )\n",
       "    )\n",
       "    (3): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [9, 9, 18]\n",
       "        (linear0_1): BlockLinear: [18, 9, 9]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [18, 9, 9]\n",
       "        (linear1_1): BlockLinear: [9, 18, 9]\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [12, 12, 24]\n",
       "        (linear0_1): BlockLinear: [24, 12, 12]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [24, 12, 12]\n",
       "        (linear1_1): BlockLinear: [12, 24, 12]\n",
       "      )\n",
       "    )\n",
       "    (4): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [9, 9, 18]\n",
       "        (linear0_1): BlockLinear: [18, 9, 9]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [18, 9, 9]\n",
       "        (linear1_1): BlockLinear: [9, 18, 9]\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [12, 12, 24]\n",
       "        (linear0_1): BlockLinear: [24, 12, 12]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [24, 12, 12]\n",
       "        (linear1_1): BlockLinear: [12, 24, 12]\n",
       "      )\n",
       "    )\n",
       "    (5): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [9, 9, 18]\n",
       "        (linear0_1): BlockLinear: [18, 9, 9]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [18, 9, 9]\n",
       "        (linear1_1): BlockLinear: [9, 18, 9]\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [12, 12, 24]\n",
       "        (linear0_1): BlockLinear: [24, 12, 12]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [24, 12, 12]\n",
       "        (linear1_1): BlockLinear: [12, 24, 12]\n",
       "      )\n",
       "    )\n",
       "    (6): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [9, 9, 18]\n",
       "        (linear0_1): BlockLinear: [18, 9, 9]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [18, 9, 9]\n",
       "        (linear1_1): BlockLinear: [9, 18, 9]\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [12, 12, 24]\n",
       "        (linear0_1): BlockLinear: [24, 12, 12]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [24, 12, 12]\n",
       "        (linear1_1): BlockLinear: [12, 24, 12]\n",
       "      )\n",
       "    )\n",
       "    (7): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [9, 9, 18]\n",
       "        (linear0_1): BlockLinear: [18, 9, 9]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [18, 9, 9]\n",
       "        (linear1_1): BlockLinear: [9, 18, 9]\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [12, 12, 24]\n",
       "        (linear0_1): BlockLinear: [24, 12, 12]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [24, 12, 12]\n",
       "        (linear1_1): BlockLinear: [12, 24, 12]\n",
       "      )\n",
       "    )\n",
       "    (8): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [9, 9, 18]\n",
       "        (linear0_1): BlockLinear: [18, 9, 9]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [18, 9, 9]\n",
       "        (linear1_1): BlockLinear: [9, 18, 9]\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [12, 12, 24]\n",
       "        (linear0_1): BlockLinear: [24, 12, 12]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [24, 12, 12]\n",
       "        (linear1_1): BlockLinear: [12, 24, 12]\n",
       "      )\n",
       "    )\n",
       "    (9): MixerBlock(\n",
       "      (ln0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_patch): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [9, 9, 18]\n",
       "        (linear0_1): BlockLinear: [18, 9, 9]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [18, 9, 9]\n",
       "        (linear1_1): BlockLinear: [9, 18, 9]\n",
       "      )\n",
       "      (ln1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_channel): MLP_SparseLinear_Monarch_Deform(\n",
       "        (linear0_0): BlockLinear: [12, 12, 24]\n",
       "        (linear0_1): BlockLinear: [24, 12, 12]\n",
       "        (actf): GELU(approximate='none')\n",
       "        (linear1_0): BlockLinear: [24, 12, 12]\n",
       "        (linear1_1): BlockLinear: [12, 24, 12]\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=11664, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  209070\n"
     ]
    }
   ],
   "source": [
    "# print(\"number of params: \", sum(p.numel() for p in model.parameters())) \n",
    "print(\"number of params: \", sum(p.numel() for p in model.mixer_blocks.parameters())) \n",
    "\n",
    "### Dense: 1,104,390\n",
    "### Sparse MLP: 215820\n",
    "### Sparse Linear: 209,070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(1, 3, 32, 32).to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asdasd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = f'mlp_mixer_sparse-mlp_c10_s{SEED}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 200\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAT ={'train_stat':[], 'test_stat':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Following is copied from \n",
    "### https://github.com/kuangliu/pytorch-cifar/blob/master/main.py\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "    STAT['train_stat'].append((epoch, train_loss/(batch_idx+1), 100.*correct/total)) ### (Epochs, Loss, Acc)\n",
    "    print(f\"[Train] {epoch} Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = -1\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    time_taken = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            start = time.time()-start\n",
    "            time_taken.append(start)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    STAT['test_stat'].append((epoch, test_loss/(batch_idx+1), 100.*correct/total, np.mean(time_taken))) ### (Epochs, Loss, Acc, time)\n",
    "    print(f\"[Test] {epoch} Loss: {test_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "    \n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'model': model.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        if not os.path.isdir('models'):\n",
    "            os.mkdir('models')\n",
    "        torch.save(state, f'./models_v1/{model_name}.pth')\n",
    "        best_acc = acc\n",
    "        \n",
    "    with open(f\"./models_v1/stats/{model_name}_data.json\", 'w') as f:\n",
    "        json.dump(STAT, f, indent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "# resume = False\n",
    "\n",
    "# if resume:\n",
    "#     # Load checkpoint.\n",
    "#     print('==> Resuming from checkpoint..')\n",
    "#     assert os.path.isdir('./models'), 'Error: no checkpoint directory found!'\n",
    "#     checkpoint = torch.load(f'./models/{model_name}.pth')\n",
    "#     model.load_state_dict(checkpoint['model'])\n",
    "#     best_acc = checkpoint['acc']\n",
    "#     start_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Train the whole damn thing\n",
    "\n",
    "# for epoch in range(start_epoch, start_epoch+EPOCHS): ## for 200 epochs\n",
    "#     train(epoch)\n",
    "#     test(epoch)\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(f'./models/{model_name}.pth')\n",
    "# best_acc = checkpoint['acc']\n",
    "# start_epoch = checkpoint['epoch']\n",
    "\n",
    "# best_acc, start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_stat = np.array(STAT['train_stat'])\n",
    "# test_stat = np.array(STAT['test_stat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_stat[:,1], label='train')\n",
    "# plt.plot(test_stat[:,1], label='test')\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend()\n",
    "# plt.savefig(f\"./output/plots/{model_name}_loss.svg\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_stat[:,2], label='train')\n",
    "# plt.plot(test_stat[:,2], label='test')\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.legend()\n",
    "# plt.savefig(f\"./output/plots/{model_name}_accs.svg\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(seed, ds):\n",
    "    BS = 64\n",
    "    if ds == 'c100': BS = 128\n",
    "    torch.manual_seed(seed)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BS, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BS, shuffle=False, num_workers=2)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir models_v1/\n",
    "# ! mkdir models_v1/stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark():\n",
    "    global model, optimizer, train_loader, test_loader, model_name, criterion, STAT, best_acc\n",
    "    EPOCHS = 200\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr = 0.001\n",
    "#     DS = 'c100'\n",
    "    DS = 'c10'\n",
    "#     for SEED in [147, 258, 369]:\n",
    "    for SEED in [741, 852, 963, 159, 357]:\n",
    "        for num_layers in [7]: ##[7, 10]\n",
    "#             for i in range(3): ## 3 models training\n",
    "            for i in [1]: ## Test Linear Only\n",
    "\n",
    "                print(\"Experiment index:\", i)\n",
    "                train_loader, test_loader = get_data_loaders(SEED, DS)\n",
    "                torch.manual_seed(SEED)\n",
    "                num_cls = 10\n",
    "                if DS=='c100': num_cls = 100\n",
    "                ### FOR ORIGINAL MIXER\n",
    "                if i == 0:\n",
    "                    model = MlpMixer((3, 32, 32), (4, 4), hidden_expansion=2.53, num_blocks=num_layers, \n",
    "                                     num_classes=num_cls, patch_mixing=\"dense\", channel_mixing=\"dense\")\n",
    "                    model_name = f'mlp_mixer_dense_l{num_layers}_{DS}_s{SEED}'\n",
    "                elif i == 1:\n",
    "                    model = MlpMixer((3, 32, 32), (4, 4), hidden_expansion=2.53, num_blocks=num_layers, \n",
    "                                     num_classes=num_cls, patch_mixing=\"sparse_linear\", channel_mixing=\"sparse_linear\")\n",
    "                    model_name = f'mlp_mixer_sparseLinear_l{num_layers}_{DS}_s{SEED}'\n",
    "                elif i == 2:\n",
    "                    model = MlpMixer((3, 32, 32), (4, 4), hidden_expansion=2.53, num_blocks=num_layers, \n",
    "                                     num_classes=num_cls, patch_mixing=\"sparse_mlp\", channel_mixing=\"sparse_mlp\")\n",
    "                    model_name = f'mlp_mixer_sparseMlp_l{num_layers}_{DS}_s{SEED}'\n",
    "                else:\n",
    "                    print(\"JPT........!!!!\")\n",
    "                    continue\n",
    "                    \n",
    "                model = model.to(device)\n",
    "                model = torch.compile(model)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "                num_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "                model_name = \"03.1_\" + model_name + \"_h2\"\n",
    "                print(f\"EXPERIMENTING FOR : {model_name} | params: {num_params}  .......\\n.......\")\n",
    "                \n",
    "#                 continue\n",
    "                STAT ={'train_stat':[], 'test_stat':[], 'num_params':num_params}\n",
    "                best_acc = -1\n",
    "                for epoch in range(0, EPOCHS): ## for 200 epochs\n",
    "                    train(epoch)\n",
    "                    test(epoch)\n",
    "                    scheduler.step()\n",
    "                print(f\"Training finished\\n\")\n",
    "                pass\n",
    "            pass\n",
    "        pass\n",
    "    return 0           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## warning - Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Mixer : Channes per patch -> Initial:48 Final:121\n",
      "mlp_mixer_dense_l7_c10_s-1\n",
      "Computational complexity:       40.89 MMac\n",
      "Number of parameters:           615.29 k\n",
      "\n",
      "MLP Mixer : Channes per patch -> Initial:48 Final:121\n",
      "mlp_mixer_sparseLinear_l7_c10_s-1\n",
      "Computational complexity:       774.53 KMac\n",
      "Number of parameters:           193.86 k\n",
      "\n",
      "MLP Mixer : Channes per patch -> Initial:48 Final:121\n",
      "mlp_mixer_sparseMlp_l7_c10_s-1\n",
      "Computational complexity:       991.36 KMac\n",
      "Number of parameters:           197.75 k\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "SEED = -1\n",
    "for num_cls in [10]:\n",
    "    DS = f\"c{num_cls}\"\n",
    "    for num_layers in [7]:\n",
    "        for i in range(3):\n",
    "            if i == 0:\n",
    "                model = MlpMixer((3, 32, 32), (4, 4), hidden_expansion=2.53, num_blocks=num_layers, \n",
    "                                 num_classes=num_cls, patch_mixing=\"dense\", channel_mixing=\"dense\")\n",
    "                model_name = f'mlp_mixer_dense_l{num_layers}_{DS}_s{SEED}'\n",
    "            elif i == 1:\n",
    "                model = MlpMixer((3, 32, 32), (4, 4), hidden_expansion=2.53, num_blocks=num_layers, \n",
    "                                 num_classes=num_cls, patch_mixing=\"sparse_linear\", channel_mixing=\"sparse_linear\")\n",
    "                model_name = f'mlp_mixer_sparseLinear_l{num_layers}_{DS}_s{SEED}'\n",
    "            elif i == 2:\n",
    "                model = MlpMixer((3, 32, 32), (4, 4), hidden_expansion=2.53, num_blocks=num_layers, \n",
    "                                 num_classes=num_cls, patch_mixing=\"sparse_mlp\", channel_mixing=\"sparse_mlp\")\n",
    "                model_name = f'mlp_mixer_sparseMlp_l{num_layers}_{DS}_s{SEED}'\n",
    "            else:\n",
    "                print(\"JPT........!!!!\")\n",
    "                continue\n",
    "\n",
    "            macs, params = get_model_complexity_info(model, (3, 32, 32), as_strings=True, ignore_modules=['channel_change'],\n",
    "                                           print_per_layer_stat=False, verbose=False)\n",
    "            \n",
    "            print(model_name)\n",
    "#             print(model)\n",
    "            print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "            print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "            print('')\n",
    "            pass\n",
    "        pass\n",
    "    pass\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment index: 1\n",
      "MLP Mixer : Channes per patch -> Initial:48 Final:121\n",
      "EXPERIMENTING FOR : 03.1_mlp_mixer_sparseLinear_l7_c10_s741_h2 | params: 193860  .......\n",
      ".......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:33<00:00, 23.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 0 Loss: 1.783 | Acc: 36.578 18289/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:08<00:00, 17.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 0 Loss: 1.428 | Acc: 48.640 4864/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:13<00:00, 57.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 1 Loss: 1.442 | Acc: 48.076 24038/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 153.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 1 Loss: 1.218 | Acc: 56.850 5685/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:13<00:00, 57.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 2 Loss: 1.273 | Acc: 54.562 27281/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 148.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 2 Loss: 1.173 | Acc: 57.930 5793/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:13<00:00, 58.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 3 Loss: 1.187 | Acc: 57.940 28970/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 150.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 3 Loss: 1.098 | Acc: 61.160 6116/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:13<00:00, 58.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 4 Loss: 1.125 | Acc: 60.388 30194/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 147.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 4 Loss: 0.994 | Acc: 65.330 6533/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:13<00:00, 58.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 5 Loss: 1.079 | Acc: 61.984 30992/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 150.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 5 Loss: 0.974 | Acc: 66.190 6619/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:13<00:00, 57.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 6 Loss: 1.039 | Acc: 63.088 31544/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 148.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 6 Loss: 0.947 | Acc: 67.270 6727/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:13<00:00, 56.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 7 Loss: 1.010 | Acc: 64.610 32305/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 131.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 7 Loss: 0.909 | Acc: 68.210 6821/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:15<00:00, 50.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 8 Loss: 0.977 | Acc: 65.770 32885/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 98.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 8 Loss: 0.875 | Acc: 69.560 6956/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:19<00:00, 39.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 9 Loss: 0.959 | Acc: 66.480 33240/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 151.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 9 Loss: 0.874 | Acc: 69.360 6936/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:14<00:00, 55.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 10 Loss: 0.927 | Acc: 67.516 33758/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 100.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 10 Loss: 0.850 | Acc: 70.620 7062/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 35.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 11 Loss: 0.898 | Acc: 68.566 34283/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 98.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 11 Loss: 0.807 | Acc: 71.650 7165/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 35.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 12 Loss: 0.886 | Acc: 68.966 34483/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 101.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 12 Loss: 0.810 | Acc: 71.540 7154/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 35.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 13 Loss: 0.867 | Acc: 69.658 34829/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 99.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 13 Loss: 0.834 | Acc: 70.690 7069/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 35.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 14 Loss: 0.855 | Acc: 70.038 35019/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 100.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 14 Loss: 0.791 | Acc: 72.770 7277/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 35.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 15 Loss: 0.832 | Acc: 70.792 35396/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 97.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 15 Loss: 0.783 | Acc: 72.200 7220/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 35.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 16 Loss: 0.826 | Acc: 71.296 35648/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 98.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 16 Loss: 0.762 | Acc: 73.190 7319/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 17 Loss: 0.802 | Acc: 71.886 35943/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 98.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 17 Loss: 0.770 | Acc: 73.400 7340/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 18 Loss: 0.791 | Acc: 72.358 36179/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 98.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 18 Loss: 0.766 | Acc: 73.540 7354/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 19 Loss: 0.780 | Acc: 72.708 36354/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 99.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 19 Loss: 0.756 | Acc: 73.600 7360/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 20 Loss: 0.772 | Acc: 72.760 36380/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 101.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 20 Loss: 0.763 | Acc: 73.670 7367/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 21 Loss: 0.757 | Acc: 73.524 36762/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 99.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 21 Loss: 0.730 | Acc: 74.680 7468/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 22 Loss: 0.749 | Acc: 73.896 36948/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 99.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 22 Loss: 0.723 | Acc: 74.540 7454/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 23 Loss: 0.732 | Acc: 74.636 37318/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 99.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 23 Loss: 0.718 | Acc: 74.800 7480/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 24 Loss: 0.732 | Acc: 74.456 37228/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 100.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 24 Loss: 0.693 | Acc: 75.730 7573/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 35.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 25 Loss: 0.716 | Acc: 74.890 37445/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 97.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 25 Loss: 0.697 | Acc: 75.810 7581/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 26 Loss: 0.704 | Acc: 75.328 37664/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 100.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 26 Loss: 0.704 | Acc: 75.960 7596/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 27 Loss: 0.702 | Acc: 75.216 37608/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 99.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 27 Loss: 0.687 | Acc: 76.220 7622/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 28 Loss: 0.698 | Acc: 75.636 37818/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 100.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 28 Loss: 0.693 | Acc: 75.960 7596/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 29 Loss: 0.682 | Acc: 76.118 38059/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 99.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 29 Loss: 0.680 | Acc: 76.880 7688/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 30 Loss: 0.673 | Acc: 76.420 38210/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 98.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 30 Loss: 0.682 | Acc: 76.560 7656/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 31 Loss: 0.667 | Acc: 76.940 38470/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 99.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 31 Loss: 0.688 | Acc: 76.260 7626/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 32 Loss: 0.660 | Acc: 77.030 38515/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 101.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 32 Loss: 0.680 | Acc: 76.910 7691/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 33 Loss: 0.655 | Acc: 76.800 38400/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 98.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 33 Loss: 0.670 | Acc: 77.010 7701/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 34 Loss: 0.646 | Acc: 77.266 38633/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 100.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 34 Loss: 0.668 | Acc: 77.000 7700/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 35 Loss: 0.638 | Acc: 77.724 38862/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 101.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 35 Loss: 0.658 | Acc: 77.320 7732/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 36 Loss: 0.632 | Acc: 77.732 38866/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 100.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 36 Loss: 0.648 | Acc: 77.680 7768/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 37 Loss: 0.624 | Acc: 78.166 39083/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 99.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 37 Loss: 0.643 | Acc: 78.020 7802/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 38 Loss: 0.622 | Acc: 78.144 39072/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 100.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 38 Loss: 0.640 | Acc: 77.850 7785/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 39 Loss: 0.608 | Acc: 78.522 39261/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 100.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 39 Loss: 0.666 | Acc: 77.040 7704/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 40 Loss: 0.609 | Acc: 78.694 39347/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 100.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 40 Loss: 0.651 | Acc: 77.780 7778/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 41 Loss: 0.601 | Acc: 78.810 39405/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 99.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 41 Loss: 0.645 | Acc: 77.620 7762/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 42 Loss: 0.601 | Acc: 78.930 39465/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 99.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 42 Loss: 0.670 | Acc: 77.060 7706/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 43 Loss: 0.586 | Acc: 79.410 39705/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 96.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 43 Loss: 0.658 | Acc: 77.450 7745/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 44 Loss: 0.588 | Acc: 79.496 39748/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 96.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 44 Loss: 0.624 | Acc: 78.850 7885/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 45 Loss: 0.579 | Acc: 79.696 39848/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 99.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 45 Loss: 0.613 | Acc: 79.060 7906/10000\n",
      "Saving..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 46 Loss: 0.573 | Acc: 79.948 39974/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 157/157 [00:01<00:00, 99.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 46 Loss: 0.638 | Acc: 78.180 7818/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:22<00:00, 34.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] 47 Loss: 0.567 | Acc: 80.162 40081/50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                             | 0/157 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The initialization on three methods are different.\n",
    "Try to remove that gap.\n",
    "\n",
    "1) Initializing everything to kaiming_uniform_ similar to nn.Linear\n",
    "    - Might have better initialization (unclear how pytorch handles init for with with block-sparse)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
