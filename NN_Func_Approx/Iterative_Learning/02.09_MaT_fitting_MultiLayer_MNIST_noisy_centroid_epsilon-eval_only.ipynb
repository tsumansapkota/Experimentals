{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bf2e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms as T\n",
    "\n",
    "import random, os, pathlib, time\n",
    "from tqdm import tqdm\n",
    "# from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9cb065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fddf618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os, time, sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd7177eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtnnlib as dtnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc49bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=[0.5,],\n",
    "        std=[0.5,],\n",
    "    ),\n",
    "])\n",
    "\n",
    "# train_dataset = datasets.FashionMNIST(root=\"../../../_Datasets/\", train=True, download=True, transform=mnist_transform)\n",
    "# test_dataset = datasets.FashionMNIST(root=\"../../../_Datasets/\", train=False, download=True, transform=mnist_transform)\n",
    "train_dataset = datasets.MNIST(root=\"../../../_Datasets/\", train=True, download=True, transform=mnist_transform)\n",
    "test_dataset = datasets.MNIST(root=\"../../../_Datasets/\", train=False, download=True, transform=mnist_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cbbb66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_loader = data.DataLoader(dataset=train_dataset, num_workers=4, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, num_workers=4, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de4dc385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 28, 28]) torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "for xx, yy in train_loader:\n",
    "    xx, yy = xx.to(device), yy.to(device)\n",
    "    print(xx.shape, yy.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc36eeb",
   "metadata": {},
   "source": [
    "## 1 Layer epsilon Softmax MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de0d7886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIdea: to not use epsilon softmax during training, but only when checking uncertainity.\\nTrain -> do not store epsilon.\\nEval -> return without epsilon, but also store epsilon..\\n\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Idea: to not use epsilon softmax during training, but only when checking uncertainity.\n",
    "Train -> do not store epsilon.\n",
    "Eval -> return without epsilon, but also store epsilon..\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b8b4ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceTransform_Epsilon_eval(dtnn.DistanceTransformBase):\n",
    "    \n",
    "    def __init__(self, input_dim, num_centers, p=2, bias=False, epsilon=0.1):\n",
    "        super().__init__(input_dim, num_centers, p=2)\n",
    "        \n",
    "        nc = num_centers\n",
    "        self.scaler = nn.Parameter(torch.log(torch.ones(1, 1)*1))\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def forward(self, x, calculate_epsilon=False):\n",
    "        dists = super().forward(x)\n",
    "        \n",
    "        if calculate_epsilon:\n",
    "            dists = torch.cat([dists, torch.ones(len(x), 1, dtype=x.dtype)*self.epsilon], dim=1)\n",
    "        \n",
    "        ### normalize similar to UMAP\n",
    "        dists = dists/torch.sqrt(dists.var(dim=1, keepdim=True)+1e-9)\n",
    "#         dists = dists/(torch.norm(dists, dim=1, keepdim=True)+1e-9)\n",
    "\n",
    "        ## scale the dists\n",
    "        dists = -dists*torch.exp(self.scaler)\n",
    "    \n",
    "        return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8399b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTeSM(DistanceTransform_Epsilon_eval):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, epsilon=1.0):\n",
    "        super().__init__(input_dim, output_dim, bias=True, epsilon=epsilon)\n",
    "        \n",
    "        self.scale_shift = dtnn.ScaleShift(-1, scaler_init=3, shifter_init=0, scaler_const=True, shifter_const=True)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.temp_activ = None\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x, calculate_epsilon=False):\n",
    "        if calculate_epsilon:\n",
    "            with torch.no_grad():\n",
    "                xo = super().forward(x, True)\n",
    "                xo = self.scale_shift(xo)\n",
    "                xo = self.softmax(xo)\n",
    "                self.temp_activ = xo\n",
    "        \n",
    "        xo = super().forward(x)\n",
    "        xo = self.scale_shift(xo)\n",
    "        xo = self.softmax(xo)\n",
    "        return xo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9026a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTeSM(2, 5, 0.1)(torch.randn(1, 2)).shape\n",
    "# DTeSM(2, 5, 0.1)(torch.randn(1, 2), calculate_epsilon=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89b448dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalMLP_epsilonsoftmax_eval(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim0, hidden_dim1, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer0 = DTeSM(input_dim, hidden_dim0, epsilon=10)\n",
    "        self.layer1 = DTeSM(hidden_dim0, hidden_dim1, epsilon=5)\n",
    "        \n",
    "        self.layer2 = nn.Linear(hidden_dim1, output_dim)\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x, calculate_epsilon = False):\n",
    "        xo = self.layer0(x, calculate_epsilon)\n",
    "        xo = self.layer1(xo, calculate_epsilon)\n",
    "        xo = self.layer2(xo)\n",
    "        return xo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36deb81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = 200\n",
    "h1 = 60\n",
    "model = LocalMLP_epsilonsoftmax_eval(784, h0, h1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "386641e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocalMLP_epsilonsoftmax_eval(\n",
       "  (layer0): DTeSM(\n",
       "    (scale_shift): ScaleShift()\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (layer1): DTeSM(\n",
       "    (scale_shift): ScaleShift()\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (layer2): Linear(in_features=60, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04b26ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xx.reshape(-1, 28*28)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "606cd672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_training_samples(N):\n",
    "    new_center = []\n",
    "    new_labels = []\n",
    "    count = 0\n",
    "    for i, (xx, yy) in enumerate(train_loader):\n",
    "        xx = xx.reshape(xx.shape[0], -1)\n",
    "        if count+xx.shape[0] < N:\n",
    "            new_center.append(xx)\n",
    "            new_labels.append(yy)\n",
    "            count += xx.shape[0]\n",
    "        elif count >= N:\n",
    "            break\n",
    "        else:\n",
    "            new_center.append(xx[:N-count])\n",
    "            new_labels.append(yy[:N-count])\n",
    "            count = N\n",
    "            break\n",
    "\n",
    "    new_center = torch.cat(new_center, dim=0)\n",
    "    new_labels = torch.cat(new_labels, dim=0)\n",
    "    \n",
    "    weights = torch.zeros(len(new_labels), 10)\n",
    "    for i in range(len(new_labels)):\n",
    "        weights[i, new_labels[i]] = 1.\n",
    "    \n",
    "    return new_center.to(device), weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3ed5e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "         [-1., -1., -1.,  ..., -1., -1., -1.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_training_samples(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f7dc067",
   "metadata": {},
   "outputs": [],
   "source": [
    "c0, _ = get_random_training_samples(h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33baeabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first layer\n",
    "model.layer0.centers.data = c0.to(model.layer0.centers.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bb25fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, v1 = get_random_training_samples(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2038002",
   "metadata": {},
   "outputs": [],
   "source": [
    "## second layer\n",
    "model.layer1.centers.data = model.layer0(c1.to(device))\n",
    "model.layer2.weight.data = v1.t().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b96cb6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(epoch, model, optimizer):\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "#         inputs, targets = inputs.to(device).view(-1, 28*28), targets.to(device)\n",
    "        \n",
    "#         ### Train with random image and \"10\" as class\n",
    "# #         inputs = torch.cat([inputs, torch.rand(batch_size//10, 28*28, dtype=inputs.dtype).to(device)*2-1], dim=0)\n",
    "# #         targets = torch.cat([targets, torch.ones(batch_size//10, dtype=targets.dtype).to(device)*10], dim=0)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         train_loss += loss.item()\n",
    "#         _, predicted = outputs.max(1)\n",
    "#         total += targets.size(0)\n",
    "#         correct += predicted.eq(targets).sum().item()\n",
    "#     print(f\"[Train] {epoch} Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cd42a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = -1\n",
    "def test(epoch, model):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "#         for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader)):\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device).view(-1, 28*28), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "    print(f\"[Test] {epoch} Loss: {test_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "    \n",
    "    acc = 100.*correct/total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9168694a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocalMLP_epsilonsoftmax_eval(\n",
       "  (layer0): DTeSM(\n",
       "    (scale_shift): ScaleShift()\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (layer1): DTeSM(\n",
       "    (scale_shift): ScaleShift()\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (layer2): Linear(in_features=60, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae30cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c6cf7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 0 Loss: 2.102 | Acc: 40.280 4028/10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40.28"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc = test(0, model)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59e0cc3",
   "metadata": {},
   "source": [
    "### Model Training - evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3db25bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "# EPOCHS = 10\n",
    "EPOCHS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d05a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1, p2 = [], []\n",
    "for p in model.named_parameters():\n",
    "    if p[0].endswith(\".centers\"):\n",
    "        p1.append(p[1])\n",
    "    else:\n",
    "        p2.append(p[1])\n",
    "\n",
    "params = [\n",
    "    {\"params\": p1, \"lr\": learning_rate*0.03}, ## default - to change little from data point\n",
    "#     {\"params\": p1},\n",
    "    {\"params\": p2},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6534bef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "best_acc = -1\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch, model, optimizer)\n",
    "    test(epoch, model)\n",
    "    scheduler.step()\n",
    "    \n",
    "\"\"\"\n",
    "Note: It trains to about 95% on MNIST\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a110a503",
   "metadata": {},
   "source": [
    "## Multilayer Noisy Selection (Type 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc0f30da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TYPE 1: DT>eSM>DT>eSM>V\n",
    "Type 2:  /DT>eSM>S\\\n",
    "        X---------+\\>eSM>V\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b19ae",
   "metadata": {},
   "source": [
    "## Add new centers to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8deb5d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1) initialize the models with data0, data1\n",
    "2) to add neurons to dt0, calculate respective activation with data1 and initialize the dt1\n",
    "    - for dt1, input dim increase and output dim stay same ;\n",
    "    - the hidden activation changes slightly..\n",
    "    \n",
    "3) to add neurons to dt1, same as 2 layer addition..\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aab0a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = 200\n",
    "h1 = 60\n",
    "model = LocalMLP_epsilonsoftmax_eval(784, h0, h1, 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c478fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_search0 = 30\n",
    "N_search1 = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb734456",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "627734e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c0, _ = get_random_training_samples(h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e41fd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layer0.centers.data = c0.to(model.layer0.centers.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03d6589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, v1 = get_random_training_samples(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5562d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layer1.centers.data = model.layer0(c1.to(device))\n",
    "model.layer2.weight.data = v1.t().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb906a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 0 Loss: 2.141 | Acc: 36.310 3631/10000\n"
     ]
    }
   ],
   "source": [
    "test_acc = test(0, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e497c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize to take the new_activation info of centers in layer 1..\n",
    "### The activation distribution changes due to entangled(softmax and normalization in DTeSM)\n",
    "def add_neurons_to_layer0(model, centers0, old_centers1): ## the parameters should not change\n",
    "    \n",
    "    ########## LAYER 0 ############\n",
    "    c0 = torch.cat((model.layer0.centers.data, centers0), dim=0)\n",
    "    \n",
    "#     s0 = torch.cat([model.layer0.bias.data, torch.ones(1, len(centers0))*0], dim=1)\n",
    "\n",
    "    model.layer0.centers.data = c0\n",
    "#     model.layer0.bias.data = s0\n",
    "\n",
    "#     v = torch.cat((model.layer1.weight.data, values.t()), dim=1)\n",
    "#     model.layer1.weight.data = v\n",
    "\n",
    "    ########## LAYER 1 ############\n",
    "#     c1 = torch.cat((model.layer1.centers.data, model.layer0(centers1)), dim=0) ## initial: add center activation1\n",
    "#     all_centers = torch.cat([old_centers1, centers1], dim=0)\n",
    "#     c1 = model.layer0(all_centers) ## initial: add center activation1\n",
    "    \n",
    "    model.layer1.centers.data = model.layer0(old_centers1)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2afce423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_neurons_to_layer1(model, centers1, values1, old_centers1):\n",
    "    all_centers = torch.cat([old_centers1, centers1], dim=0)\n",
    "    \n",
    "#     c1 = torch.cat((model.layer1.centers.data, model.layer0(centers1)), dim=0)\n",
    "    c1 = model.layer0(all_centers)\n",
    "#     s1 = torch.cat([model.layer1.bias.data, torch.ones(1, len(centers1))*0], dim=1)\n",
    "    v = torch.cat((model.layer2.weight.data, values1.t()), dim=1)\n",
    "\n",
    "    model.layer1.centers.data = c1\n",
    "#     model.layer1.bias.data = s1\n",
    "    model.layer2.weight.data = v\n",
    "    \n",
    "    return all_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "226d623d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([200, 784]), torch.Size([60, 200]), torch.Size([10, 60]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer0.centers.data.shape, model.layer1.centers.data.shape, model.layer2.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b92f4a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_c0 = get_random_training_samples(N_search0)[0]\n",
    "add_neurons_to_layer0(model, _c0, c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df0cfac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([230, 784]), torch.Size([60, 230]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer0.centers.data.shape, model.layer1.centers.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "792a0f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 0 Loss: 2.160 | Acc: 31.630 3163/10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31.63, 36.31)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc2 = test(0, model)\n",
    "test_acc2, test_acc ### ?? why does adding new centers to layer0 reduce the accuracy ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f1a7926",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add neurons to second layer\n",
    "_c1, _v1 = get_random_training_samples(N_search1)\n",
    "c1 = add_neurons_to_layer1(model, _c1, _v1, c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b51c3ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([70, 230]), torch.Size([10, 70]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer1.centers.data.shape, model.layer2.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e08788f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 0 Loss: 2.143 | Acc: 33.090 3309/10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(33.09, 31.63, 36.31)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc3 = test(0, model)\n",
    "test_acc3, test_acc2, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "00ca15de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asdsadsd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de78e7c3",
   "metadata": {},
   "source": [
    "## Calculate Neuron Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8db0101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_keys = [model.layer0, model.layer1]\n",
    "outputs = {k.softmax:None for k in layer_keys}\n",
    "gradients = {k.softmax:None for k in layer_keys}\n",
    "\n",
    "def capture_outputs(module, inp, out):\n",
    "    global outputs\n",
    "    outputs[module] = out.data.cpu()\n",
    "\n",
    "def capture_gradients(module, gradi, grado):\n",
    "    global gradients\n",
    "    gradients[module] = grado[0].data.cpu()\n",
    "        \n",
    "forw_hooks = [k.softmax.register_forward_hook(capture_outputs) for k in layer_keys]\n",
    "back_hooks = [k.softmax.register_backward_hook(capture_gradients) for k in layer_keys]\n",
    "\n",
    "def remove_hook():\n",
    "    for hook in forw_hooks+back_hooks:\n",
    "        hook.remove()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "535fe829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Softmax(dim=-1): tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " Softmax(dim=-1): tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "significance = {k.softmax:torch.zeros(k.centers.shape[0]) for k in layer_keys}\n",
    "significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c6ba7775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 784]) torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "for xx, yy in train_loader:\n",
    "    xx, yy = xx.to(device).view(-1, 28*28), yy.to(device)\n",
    "    print(xx.shape, yy.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6fb49217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 10])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yout = model(xx)\n",
    "yout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0217aeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0.centers \t torch.Size([230, 784])\n",
      "layer0.scaler \t torch.Size([1, 1])\n",
      "layer1.centers \t torch.Size([70, 230])\n",
      "layer1.scaler \t torch.Size([1, 1])\n",
      "layer2.weight \t torch.Size([10, 70])\n",
      "layer2.bias \t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    print(n,\"\\t\" ,p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e596ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def none_grad():\n",
    "    for p in model.parameters():\n",
    "        p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0f296704",
   "metadata": {},
   "outputs": [],
   "source": [
    "none_grad()\n",
    "yout.register_hook(lambda grad: grad/torch.norm(grad, dim=1, keepdim=True))\n",
    "\n",
    "# grad = torch.randn_like(yout)\n",
    "# ### grad = grad/torch.norm(grad, dim=1, keepdim=True)\n",
    "# yout.backward(gradient=grad, retain_graph=False)\n",
    "\n",
    "criterion(yout, yy).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "014b5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_hook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1284c274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Softmax(dim=-1): tensor([[5.5955e-03, 2.6959e-05, 1.4790e-05,  ..., 2.4560e-06, 1.0225e-03,\n",
       "          9.2740e-05],\n",
       "         [1.2230e-06, 1.4006e-01, 2.3846e-06,  ..., 8.8221e-08, 1.0266e-04,\n",
       "          3.8485e-05],\n",
       "         [3.8386e-06, 9.2652e-03, 2.5326e-06,  ..., 2.2203e-06, 3.4260e-04,\n",
       "          6.5213e-05],\n",
       "         ...,\n",
       "         [3.2783e-05, 6.4050e-04, 6.0128e-06,  ..., 2.9462e-07, 7.6330e-04,\n",
       "          1.0532e-04],\n",
       "         [6.7561e-05, 2.1577e-06, 1.7320e-04,  ..., 1.4732e-07, 1.5507e-06,\n",
       "          9.3436e-08],\n",
       "         [6.4740e-06, 1.5893e-03, 3.2063e-06,  ..., 3.6412e-06, 3.0054e-04,\n",
       "          6.6040e-05]]),\n",
       " Softmax(dim=-1): tensor([[4.8367e-06, 1.3909e-02, 5.4262e-06,  ..., 3.4233e-03, 2.6036e-03,\n",
       "          5.3273e-04],\n",
       "         [1.8988e-05, 5.6326e-02, 2.1200e-05,  ..., 1.1724e-02, 1.2167e-02,\n",
       "          2.2377e-03],\n",
       "         [2.1262e-05, 8.5921e-02, 2.3758e-05,  ..., 1.5280e-02, 3.3241e-02,\n",
       "          2.5969e-03],\n",
       "         ...,\n",
       "         [1.4054e-05, 1.7406e-01, 1.5574e-05,  ..., 2.5508e-02, 6.4390e-03,\n",
       "          1.1922e-03],\n",
       "         [2.0007e-08, 1.4582e-05, 2.2208e-08,  ..., 4.8517e-06, 4.9180e-06,\n",
       "          1.4000e-06],\n",
       "         [1.9176e-05, 5.8961e-02, 2.1766e-05,  ..., 1.2859e-02, 2.7485e-01,\n",
       "          2.6028e-03]])}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f1c6cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Softmax(dim=-1): tensor([[-2.1100e-01,  2.2212e-02,  1.7308e-03,  ...,  1.1795e-05,\n",
       "           3.1190e-02,  3.8565e-03],\n",
       "         [ 1.2575e-02, -2.2375e-01,  1.2142e-03,  ...,  1.1818e-05,\n",
       "           8.6896e-03,  1.0803e-03],\n",
       "         [ 3.6899e-03,  3.4449e-02,  1.8759e-03,  ...,  5.2858e-06,\n",
       "           3.0681e-02,  3.3892e-03],\n",
       "         ...,\n",
       "         [-2.2441e-02, -1.2346e-02,  6.3127e-04,  ...,  1.9360e-05,\n",
       "           2.6107e-02,  4.6250e-03],\n",
       "         [-9.2462e-05,  5.9037e-05, -8.5532e-07,  ..., -3.7203e-08,\n",
       "           4.8198e-05,  5.6157e-06],\n",
       "         [ 3.7770e-03,  1.0704e-02,  1.1123e-03,  ...,  2.0596e-06,\n",
       "           1.1598e-02,  1.2042e-03]]),\n",
       " Softmax(dim=-1): tensor([[0.1006, 0.1006, 0.1060,  ..., 0.1107, 0.1060, 0.1107],\n",
       "         [0.1044, 0.1044, 0.1017,  ..., 0.1075, 0.1017, 0.1075],\n",
       "         [0.1144, 0.1144, 0.1078,  ..., 0.1085, 0.1078, 0.1085],\n",
       "         ...,\n",
       "         [0.1231, 0.1231, 0.0996,  ..., 0.1058, 0.0996, 0.1058],\n",
       "         [0.0995, 0.0995, 0.1069,  ..., 0.1113, 0.1069, 0.1113],\n",
       "         [0.1041, 0.1041, 0.1394,  ..., 0.1059, 0.1394, 0.1059]])}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "be389bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Softmax(dim=-1): tensor([1.9249e-02, 1.0787e-03, 1.1615e-11, 9.0257e-13, 6.5465e-05, 4.0700e-05,\n",
       "         3.9585e-08, 2.5843e-05, 1.3649e-08, 4.5141e-07, 1.7529e-02, 5.9360e-05,\n",
       "         6.9902e-05, 3.1605e-06, 1.1921e-05, 2.8067e-03, 8.4503e-07, 2.9518e-03,\n",
       "         2.6002e-09, 4.0400e-02, 1.6231e-04, 4.2388e-06, 2.3011e-08, 1.0238e-02,\n",
       "         5.1121e-09, 5.2800e+00, 1.6031e-03, 2.3117e-04, 1.1233e-10, 6.9011e-05,\n",
       "         4.7227e-04, 1.8110e-05, 1.1922e-05, 3.7548e-05, 2.1607e-08, 3.5626e-03,\n",
       "         2.3790e+00, 1.0455e-04, 5.4865e-08, 1.8841e-02, 9.5044e-08, 2.0742e-04,\n",
       "         1.1890e-04, 2.1775e-09, 5.2436e-05, 1.0845e-05, 7.7015e-08, 1.7216e-05,\n",
       "         1.1164e-02, 1.7985e-06, 3.9749e-07, 1.1654e-08, 1.4592e-05, 5.0684e-01,\n",
       "         3.6223e-04, 3.2234e-04, 1.2663e-08, 1.3185e-12, 1.2040e-09, 2.8953e-02,\n",
       "         4.6576e-06, 1.2930e-03, 3.8716e-04, 1.7419e-11, 7.4305e-05, 1.3809e+00,\n",
       "         1.0175e-05, 2.4941e-07, 6.6955e-07, 2.0041e-03, 6.5995e-09, 3.1922e-05,\n",
       "         5.6874e-07, 2.5254e-08, 9.0532e-04, 7.0953e-06, 2.2294e-05, 9.8991e-05,\n",
       "         1.1652e-08, 1.6033e-06, 3.1983e-10, 3.6772e-05, 2.6131e-05, 8.9306e-11,\n",
       "         1.1817e-15, 5.3426e-14, 1.5979e-06, 5.9897e-06, 1.5903e-06, 1.4828e-06,\n",
       "         1.5505e-03, 6.8508e-09, 1.0320e-05, 2.1918e-03, 6.9293e-08, 1.1283e-04,\n",
       "         1.1000e-06, 2.0983e-08, 6.0782e-09, 1.7743e-08, 6.0035e-04, 1.1459e-03,\n",
       "         3.5057e-14, 4.5078e-07, 3.1570e-01, 8.7076e-04, 2.8468e-07, 2.7152e-09,\n",
       "         1.8480e-04, 2.4442e-04, 2.9968e-02, 2.1758e-04, 3.7530e-07, 3.4833e-03,\n",
       "         4.5176e-01, 9.7528e-04, 1.4793e-03, 2.8562e-12, 4.7081e-07, 6.4098e-03,\n",
       "         2.1739e-04, 2.0985e-05, 5.2827e-04, 1.9991e-02, 2.1786e-04, 2.4788e-06,\n",
       "         1.4625e-06, 2.8562e-09, 1.9863e-16, 1.8963e-08, 5.1064e-10, 1.1875e-03,\n",
       "         2.9266e-06, 2.1240e-06, 6.1843e-07, 3.3153e-02, 2.4118e-11, 1.5143e-04,\n",
       "         4.9321e-09, 2.0365e-07, 8.5459e-09, 3.1388e-06, 1.7696e-08, 1.8104e-05,\n",
       "         2.5710e-06, 1.4979e-03, 2.3257e-07, 3.8492e-08, 5.7016e-07, 1.5270e-06,\n",
       "         6.0197e-11, 1.6661e-06, 9.0939e-04, 2.8130e-04, 9.0926e-13, 7.6494e-13,\n",
       "         3.0625e-12, 3.4936e-06, 1.8897e-04, 5.5497e-04, 3.9448e-02, 8.5519e-02,\n",
       "         1.7368e-12, 4.5894e-08, 8.7586e-05, 4.7569e-04, 2.7336e-06, 3.8341e-01,\n",
       "         1.1986e-07, 3.4815e-04, 1.4693e-04, 1.9092e-03, 2.4838e-06, 2.9592e-04,\n",
       "         8.5335e-10, 5.0203e-02, 5.3474e-02, 4.3789e-07, 5.9201e-02, 2.9323e-09,\n",
       "         2.2908e-04, 1.8600e-05, 1.7104e-14, 9.8272e-04, 3.4700e-07, 3.2693e-03,\n",
       "         1.2587e-09, 4.0846e-05, 1.5901e-04, 2.7194e-07, 1.6815e-07, 7.2116e-05,\n",
       "         6.3092e-04, 9.1477e-04, 3.4792e-10, 4.3349e-08, 2.1213e-07, 1.9123e-04,\n",
       "         1.9493e-07, 3.3835e-07, 8.8993e-11, 1.0684e-05, 1.0140e-05, 1.0539e-06,\n",
       "         9.7587e-12, 4.0176e-03, 2.7147e-02, 1.0373e-08, 1.6101e-08, 8.1822e-08,\n",
       "         6.1748e-05, 2.3151e-05, 1.0140e-06, 7.7475e-06, 5.9154e-08, 2.4336e-03,\n",
       "         9.2264e+00, 9.3340e-02, 8.8210e-07, 1.5102e-06, 5.0011e-07, 3.4274e-10,\n",
       "         2.4699e-07, 6.9648e-04, 3.7196e-11, 5.9968e-08, 2.9193e-10, 2.9946e-07,\n",
       "         3.5730e-04, 3.2420e-10]),\n",
       " Softmax(dim=-1): tensor([1.7674e-01, 4.0974e-02, 3.1998e-02, 2.4797e-06, 6.9059e-05, 2.4580e-04,\n",
       "         6.3322e-03, 1.2574e-02, 7.7625e-09, 8.1120e-11, 9.0456e-04, 1.9523e-01,\n",
       "         3.8523e-05, 1.8042e-02, 8.9911e-08, 8.2658e-07, 6.7835e-11, 1.3698e-04,\n",
       "         1.6162e-08, 1.1340e+00, 3.6875e-01, 4.4524e-10, 8.3674e-08, 6.3365e-01,\n",
       "         5.5798e-03, 3.4242e-11, 3.6650e-03, 1.3701e-09, 4.5430e-03, 6.4327e-04,\n",
       "         1.6346e-01, 1.9680e-01, 7.4371e-05, 8.9918e-07, 3.2671e-05, 4.3934e-07,\n",
       "         8.1517e-03, 2.9291e-02, 1.4588e-04, 1.5759e-11, 3.5664e-02, 9.2604e-05,\n",
       "         7.9039e-01, 1.0366e-05, 8.9936e-01, 2.1688e-01, 6.8163e-03, 4.6377e-10,\n",
       "         1.1631e-05, 1.5311e-02, 8.5143e-01, 6.2856e-03, 2.3518e-06, 3.2103e-02,\n",
       "         6.3540e-07, 1.2078e-01, 5.6295e-02, 2.6529e-05, 8.9151e-04, 2.5272e-01,\n",
       "         8.2976e-01, 2.9235e-06, 2.7420e-09, 3.8839e-03, 1.8922e-04, 7.2783e-06,\n",
       "         1.5713e-02, 2.5178e-03, 1.3002e-02, 7.3680e-05])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for k in layer_keys:\n",
    "        lk = k.softmax\n",
    "#         print(outputs[lk])\n",
    "#         print(gradients[lk])\n",
    "#         print(significance[lk])\n",
    "        significance[lk] += torch.sum((outputs[lk]*gradients[lk])**2, dim=0)\n",
    "significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fd4ea068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bae61d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = significance[layer_keys[0].softmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "18f1b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.topk(sig, k=h0, sorted=True, largest=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a0da6f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([216,  25,  36,  65,  53, 114, 167, 104, 217, 161, 178, 176, 175,  19,\n",
       "        160, 135, 110,  59, 206, 123,   0,  39,  10,  48,  23, 119, 205,  35,\n",
       "        113, 185,  17,  15, 215,  93,  69, 171,  26,  90, 145, 116,  61, 131,\n",
       "        101,   1, 183, 115, 193, 152,  74, 105, 223, 192, 100, 159, 122, 165,\n",
       "         30,  62,  54, 228, 169,  55, 173, 153, 109,  27, 180, 124, 111, 120,\n",
       "         41, 197, 158, 108,  20, 188, 137, 170,  42,  95,  37,  77, 164,  64,\n",
       "        191,  12,  29,   4, 210,  11,  44, 187,   5,  33,  81,  71,  82,   7,\n",
       "        211,  76, 121, 181,  31, 143,  47,  52,  32,  14,  45, 201,  92,  66,\n",
       "        202, 213,  75,  87,  60,  21, 157,  13, 141, 132, 166, 144, 172, 125,\n",
       "        133,  49, 151,  79,  86,  88, 149, 219,  89, 126,  96, 203, 212, 218,\n",
       "         16,  68, 134, 148,  72, 220, 118,   9, 103, 177,  50, 112, 184, 199,\n",
       "        227, 106, 189,  67, 222, 146, 196, 139, 198, 190, 168,  40, 209,  46,\n",
       "         94, 225, 214,  38, 163, 195,   6, 147,  73,  22,  34,  97, 129,  99,\n",
       "        142, 208,   8,  56,  51,  78, 207, 140,  91,  70,  98,  24, 138, 179,\n",
       "        127, 107,  18,  43])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_idx = torch.topk(sig, k=h0, sorted=True)[1]\n",
    "topk_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2d177c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_neurons_from_layer1(model, importance, num_prune, old_centers1):\n",
    "    N = model.layer1.centers.shape[0]\n",
    "    topk_idx = torch.topk(importance, k=N-num_prune, largest=True)[1]\n",
    "    \n",
    "    c = model.layer1.centers.data[topk_idx]\n",
    "    v = model.layer2.weight.data[:,topk_idx]\n",
    "#     s = model.layer1.bias.data[:,topk_idx]\n",
    "    model.layer1.centers.data = c\n",
    "    model.layer2.weight.data = v\n",
    "#     model.layer1.bias.data = s\n",
    "    \n",
    "    return old_centers1[topk_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "882b4ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_neurons_from_layer0(model, importance, num_prune, old_centers1):\n",
    "    N = model.layer0.centers.shape[0]\n",
    "    topk_idx = torch.topk(importance, k=N-num_prune, largest=True)[1]\n",
    "    \n",
    "    c = model.layer0.centers.data[topk_idx]\n",
    "#     s = model.layer0.bias.data[:,topk_idx]\n",
    "    model.layer0.centers.data = c\n",
    "#     model.layer0.bias.data = s\n",
    "    \n",
    "    model.layer1.centers.data = model.layer0(old_centers1)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5bf3af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cfae416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = remove_neurons_from_layer1(model, significance[layer_keys[1].softmax], N_search1, c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "23b59f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 784])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "09a4f31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([230, 784]), torch.Size([60, 230]), torch.Size([10, 60]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer0.centers.data.shape, model.layer1.centers.data.shape, model.layer2.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6a09ec22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 0 Loss: 2.140 | Acc: 32.430 3243/10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32.43, 33.09, 31.63, 36.31)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc4 = test(0, model)\n",
    "\n",
    "test_acc4, test_acc3, test_acc2, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fc1a23ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_neurons_from_layer0(model, significance[layer_keys[0].softmax], N_search0, c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c110d869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([200, 784]), torch.Size([60, 200]), torch.Size([10, 60]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer0.centers.data.shape, model.layer1.centers.data.shape, model.layer2.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c8d1aba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 0 Loss: 2.130 | Acc: 32.390 3239/10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32.39, 32.43, 33.09, 31.63, 36.31)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc5 = test(0, model)\n",
    "test_acc5, test_acc4, test_acc3, test_acc2, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f03f8812",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdasd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8483/2657870816.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0masdasd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'asdasd' is not defined"
     ]
    }
   ],
   "source": [
    "asdasd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1578759",
   "metadata": {},
   "source": [
    "## Do this in Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "746977a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_c0 = get_random_training_samples(N_search0)[0]\n",
    "add_neurons_to_layer0(model, _c0, c1)\n",
    "\n",
    "_c1, _v1 = get_random_training_samples(N_search1)\n",
    "c1 = add_neurons_to_layer1(model, _c1, _v1, c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ba6e0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "significance = {k.softmax:torch.zeros(k.centers.shape[0]) for k in layer_keys}\n",
    "\n",
    "forw_hooks = [k.softmax.register_forward_hook(capture_outputs) for k in layer_keys]\n",
    "back_hooks = [k.softmax.register_backward_hook(capture_gradients) for k in layer_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fa537ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a8246371",
   "metadata": {},
   "outputs": [],
   "source": [
    "for xx, yy in train_loader:\n",
    "    xx = xx.to(device).view(-1, 28*28)\n",
    "    yout = model(xx)\n",
    "    \n",
    "    none_grad()\n",
    "#     yout.register_hook(lambda grad: grad/torch.norm(grad, dim=1, keepdim=True))\n",
    "    \n",
    "#     grad = torch.randn_like(yout)\n",
    "#     ### grad = grad/torch.norm(grad, dim=1, keepdim=True)\n",
    "#     yout.backward(gradient=grad)\n",
    "    \n",
    "    criterion(yout, yy).backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for k in layer_keys:\n",
    "            lk = k.softmax\n",
    "            significance[lk] += torch.sum((outputs[lk]*gradients[lk])**2, dim=0)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2b510134",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_hook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ab8351b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = remove_neurons_from_layer1(model, significance[layer_keys[1].softmax], N_search1, c1) ## anything can be done first\n",
    "remove_neurons_from_layer0(model, significance[layer_keys[0].softmax], N_search0, c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b8124be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 0 Loss: 2.131 | Acc: 34.430 3443/10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34.43"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(0, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737318f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdasd  ### ^^ expected test_acc2 > test_acc3 > test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b08c7e",
   "metadata": {},
   "source": [
    "## Optimize for multiple steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2e8b7e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] 0 Loss: 2.004 | Acc: 54.350 5435/10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54.35"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0 = 100\n",
    "h1 = 100\n",
    "N_search0 = 30\n",
    "N_search1 = 30\n",
    "model = LocalMLP_epsilonsoftmax_eval(784, h0, h1, 10).to(device)\n",
    "\n",
    "### Initialization\n",
    "c0, _ = get_random_training_samples(h0)\n",
    "model.layer0.centers.data = c0.to(model.layer0.centers.device)\n",
    "\n",
    "c1, v1 = get_random_training_samples(h1)\n",
    "model.layer1.centers.data = model.layer0(c1.to(device))\n",
    "model.layer2.weight.data = v1.t().to(device)\n",
    "\n",
    "test(0, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2a23298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_keys = [model.layer0, model.layer1]\n",
    "outputs = {k.softmax:None for k in layer_keys}\n",
    "gradients = {k.softmax:None for k in layer_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "006b950f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding and Pruning for STEP: 0\n",
      "[Test] 0 Loss: 1.885 | Acc: 68.820 6882/10000\n",
      "Adding and Pruning for STEP: 1\n",
      "[Test] 0 Loss: 1.856 | Acc: 67.390 6739/10000\n",
      "Adding and Pruning for STEP: 2\n",
      "[Test] 0 Loss: 1.849 | Acc: 68.710 6871/10000\n",
      "Adding and Pruning for STEP: 3\n",
      "[Test] 0 Loss: 1.842 | Acc: 66.100 6610/10000\n",
      "Adding and Pruning for STEP: 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8483/3086710312.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0myout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mnone_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Program_Files/Python/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8483/2664630498.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, calculate_epsilon)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculate_epsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mxo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculate_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mxo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculate_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mxo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Program_Files/Python/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8483/780448482.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, calculate_epsilon)\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp_activ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mxo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mxo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_shift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mxo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8483/2621043835.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, calculate_epsilon)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculate_epsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcalculate_epsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Artificial_Intelligence/Notebooks/Experimentals/NN_Func_Approx/Iterative_Learning/dtnnlib.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m### normalize similar to UMAP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Program_Files/Python/miniconda3/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mcdist\u001b[0;34m(x1, x2, p, compute_mode)\u001b[0m\n\u001b[1;32m   1117\u001b[0m             cdist, (x1, x2), x1, x2, p=p, compute_mode=compute_mode)\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'use_mm_for_euclid_dist_if_necessary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcompute_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'use_mm_for_euclid_dist'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Run multiple times for convergence\n",
    "STEPS = 10\n",
    "for s in range(STEPS):\n",
    "    print(f\"Adding and Pruning for STEP: {s}\")\n",
    "    _c0 = get_random_training_samples(N_search0)[0]\n",
    "    add_neurons_to_layer0(model, _c0, c1)\n",
    "\n",
    "    _c1, _v1 = get_random_training_samples(N_search1)\n",
    "    c1 = add_neurons_to_layer1(model, _c1, _v1, c1)\n",
    "    #############################\n",
    "    significance = {k.softmax:torch.zeros(k.centers.shape[0]) for k in layer_keys}\n",
    "\n",
    "    forw_hooks = [k.softmax.register_forward_hook(capture_outputs) for k in layer_keys]\n",
    "    back_hooks = [k.softmax.register_backward_hook(capture_gradients) for k in layer_keys]\n",
    "    #############################\n",
    "    \n",
    "    for xx, yy in train_loader:\n",
    "        xx = xx.to(device).view(-1, 28*28)\n",
    "        yout = model(xx)\n",
    "\n",
    "        none_grad()\n",
    "#         yout.register_hook(lambda grad: grad/torch.norm(grad, dim=1, keepdim=True))\n",
    "        ####################################\n",
    "#         grad = torch.randn_like(yout)\n",
    "#         ### grad = grad/torch.norm(grad, dim=1, keepdim=True)\n",
    "#         yout.backward(gradient=grad)\n",
    "        ###################################\n",
    "        criterion(yout, yy).backward()\n",
    "        with torch.no_grad():\n",
    "            for k in layer_keys:\n",
    "                lk = k.softmax\n",
    "                significance[lk] += torch.sum((outputs[lk]*gradients[lk])**2, dim=0)\n",
    "    \n",
    "    remove_hook()\n",
    "    c1 = remove_neurons_from_layer1(model, significance[layer_keys[1].softmax], N_search1, c1)\n",
    "    remove_neurons_from_layer0(model, significance[layer_keys[0].softmax], N_search0, c1)\n",
    "    test_acc = test(0, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8abce64",
   "metadata": {},
   "source": [
    "## Test for Adverserial Rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "41c27465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import foolbox as fb\n",
    "import foolbox.attacks as fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "39b67654",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel = fb.PyTorchModel(model.eval(), bounds=(-1, 1), device=device)\n",
    "\n",
    "# attack = fa.LinfPGD()\n",
    "attack = fa.FGSM()\n",
    "# epsilons = [0.0, 0.001, 0.01, 0.03, 0.1, 0.3, 0.5, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1d0ed4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 200/200 [00:03<00:00, 62.80it/s]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "failed = 0\n",
    "rejected = 0\n",
    "x_rejected = 0\n",
    "for i, (xx, yy) in enumerate(tqdm(test_loader)):\n",
    "    xx = xx.reshape(-1, 28*28)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        yout = model(xx, calculate_epsilon=True)\n",
    "    reject0 = model.layer0.temp_activ.max(dim=1)[1] == h0\n",
    "    reject1 = model.layer1.temp_activ.max(dim=1)[1] == h1\n",
    "#     reject = torch.bitwise_or(reject0, reject1)\n",
    "#     reject = torch.bitwise_and(reject0, reject1)\n",
    "    reject = reject1\n",
    "    \n",
    "    x_rejected += int(reject.type(torch.float32).sum())\n",
    "    \n",
    "    _, advs, success = attack(fmodel, xx, yy, epsilons=0.5)\n",
    "    with torch.no_grad(): ### just to access neuron activation for adverserial examples\n",
    "        yout = model(advs, calculate_epsilon=True)\n",
    "    reject0 = model.layer0.temp_activ.max(dim=1)[1] == h0\n",
    "    reject1 = model.layer1.temp_activ.max(dim=1)[1] == h1\n",
    "#     reject = torch.bitwise_or(reject0, reject1)\n",
    "#     reject = torch.bitwise_and(reject0, reject1)\n",
    "    reject = reject1\n",
    "    \n",
    "    \n",
    "    rejected += int(reject.type(torch.float32).sum())\n",
    "    fail = torch.bitwise_and(success, ~reject).type(torch.float32).sum()\n",
    "#     fail = success.type(torch.float32).sum()\n",
    "    failed += int(fail)    \n",
    "    count += len(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fdf868d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 6466, 33, 27)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count, failed, rejected, x_rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5f617adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6599"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(failed+x_rejected)/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cd8de064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.8285e-07, 1.0099e-07, 1.1327e-06, 2.3972e-11, 7.5110e-11, 4.5988e-11,\n",
       "        1.1596e-06, 2.5228e-07, 1.7483e-06, 7.1083e-09, 1.0149e-06, 4.0222e-07,\n",
       "        2.6435e-08, 9.2934e-07, 1.1135e-06, 3.9962e-07, 4.3558e-07, 1.9338e-06,\n",
       "        4.1599e-07, 1.2907e-06, 1.8486e-07, 1.6300e-06, 1.0396e-06, 6.9607e-09,\n",
       "        3.1582e-07, 5.0280e-07, 2.6623e-07, 4.6052e-07, 5.9852e-07, 4.0224e-08,\n",
       "        1.1411e-08, 2.1210e-06, 4.0947e-09, 2.8928e-07, 2.2215e-06, 4.6775e-09,\n",
       "        9.1609e-09, 1.4680e-06, 3.6794e-07, 2.1442e-08, 5.0630e-07, 3.4584e-07,\n",
       "        3.0299e-07, 7.1831e-06, 2.6941e-07, 1.8755e-07, 2.2703e-08, 4.4811e-07,\n",
       "        6.1318e-07, 4.2122e-08])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer0.temp_activ[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bb61b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reject1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b5485fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(advs[0].reshape(28,28))\n",
    "plt.show()\n",
    "plt.imshow(xx[0].reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "65dabd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layer0.epsilon = 15.0\n",
    "model.layer1.epsilon = 20."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2b8178",
   "metadata": {},
   "source": [
    "### Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "9416f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel = fb.PyTorchModel(model.eval(), bounds=(-1, 1), device=device)\n",
    "attack = fa.FGSM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ed59fa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                             | 0/200 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, (xx, yy) in enumerate(tqdm(test_loader)):\n",
    "    xx = xx.reshape(-1, 28*28)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        yout = model(xx, calculate_epsilon=True)\n",
    "    reject0 = model.layer0.temp_activ\n",
    "    reject1 = model.layer1.temp_activ\n",
    "    \n",
    "    _, advs, success = attack(fmodel, xx, yy, epsilons=0.5)\n",
    "    with torch.no_grad(): ### just to access neuron activation for adverserial examples\n",
    "        yout = model(advs, calculate_epsilon=True)\n",
    "    _reject0 = model.layer0.temp_activ\n",
    "    _reject1 = model.layer1.temp_activ\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "70fbc2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 21, 130,  12, 130,  59,  12,  94, 130, 130,  40,   1, 130,   7,  26,\n",
       "         33, 108,  14, 101, 130,  39, 117,  27, 130,  72,  39,  44,   0,  51,\n",
       "        130,  93,  76,  54,   8, 130,  47, 130, 100,  54, 118,  85,  80,  47,\n",
       "         14,  12,  54, 116,  70,  39,  51,  81])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reject0.max(dim=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3562c9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 21,  17,  80,   6,  59,  56, 130, 130, 130, 129,  57,  22,  94, 130,\n",
       "         85, 108,  14, 130, 130,  39, 117,  66, 130,  71,  91, 130,  39,  94,\n",
       "        130,  93,  76,  54,  17, 130, 130, 130,  21,  33, 118,  79,  80, 130,\n",
       "        102, 130, 130,  42, 130,  39, 130,  14])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_reject0.max(dim=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "37e500cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.8448e-03, 7.4307e-01, 4.7572e-04, 4.0520e-01, 2.4440e-02, 1.0498e-03,\n",
       "        1.1349e-01, 1.7271e-01, 9.3908e-01, 2.4719e-02, 1.4344e-02, 5.0568e-01,\n",
       "        5.6638e-03, 6.0501e-02, 5.3775e-04, 9.8483e-02, 1.9524e-02, 8.3758e-02,\n",
       "        7.3836e-01, 3.3046e-03, 1.4253e-02, 6.9561e-03, 4.3269e-01, 4.0628e-02,\n",
       "        4.9133e-03, 6.6441e-02, 1.1398e-02, 9.7606e-03, 4.0948e-01, 7.8524e-04,\n",
       "        1.2003e-01, 2.8765e-04, 4.1776e-02, 9.2561e-01, 4.5377e-02, 9.6609e-01,\n",
       "        1.8023e-02, 4.6355e-04, 5.4945e-02, 9.4185e-04, 8.0242e-04, 1.1425e-02,\n",
       "        1.6065e-02, 6.0196e-02, 2.4395e-02, 3.0491e-02, 7.7579e-03, 5.5520e-02,\n",
       "        7.1278e-02, 5.6799e-02])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reject0[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "fed15eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0007e-02, 2.1700e-01, 1.7247e-03, 1.1148e-01, 4.8448e-02, 1.5924e-02,\n",
       "        5.6532e-01, 7.1196e-01, 9.8420e-01, 1.2974e-01, 1.1823e-02, 3.3166e-02,\n",
       "        1.4354e-01, 5.5179e-01, 8.0516e-04, 9.1183e-03, 2.1296e-01, 5.7920e-01,\n",
       "        9.7325e-01, 6.9618e-03, 2.7986e-02, 9.0466e-03, 6.5318e-01, 4.2458e-02,\n",
       "        9.1681e-03, 6.5924e-01, 1.6065e-02, 1.7106e-01, 9.5670e-01, 1.0005e-02,\n",
       "        4.4473e-01, 9.8424e-04, 2.2908e-02, 9.3337e-01, 6.3539e-01, 9.8197e-01,\n",
       "        2.4151e-02, 7.0343e-04, 7.4070e-02, 1.6468e-03, 3.2367e-03, 1.1337e-01,\n",
       "        1.6770e-01, 4.7839e-01, 2.8704e-01, 2.0560e-02, 2.1522e-01, 9.0274e-02,\n",
       "        7.8886e-01, 8.0312e-02])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_reject0[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b854e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
