{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mylibrary.nnlib as tnn\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import mylibrary.datasets as datasets\n",
    "import prunelib\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = datasets.MNIST()\n",
    "train_data, train_label_, test_data, test_label_ = mnist.load()\n",
    "\n",
    "train_data = train_data / 255.\n",
    "test_data = test_data / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = tnn.Logits.index_to_logit(train_label_)\n",
    "train_size = len(train_label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.Tensor(train_data)\n",
    "test_data = torch.Tensor(test_data)\n",
    "yy = torch.LongTensor(train_label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "output_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nets\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "settings:\n",
    "1,2,3 -> net = nn.Sequential(\n",
    "    nn.Linear(784, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10),\n",
    ")\n",
    "\n",
    "4,5 -> net = nn.Sequential(\n",
    "    nn.Linear(784, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10),\n",
    ")\n",
    "\n",
    "6, 7 -> net = nn.Sequential(\n",
    "    nn.Linear(784, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10),\n",
    ")\n",
    "8, 9 -> net = nn.Sequential(\n",
    "    nn.Linear(784, 400),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(400, 300),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(300, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10),\n",
    ")\n",
    "\"\"\"\n",
    "print(\"Nets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config0 = [256, 128, 64]\n",
    "config1 = [100, 100, 100]\n",
    "config2 = [200, 100, 100, 100]\n",
    "config3 = [400, 300, 200, 100]\n",
    "\n",
    "layer_dims = config0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp(config, batch_norm=False, final_activation=None):\n",
    "    config = [input_dim]+config\n",
    "    layers = []\n",
    "    for i in range(len(config)-1):\n",
    "        l = nn.Linear(config[i], config[i+1])\n",
    "        layers.append(l)\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(config[i+1]))\n",
    "        layers.append(nn.ReLU())\n",
    "            \n",
    "    l = nn.Linear(config[-1], output_dim)\n",
    "    layers.append(l)\n",
    "    if final_activation:\n",
    "        layers.append(final_activation)\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "expindx = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): ReLU()\n",
       "  (9): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = get_mlp(config0, batch_norm=True)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.003)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(60):\n",
    "#     yout = net(xx)\n",
    "\n",
    "#     loss = criterion(yout, yy)\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     error = float(loss)\n",
    "#     print(epoch, 'Error = ', error)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         yout = net(xx)\n",
    "#         out = torch.argmax(yout, axis=1)\n",
    "#         acc = (out.data.numpy() == np.array(train_label_)).astype(np.float).mean()\n",
    "#         print(\"Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list(net._modules.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\"model\":net.state_dict(), \"optimizer\":optimizer.state_dict()},\n",
    "#           \"./mnist_100_mlp_bn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"./mnist_100_mlp_bn.pth\")\n",
    "net.load_state_dict(checkpoint[\"model\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert BN network to Linear only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_batchnorm(net, layer_dims):\n",
    "    net_ = get_mlp(layer_dims)\n",
    "    count = 0\n",
    "    for name, module in list(net._modules.items()):\n",
    "        if isinstance(module, nn.BatchNorm1d):\n",
    "            count += 1\n",
    "            \n",
    "    i = 0\n",
    "    j = 0\n",
    "    print(count)\n",
    "    for _ in range(count):\n",
    "        gamma = net[i+1].weight.data\n",
    "        beta = net[i+1].bias.data\n",
    "        mean = net[i+1].running_mean\n",
    "        var = torch.sqrt(net[i+1].running_var)\n",
    "\n",
    "        w = net[i].weight.data\n",
    "        b = net[i].bias.data\n",
    "\n",
    "        newW = (gamma/var).reshape(-1, 1)*w\n",
    "        newb = beta + gamma/var*(b-mean)\n",
    "\n",
    "        net_[j].weight.data *= 0.\n",
    "        net_[j].bias.data *= 0\n",
    "\n",
    "        net_[j].weight.data += newW\n",
    "        net_[j].bias.data += newb\n",
    "\n",
    "        i = i + 3\n",
    "        j = j + 2\n",
    "        \n",
    "    net_[-1].weight.data *= 0\n",
    "    net_[-1].bias.data *= 0\n",
    "    net_[-1].weight.data += net[-1].weight.data\n",
    "    net_[-1].bias.data += net[-1].bias.data\n",
    "    \n",
    "    return  net_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_ = remove_batchnorm(net, layer_dims)\n",
    "net_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9959166666666667\n"
     ]
    }
   ],
   "source": [
    "## accuracy of new network\n",
    "# yout = net.eval()(xx)\n",
    "yout = net_(xx)\n",
    "out = torch.argmax(yout, axis=1)\n",
    "acc = (out.data.numpy() == np.array(train_label_)).astype(np.float).mean()\n",
    "print(\"Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle Pruning Modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Importance_TaylorFO_Modified_BN(prunelib.Importance):\n",
    "    \n",
    "    def __init__(self, net, criterion, config=None):\n",
    "        self.net = net\n",
    "        self.config = config\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        self.activations = {}\n",
    "        self.gradients = {}\n",
    "        self.forward_hook = {}\n",
    "        self.backward_hook = {}\n",
    "        self.keys = []\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def add_hook(self):\n",
    "        self.activations = {}\n",
    "        self.gradients = {}\n",
    "        self.forward_hook = {}\n",
    "        self.backward_hook = {}\n",
    "        self.keys = []\n",
    "        \n",
    "        modules = list(self.net._modules.items())\n",
    "        for name, module in modules:\n",
    "            if isinstance(module, torch.nn.BatchNorm1d):\n",
    "#             if isinstance(module, torch.nn.Linear):\n",
    "                hook = module.register_backward_hook(self.capture_gradients)\n",
    "                self.backward_hook[module] = hook\n",
    "                hook = module.register_forward_hook(self.capture_inputs)\n",
    "                self.forward_hook[module] = hook\n",
    "                \n",
    "                self.activations[module] = None\n",
    "                self.gradients[module] = None\n",
    "                self.keys.append(module)\n",
    "                \n",
    "        for name, module in reversed(modules):\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                hook = module.register_backward_hook(self.capture_gradients)\n",
    "                self.backward_hook[module] = hook\n",
    "                hook = module.register_forward_hook(self.capture_inputs)\n",
    "                self.forward_hook[module] = hook\n",
    "                \n",
    "                self.activations[module] = None\n",
    "                self.gradients[module] = None\n",
    "                self.keys.append(module)\n",
    "                break\n",
    "                \n",
    "    def remove_hook(self):\n",
    "        for module in self.keys:\n",
    "            hook = self.forward_hook[module]\n",
    "            hook.remove()\n",
    "            hook = self.backward_hook[module]\n",
    "            hook.remove()\n",
    "    \n",
    "    def capture_inputs(self, module, inp, out):\n",
    "        self.activations[module] = out.data\n",
    "        \n",
    "    def capture_gradients(self, module, gradi, grado):\n",
    "        self.gradients[module] = grado[0]\n",
    "        \n",
    "    def gather_inputs_gradients(self, x, t):\n",
    "        self.add_hook()\n",
    "\n",
    "        self.net.zero_grad()\n",
    "        y = self.net(x)\n",
    "        error = self.criterion(y,t)\n",
    "        error.backward()\n",
    "        \n",
    "        self.remove_hook()\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def compute_significance(self, x, t, config=None, normalize=True, layerwise_norm=False):\n",
    "        self.gather_inputs_gradients(x, t)\n",
    "        \n",
    "        if config is None:\n",
    "            if self.config is None:\n",
    "                raise ValueError(\"config is not known. Please specify the config.\") \n",
    "            else:\n",
    "                config = self.config\n",
    "        \n",
    "        ## compute importance score\n",
    "        importance = []\n",
    "        if config[\"grad_rescale\"]:\n",
    "            scaler = torch.norm(self.gradients[self.keys[-1]], p=2, dim=1, keepdim=True) + 1e-5\n",
    "\n",
    "        for module in self.keys[:-1]:\n",
    "            z = self.activations[module] * self.gradients[module]\n",
    "            if config[\"grad_rescale\"]:\n",
    "                z = z / scaler\n",
    "            if config[\"imp_norm\"] == \"abs\":\n",
    "                z = z.abs()\n",
    "            elif config[\"imp_norm\"] == \"sq\":\n",
    "                z = z.pow(2)\n",
    "\n",
    "            z = z.sum(dim=0).abs()\n",
    "            if not config[\"allow_linear\"]:\n",
    "                apnz = torch.mean(self.activations[module] > 0., dim=0, dtype=torch.float)\n",
    "                z = z*(1-apnz) * 4 ## tried on desmos.\n",
    "\n",
    "            if layerwise_norm:\n",
    "                z = z / torch.norm(z, p=2)\n",
    "\n",
    "            importance.append(z)\n",
    "\n",
    "        if normalize:\n",
    "            sums = 0\n",
    "            count = 0\n",
    "            for imp in importance:\n",
    "                sums += imp.sum()\n",
    "                count += len(imp)\n",
    "            divider = sums/count ## total importance is number of neurons\n",
    "            for i in range(len(importance)):\n",
    "                importance[i] = importance[i]/divider\n",
    "            \n",
    "        \n",
    "        del self.activations[self.keys[-1]]\n",
    "#         self.activations = {}\n",
    "        self.gradients = {}\n",
    "        self.forward_hook = {}\n",
    "        self.backward_hook = {}\n",
    "        \n",
    "        aponz, std = self.get_aponz(True, True)\n",
    "        return importance, aponz, std\n",
    "    \n",
    "    def get_aponz(self, return_std=True, remove_activations = True):\n",
    "        if len(self.activations) < 1:\n",
    "            print(\"Activation has not been accumulated.. run compute_significance function\")\n",
    "            return\n",
    "        aponz = []\n",
    "        std = []\n",
    "        for module in self.keys[:-1]:\n",
    "            apnz = torch.mean(self.activations[module] > 0., dim=0, dtype=torch.float)\n",
    "            aponz.append(apnz)\n",
    "            if return_std:\n",
    "                std.append(self.activations[module].std(dim=0))\n",
    "        \n",
    "        if remove_activations:\n",
    "            self.activations = {}\n",
    "        \n",
    "        if return_std:\n",
    "            return aponz, std\n",
    "        return aponz\n",
    "\n",
    "        \n",
    "\n",
    "class Importance_Molchanov_BN(prunelib.Importance):\n",
    "\n",
    "    def __init__(self, net, criterion):\n",
    "        self.net = net\n",
    "        self.criterion = criterion\n",
    "        self.keys = []\n",
    "        for name, module in list(self.net._modules.items()):\n",
    "            if isinstance(module, torch.nn.BatchNorm1d):\n",
    "                self.keys.append(module)\n",
    "        \n",
    "    def compute_significance(self, x, t, normalize=True, batch_size=32):\n",
    "\n",
    "        importance = [0]*len(self.keys)\n",
    "        bstrt = list(range(0, len(x), batch_size))\n",
    "        bstop = bstrt[1:]+[len(x)]\n",
    "        for i in tqdm(range(len(bstrt))):\n",
    "            self.net.zero_grad()\n",
    "            y = self.net(x[bstrt[i]:bstop[i]])\n",
    "            error = self.criterion(y,t[bstrt[i]:bstop[i]])\n",
    "            error.backward()\n",
    "        \n",
    "            ## compute importance for each input\n",
    "            for j, module in enumerate(self.keys):\n",
    "                z = (module.weight.data*module.weight.grad +\\\n",
    "                     module.bias.data*module.bias.grad).pow(2)\n",
    "                importance[j] += z\n",
    "                \n",
    "        ## compute mean\n",
    "        for i, module in enumerate(self.keys):\n",
    "            importance[i] = importance[i]/len(bstrt) \n",
    "\n",
    "        if normalize:\n",
    "            sums = 0\n",
    "            count = 0\n",
    "            for imp in importance:\n",
    "                sums += imp.sum()\n",
    "                count += len(imp)\n",
    "            divider = sums/count ## total importance is number of neurons\n",
    "            for i in range(len(importance)):\n",
    "                importance[i] = importance[i]/divider\n",
    "            \n",
    "        return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(20, 10).std(dim=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pruning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pruner():\n",
    "    \n",
    "    def __init__(self, net, prune_mask=None):\n",
    "        self.net = net\n",
    "        self.keys = []\n",
    "        self.prune_mask = {}\n",
    "        self.forward_hook = {}\n",
    "        \n",
    "        self.activations = []\n",
    "        \n",
    "        for name, module in list(self.net._modules.items()):\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                self.keys.append(module)\n",
    "\n",
    "        if prune_mask is not None:\n",
    "            self.add_prune_mask(prune_mask)\n",
    "        self.remove_hook()\n",
    "        \n",
    "    def add_prune_mask(self, prune_mask):\n",
    "        for module, pm in zip(self.keys[:-1], prune_mask):\n",
    "            self.prune_mask[module] = pm.type(torch.float)\n",
    "        self.prune_mask[self.keys[-1]] = torch.ones(self.keys[-1].out_features, dtype=torch.float)\n",
    "            \n",
    "        \n",
    "    def prune_neurons(self, module, inp, out):\n",
    "        mask = self.prune_mask[module]\n",
    "        output = out*mask\n",
    "        \n",
    "        self.activations.append(output)\n",
    "        return output\n",
    "        \n",
    "    def forward(self, x, prune_mask=None):\n",
    "        if prune_mask:\n",
    "            self.add_prune_mask(prune_mask)\n",
    "            if len(self.forward_hook) == 0:\n",
    "                self.add_hook()\n",
    "        \n",
    "        y = self.net(x)\n",
    "        self.remove_hook()\n",
    "        return y\n",
    "        \n",
    "        \n",
    "    def add_hook(self):\n",
    "        if len(self.forward_hook) > 0:\n",
    "            self.remove_hook()\n",
    "            \n",
    "        self.forward_hook = {}\n",
    "        for name, module in list(self.net._modules.items()):\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                hook = module.register_forward_hook(self.prune_neurons)\n",
    "                self.forward_hook[module] = hook\n",
    "        return\n",
    "        \n",
    "    def remove_hook(self):       \n",
    "        for module in self.forward_hook.keys():\n",
    "            hook = self.forward_hook[module]\n",
    "            hook.remove()\n",
    "        self.forward_hook = {}\n",
    "        self.prune_mask = {}\n",
    "        self.activations = []\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting importance and aponz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = []\n",
    "classes = []\n",
    "\n",
    "## taylor fo modified\n",
    "methods = prunelib.taylorfo_mode_list[:3]\n",
    "classes += [Importance_TaylorFO_Modified_BN(net, criterion, config=prunelib.taylorfo_mode_config[method]) for method in methods]\n",
    "\n",
    "\n",
    "## Molchanov_group, APnZ, Magnitude\n",
    "methods += [\"Molchanov_group\", \"Molchanov_BN\", \"APnZ\"]\n",
    "classes += [\n",
    "            prunelib.Importance_Molchanov_2019(net, criterion),\n",
    "            Importance_Molchanov_BN(net, criterion),\n",
    "            prunelib.Importance_APoZ(net, criterion),\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taylorfo\n",
      "taylorfo_abs\n",
      "taylorfo_sq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1875 [00:00<00:16, 114.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Molchanov_group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:15<00:00, 123.38it/s]\n",
      "  1%|          | 11/1875 [00:00<00:17, 107.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Molchanov_BN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:14<00:00, 128.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APnZ\n"
     ]
    }
   ],
   "source": [
    "## gather all importances\n",
    "importances = []\n",
    "aponzs = []\n",
    "stds = []\n",
    "for i in range(len(methods)):\n",
    "    print(methods[i])\n",
    "    imp = classes[i].compute_significance(xx, yy)\n",
    "    if methods[i].startswith(\"taylorfo\"):\n",
    "        imp, aponz, std = imp\n",
    "        aponzs.append(aponz)\n",
    "        stds.append(std)\n",
    "    importances.append(imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.4595, 0.4791, 0.5166, 0.4861, 0.4944, 0.5337, 0.4693, 0.3742, 0.5517,\n",
       "         0.4649, 0.5320, 0.4928, 0.4915, 0.4889, 0.5059, 0.4861, 0.5688, 0.4711,\n",
       "         0.4488, 0.5264, 0.4800, 0.5444, 0.5210, 0.4251, 0.5261, 0.5160, 0.5289,\n",
       "         0.4850, 0.4975, 0.5541, 0.4770, 0.4448, 0.4113, 0.4777, 0.4723, 0.4626,\n",
       "         0.4122, 0.4629, 0.5275, 0.4990, 0.5132, 0.4519, 0.5701, 0.4670, 0.5548,\n",
       "         0.4778, 0.5104, 0.5284, 0.4530, 0.5938, 0.3519, 0.4909, 0.4295, 0.4384,\n",
       "         0.5258, 0.4792, 0.4690, 0.4087, 0.5297, 0.4581, 0.4712, 0.4616, 0.4160,\n",
       "         0.5481, 0.4981, 0.5123, 0.4907, 0.4775, 0.4674, 0.5692, 0.4992, 0.4146,\n",
       "         0.4655, 0.4683, 0.4863, 0.4900, 0.5031, 0.5070, 0.4227, 0.4076, 0.4235,\n",
       "         0.4841, 0.4986, 0.5068, 0.4988, 0.4849, 0.5232, 0.4821, 0.4702, 0.4958,\n",
       "         0.4115, 0.4232, 0.4593, 0.4891, 0.3406, 0.4801, 0.4864, 0.4383, 0.5098,\n",
       "         0.4651, 0.4922, 0.5456, 0.4212, 0.4891, 0.4661, 0.4977, 0.5504, 0.5469,\n",
       "         0.4725, 0.4326, 0.4662, 0.4653, 0.4245, 0.4557, 0.4380, 0.4657, 0.5289,\n",
       "         0.4602, 0.4055, 0.4831, 0.5285, 0.5030, 0.5787, 0.4976, 0.4573, 0.5053,\n",
       "         0.4792, 0.5126, 0.5073, 0.4785, 0.5074, 0.5143, 0.4732, 0.4795, 0.4928,\n",
       "         0.4648, 0.5266, 0.4596, 0.4980, 0.4959, 0.4823, 0.4862, 0.4972, 0.4083,\n",
       "         0.4124, 0.4418, 0.5076, 0.5448, 0.5187, 0.4875, 0.4879, 0.5150, 0.5103,\n",
       "         0.4784, 0.4708, 0.4778, 0.5395, 0.5106, 0.4896, 0.3999, 0.5341, 0.5283,\n",
       "         0.4906, 0.5192, 0.4905, 0.5071, 0.4686, 0.4853, 0.4454, 0.4804, 0.4557,\n",
       "         0.4115, 0.4828, 0.5088, 0.4645, 0.5185, 0.5108, 0.5148, 0.4723, 0.4986,\n",
       "         0.4253, 0.4809, 0.4924, 0.3980, 0.4338, 0.4622, 0.5041, 0.5312, 0.4470,\n",
       "         0.4619, 0.4581, 0.5718, 0.5181, 0.4608, 0.4493, 0.4717, 0.4294, 0.4745,\n",
       "         0.4723, 0.4763, 0.4834, 0.4757, 0.5037, 0.4660, 0.4512, 0.5532, 0.4548,\n",
       "         0.5169, 0.5329, 0.4835, 0.5099, 0.4307, 0.4629, 0.5012, 0.4842, 0.4523,\n",
       "         0.4860, 0.5012, 0.5431, 0.4594, 0.4684, 0.5112, 0.4517, 0.5373, 0.4894,\n",
       "         0.3938, 0.4662, 0.4489, 0.4878, 0.4668, 0.3989, 0.4859, 0.5221, 0.5445,\n",
       "         0.5037, 0.4457, 0.4313, 0.5446, 0.4751, 0.5271, 0.4786, 0.4799, 0.4560,\n",
       "         0.5211, 0.4683, 0.4847, 0.4745, 0.4599, 0.4316, 0.4836, 0.4777, 0.4260,\n",
       "         0.4577, 0.5065, 0.5256, 0.4705]),\n",
       " tensor([0.4962, 0.4757, 0.4130, 0.4578, 0.5272, 0.4705, 0.5018, 0.4403, 0.4599,\n",
       "         0.3955, 0.3643, 0.4394, 0.4822, 0.5167, 0.4602, 0.5406, 0.4481, 0.5581,\n",
       "         0.5002, 0.4499, 0.4149, 0.5213, 0.3822, 0.4266, 0.4026, 0.5664, 0.4609,\n",
       "         0.5629, 0.4601, 0.4594, 0.4775, 0.4153, 0.4975, 0.4702, 0.5021, 0.4847,\n",
       "         0.3810, 0.4613, 0.3638, 0.5024, 0.5049, 0.4535, 0.4333, 0.5421, 0.4412,\n",
       "         0.4923, 0.4987, 0.6083, 0.4835, 0.4500, 0.4520, 0.4588, 0.4604, 0.4525,\n",
       "         0.4523, 0.4088, 0.4084, 0.3899, 0.4078, 0.5048, 0.3875, 0.4295, 0.4324,\n",
       "         0.4618, 0.5597, 0.5045, 0.4672, 0.4584, 0.4887, 0.5257, 0.4443, 0.4790,\n",
       "         0.5797, 0.4507, 0.5311, 0.4879, 0.5271, 0.5050, 0.4623, 0.5077, 0.4170,\n",
       "         0.4131, 0.5248, 0.4204, 0.4299, 0.4271, 0.4680, 0.4315, 0.4484, 0.4807,\n",
       "         0.5428, 0.4494, 0.5013, 0.5236, 0.4339, 0.4471, 0.4888, 0.4501, 0.4250,\n",
       "         0.4456, 0.4160, 0.3791, 0.4355, 0.5551, 0.4487, 0.4895, 0.5027, 0.4562,\n",
       "         0.4634, 0.5130, 0.5129, 0.4150, 0.5326, 0.4929, 0.4765, 0.4329, 0.4830,\n",
       "         0.4527, 0.4870, 0.5055, 0.4378, 0.4785, 0.5137, 0.5220, 0.5133, 0.5157,\n",
       "         0.4209, 0.4753]),\n",
       " tensor([0.3916, 0.3703, 0.4473, 0.3859, 0.4497, 0.4004, 0.3680, 0.4360, 0.3830,\n",
       "         0.4220, 0.3936, 0.3906, 0.4386, 0.3974, 0.3518, 0.4102, 0.3944, 0.3961,\n",
       "         0.4008, 0.3198, 0.4094, 0.3923, 0.3978, 0.3473, 0.3539, 0.4550, 0.3750,\n",
       "         0.3568, 0.4240, 0.4134, 0.3552, 0.3986, 0.4050, 0.3784, 0.3996, 0.3325,\n",
       "         0.3968, 0.4376, 0.3701, 0.3857, 0.3855, 0.3908, 0.4212, 0.3974, 0.3493,\n",
       "         0.4517, 0.3579, 0.4072, 0.3300, 0.4109, 0.3929, 0.3825, 0.4265, 0.3546,\n",
       "         0.3251, 0.3645, 0.3715, 0.3794, 0.4382, 0.3785, 0.3796, 0.3576, 0.3616,\n",
       "         0.3893])]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aponzs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5561, 0.6352, 0.5023, 0.5189, 0.5694, 0.4510, 0.6529, 0.6222, 0.5174,\n",
      "        0.6005, 0.5191, 0.4678, 0.5526, 0.5611, 0.5083, 0.5567, 0.5057, 0.5329,\n",
      "        0.6497, 0.5042, 0.6027, 0.5472, 0.6309, 0.5837, 0.5707, 0.5306, 0.5622,\n",
      "        0.5831, 0.5243, 0.4830, 0.5475, 0.5679, 0.5639, 0.4948, 0.5485, 0.5767,\n",
      "        0.5637, 0.4863, 0.6139, 0.5428, 0.5831, 0.5952, 0.5368, 0.5031, 0.4752,\n",
      "        0.5230, 0.4949, 0.5379, 0.5413, 0.5656, 0.6220, 0.6306, 0.5856, 0.5674,\n",
      "        0.6088, 0.6318, 0.5420, 0.6031, 0.4025, 0.5256, 0.5206, 0.5399, 0.6209,\n",
      "        0.5127, 0.4317, 0.5459, 0.5434, 0.6067, 0.5251, 0.3440, 0.5300, 0.4433,\n",
      "        0.5023, 0.6380, 0.5146, 0.5759, 0.5137, 0.5357, 0.5722, 0.5486, 0.7055,\n",
      "        0.5236, 0.5066, 0.5864, 0.5412, 0.5445, 0.4892, 0.6100, 0.5556, 0.4538,\n",
      "        0.6001, 0.5635, 0.6861, 0.6062, 0.6894, 0.6244, 0.5819, 0.6021, 0.5455,\n",
      "        0.6175, 0.5067, 0.6357, 0.7128, 0.6425, 0.5474, 0.4082, 0.5430, 0.5556,\n",
      "        0.6264, 0.5220, 0.5381, 0.5396, 0.6073, 0.5409, 0.6055, 0.6374, 0.5767,\n",
      "        0.6819, 0.5910, 0.5019, 0.5205, 0.4057, 0.4841, 0.5248, 0.5870, 0.6641,\n",
      "        0.5520, 0.3870, 0.5827, 0.5145, 0.4819, 0.6096, 0.5727, 0.7053, 0.5430,\n",
      "        0.5293, 0.5320, 0.6138, 0.5108, 0.4891, 0.5001, 0.6134, 0.5941, 0.5912,\n",
      "        0.4986, 0.5971, 0.6679, 0.5066, 0.5052, 0.5287, 0.5483, 0.5433, 0.5548,\n",
      "        0.3875, 0.5467, 0.5077, 0.5968, 0.5516, 0.6468, 0.7089, 0.5085, 0.4860,\n",
      "        0.5992, 0.5954, 0.5594, 0.4914, 0.5479, 0.6335, 0.6386, 0.5718, 0.5360,\n",
      "        0.5188, 0.5446, 0.4262, 0.6379, 0.6497, 0.4798, 0.5562, 0.5519, 0.6334,\n",
      "        0.7105, 0.4968, 0.5097, 0.6784, 0.6393, 0.5762, 0.5855, 0.5802, 0.5389,\n",
      "        0.5675, 0.5399, 0.5309, 0.5533, 0.5944, 0.5160, 0.6657, 0.5550, 0.5476,\n",
      "        0.5854, 0.5526, 0.4842, 0.5662, 0.5452, 0.5302, 0.6517, 0.5136, 0.5934,\n",
      "        0.5663, 0.4376, 0.5294, 0.5739, 0.6133, 0.6224, 0.6401, 0.4868, 0.5902,\n",
      "        0.5825, 0.5702, 0.5033, 0.4851, 0.5385, 0.5636, 0.5697, 0.4318, 0.5200,\n",
      "        0.5010, 0.5622, 0.5974, 0.5599, 0.6124, 0.5630, 0.5686, 0.6159, 0.5895,\n",
      "        0.5413, 0.6300, 0.6959, 0.5814, 0.6149, 0.5850, 0.6335, 0.5346, 0.5725,\n",
      "        0.5062, 0.5584, 0.5346, 0.6763, 0.4765, 0.6206, 0.5574, 0.6488, 0.5045,\n",
      "        0.6095, 0.5319, 0.5338, 0.5924])\n",
      "tensor([0.4638, 0.5503, 0.5649, 0.5525, 0.5026, 0.5119, 0.4739, 0.5907, 0.5394,\n",
      "        0.5937, 0.7467, 0.5597, 0.5283, 0.5292, 0.5600, 0.4943, 0.6654, 0.4152,\n",
      "        0.4731, 0.5001, 0.5153, 0.5468, 0.6123, 0.5376, 0.5394, 0.5194, 0.5927,\n",
      "        0.6012, 0.5104, 0.5384, 0.4613, 0.5184, 0.5048, 0.5208, 0.4483, 0.5116,\n",
      "        0.6623, 0.5390, 0.6664, 0.4650, 0.4626, 0.5343, 0.6232, 0.4602, 0.6029,\n",
      "        0.4608, 0.4689, 0.4344, 0.5085, 0.5093, 0.4935, 0.4655, 0.5711, 0.6383,\n",
      "        0.5682, 0.5006, 0.5214, 0.5200, 0.6073, 0.3805, 0.5019, 0.5341, 0.4660,\n",
      "        0.4870, 0.4688, 0.5386, 0.5253, 0.4899, 0.4863, 0.5642, 0.5971, 0.6003,\n",
      "        0.5370, 0.4948, 0.4968, 0.4719, 0.4612, 0.4728, 0.5760, 0.4638, 0.6016,\n",
      "        0.6269, 0.5709, 0.5214, 0.6482, 0.5953, 0.5598, 0.6501, 0.5654, 0.5886,\n",
      "        0.5311, 0.5258, 0.4341, 0.5263, 0.4781, 0.5630, 0.4867, 0.5113, 0.6110,\n",
      "        0.5221, 0.5088, 0.5431, 0.5594, 0.5191, 0.5354, 0.4737, 0.4790, 0.5375,\n",
      "        0.4914, 0.4541, 0.5181, 0.5849, 0.6290, 0.5435, 0.5919, 0.5222, 0.5130,\n",
      "        0.5766, 0.6318, 0.5341, 0.5540, 0.4722, 0.4762, 0.4711, 0.4711, 0.4483,\n",
      "        0.6249, 0.5177])\n",
      "tensor([0.3102, 0.3322, 0.3308, 0.3438, 0.3474, 0.3401, 0.3283, 0.3471, 0.3457,\n",
      "        0.3107, 0.3498, 0.3310, 0.3468, 0.3364, 0.2935, 0.3254, 0.3424, 0.3299,\n",
      "        0.3222, 0.3628, 0.3478, 0.3136, 0.3146, 0.3529, 0.3553, 0.3059, 0.3427,\n",
      "        0.3402, 0.3499, 0.3078, 0.3410, 0.3213, 0.3289, 0.3399, 0.3427, 0.3378,\n",
      "        0.3473, 0.3296, 0.3572, 0.3453, 0.3285, 0.3419, 0.3454, 0.3410, 0.3331,\n",
      "        0.3446, 0.3373, 0.3692, 0.3633, 0.3246, 0.3509, 0.3092, 0.3253, 0.3565,\n",
      "        0.3461, 0.3380, 0.3458, 0.3301, 0.3338, 0.3366, 0.3322, 0.3189, 0.3532,\n",
      "        0.3566])\n"
     ]
    }
   ],
   "source": [
    "for bns in classes[0].keys[:-1]:\n",
    "#     print(bns.weight.data)\n",
    "    print(((-bns.bias.data*3/bns.weight.data)+1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([1.0047, 1.0040, 0.9974, 0.9939, 0.9754, 1.0131, 0.9484, 1.0526, 1.0099,\n",
       "          1.0419, 0.9687, 1.0016, 1.0218, 1.0697, 1.0319, 0.9778, 0.9604, 0.9736,\n",
       "          0.9775, 0.9658, 0.9999, 1.0364, 0.9842, 0.9982, 1.0128, 0.9619, 0.9997,\n",
       "          0.9969, 1.0345, 1.0102, 1.0058, 1.0090, 1.0048, 1.0381, 1.0308, 0.9535,\n",
       "          1.0027, 1.0268, 0.9707, 0.9913, 0.9427, 1.0130, 0.9583, 1.0028, 1.0203,\n",
       "          1.0211, 1.0575, 1.0910, 1.0033, 0.9904, 1.0057, 0.9605, 1.0136, 0.9985,\n",
       "          1.0124, 0.9619, 1.0070, 1.0375, 1.0939, 1.0133, 1.0059, 1.0350, 0.9605,\n",
       "          0.9993, 1.0747, 1.0371, 0.9977, 0.9568, 1.0040, 0.9930, 0.9859, 1.0164,\n",
       "          1.0248, 0.9603, 0.9895, 0.9816, 0.9840, 0.9857, 0.9541, 1.0218, 0.9861,\n",
       "          0.9882, 1.0116, 0.9890, 0.9961, 0.9799, 1.0911, 0.9819, 1.0171, 1.0628,\n",
       "          0.9829, 0.9850, 0.9589, 0.9577, 0.9977, 0.9939, 1.0112, 0.9768, 1.0686,\n",
       "          1.0034, 1.0120, 1.0165, 0.9852, 0.9822, 0.9795, 1.0676, 0.9930, 0.9687,\n",
       "          0.8955, 0.9724, 1.0391, 1.0722, 0.9692, 0.9889, 1.0529, 1.0311, 1.0043,\n",
       "          0.9382, 0.9861, 0.9931, 1.0304, 1.0517, 1.0067, 0.9939, 0.9916, 0.9095,\n",
       "          0.9626, 0.9144, 1.0793, 0.9712, 1.0059, 0.9539, 1.0156, 0.9983, 0.9931,\n",
       "          0.9871, 1.0274, 1.0451, 0.9534, 1.0407, 0.9706, 0.9845, 0.9527, 0.9893,\n",
       "          0.9704, 0.9827, 1.0723, 1.0146, 0.9593, 0.9615, 1.0172, 1.0598, 0.9821,\n",
       "          0.9878, 0.9665, 1.0500, 0.9800, 1.0029, 0.9806, 0.9987, 1.0865, 1.0403,\n",
       "          0.9707, 1.0094, 1.0094, 0.9828, 1.0217, 0.9839, 1.0249, 1.0500, 1.0287,\n",
       "          1.0271, 0.9750, 1.0016, 0.9558, 1.0278, 1.0340, 1.0043, 1.0783, 0.9731,\n",
       "          0.9437, 0.9828, 1.0433, 0.9561, 1.0305, 1.0389, 1.0071, 0.9986, 1.0319,\n",
       "          1.0432, 0.9653, 1.0085, 1.0996, 1.0880, 0.9945, 1.0361, 1.0033, 0.9768,\n",
       "          0.9913, 0.9507, 0.9968, 1.0155, 0.9684, 0.9296, 0.9488, 0.9205, 0.9975,\n",
       "          1.0781, 0.9992, 0.9412, 0.9867, 1.0127, 0.9665, 0.9785, 0.9715, 1.0382,\n",
       "          0.9572, 1.0097, 0.9739, 1.0471, 0.9980, 0.9109, 1.0569, 0.9799, 0.9866,\n",
       "          0.9985, 1.0070, 1.0184, 1.0052, 0.9940, 1.0096, 0.9939, 1.0620, 1.0223,\n",
       "          0.9483, 0.9669, 0.9607, 1.0174, 0.9808, 0.9916, 0.9570, 1.1180, 0.9961,\n",
       "          0.9702, 0.9760, 0.9939, 1.0275, 0.9584, 1.0149, 0.9924, 0.9605, 1.0006,\n",
       "          0.9714, 0.9733, 1.0164, 1.0017]),\n",
       "  tensor([0.9705, 0.9654, 0.9960, 1.0596, 0.9977, 1.0268, 0.9747, 0.9711, 0.9484,\n",
       "          1.0063, 1.0534, 0.9578, 0.9550, 0.9608, 0.9877, 0.9858, 1.0166, 0.9717,\n",
       "          1.0297, 1.0636, 1.0394, 0.9566, 0.9582, 0.9388, 0.9611, 0.9704, 1.1071,\n",
       "          0.9072, 1.0565, 0.9444, 1.0030, 0.9883, 0.9593, 1.0097, 1.0178, 0.9552,\n",
       "          0.9887, 0.9826, 1.0073, 1.0327, 0.9369, 0.9731, 1.0414, 1.0218, 1.0480,\n",
       "          1.0468, 1.0467, 1.0311, 1.0235, 1.0495, 1.0338, 1.0462, 0.9355, 0.9516,\n",
       "          0.9691, 1.0361, 1.0001, 0.9578, 0.9837, 1.0454, 1.0083, 1.0078, 0.9621,\n",
       "          0.9946, 1.0758, 0.9668, 1.0375, 1.0292, 1.0760, 0.9631, 1.0497, 0.9397,\n",
       "          0.9494, 1.0312, 0.9911, 0.9672, 1.0659, 1.0480, 0.9614, 1.0378, 0.9651,\n",
       "          1.0011, 0.9583, 1.0543, 1.0728, 0.9621, 0.9334, 1.0130, 0.9988, 1.0435,\n",
       "          0.9702, 0.9757, 0.9886, 0.9447, 1.0189, 1.0615, 0.9743, 0.9697, 0.9958,\n",
       "          1.0059, 1.0541, 0.9564, 0.9955, 1.0059, 0.9396, 1.0007, 1.0110, 1.0251,\n",
       "          0.9696, 1.0562, 0.9775, 0.9638, 0.9465, 1.0075, 1.0643, 1.0322, 1.0491,\n",
       "          0.9333, 0.9281, 1.0067, 0.9868, 0.8995, 1.0718, 1.0914, 0.9709, 1.0617,\n",
       "          0.9121, 1.0299]),\n",
       "  tensor([1.1379, 1.1327, 1.1292, 1.1226, 1.1153, 1.1314, 1.1308, 1.1180, 1.1306,\n",
       "          1.1381, 1.1290, 1.1326, 1.1332, 1.1412, 1.1530, 1.1468, 1.1321, 1.1260,\n",
       "          1.1384, 1.1260, 1.1318, 1.1420, 1.1443, 1.1323, 1.1113, 1.1422, 1.1177,\n",
       "          1.1369, 1.1161, 1.1307, 1.1208, 1.1474, 1.1344, 1.1249, 1.1269, 1.1224,\n",
       "          1.1234, 1.1304, 1.1158, 1.1280, 1.1374, 1.1313, 1.1144, 1.1302, 1.1322,\n",
       "          1.1195, 1.1308, 1.1116, 1.1200, 1.1399, 1.1190, 1.1422, 1.1391, 1.1147,\n",
       "          1.1221, 1.1220, 1.1247, 1.1365, 1.1361, 1.1282, 1.1415, 1.1361, 1.1200,\n",
       "          1.1271])],\n",
       " [tensor([1.0047, 1.0040, 0.9974, 0.9939, 0.9754, 1.0131, 0.9484, 1.0526, 1.0099,\n",
       "          1.0419, 0.9687, 1.0016, 1.0218, 1.0697, 1.0319, 0.9778, 0.9604, 0.9736,\n",
       "          0.9775, 0.9658, 0.9999, 1.0364, 0.9842, 0.9982, 1.0128, 0.9619, 0.9997,\n",
       "          0.9969, 1.0345, 1.0102, 1.0058, 1.0090, 1.0048, 1.0381, 1.0308, 0.9535,\n",
       "          1.0027, 1.0268, 0.9707, 0.9913, 0.9427, 1.0130, 0.9583, 1.0028, 1.0203,\n",
       "          1.0211, 1.0575, 1.0910, 1.0033, 0.9904, 1.0057, 0.9605, 1.0136, 0.9985,\n",
       "          1.0124, 0.9619, 1.0070, 1.0375, 1.0939, 1.0133, 1.0059, 1.0350, 0.9605,\n",
       "          0.9993, 1.0747, 1.0371, 0.9977, 0.9568, 1.0040, 0.9930, 0.9859, 1.0164,\n",
       "          1.0248, 0.9603, 0.9895, 0.9816, 0.9840, 0.9857, 0.9541, 1.0218, 0.9861,\n",
       "          0.9882, 1.0116, 0.9890, 0.9961, 0.9799, 1.0911, 0.9819, 1.0171, 1.0628,\n",
       "          0.9829, 0.9850, 0.9589, 0.9577, 0.9977, 0.9939, 1.0112, 0.9768, 1.0686,\n",
       "          1.0034, 1.0120, 1.0165, 0.9852, 0.9822, 0.9795, 1.0676, 0.9930, 0.9687,\n",
       "          0.8955, 0.9724, 1.0391, 1.0722, 0.9692, 0.9889, 1.0529, 1.0311, 1.0043,\n",
       "          0.9382, 0.9861, 0.9931, 1.0304, 1.0517, 1.0067, 0.9939, 0.9916, 0.9095,\n",
       "          0.9626, 0.9144, 1.0793, 0.9712, 1.0059, 0.9539, 1.0156, 0.9983, 0.9931,\n",
       "          0.9871, 1.0274, 1.0451, 0.9534, 1.0407, 0.9706, 0.9845, 0.9527, 0.9893,\n",
       "          0.9704, 0.9827, 1.0723, 1.0146, 0.9593, 0.9615, 1.0172, 1.0598, 0.9821,\n",
       "          0.9878, 0.9665, 1.0500, 0.9800, 1.0029, 0.9806, 0.9987, 1.0865, 1.0403,\n",
       "          0.9707, 1.0094, 1.0094, 0.9828, 1.0217, 0.9839, 1.0249, 1.0500, 1.0287,\n",
       "          1.0271, 0.9750, 1.0016, 0.9558, 1.0278, 1.0340, 1.0043, 1.0783, 0.9731,\n",
       "          0.9437, 0.9828, 1.0433, 0.9561, 1.0305, 1.0389, 1.0071, 0.9986, 1.0319,\n",
       "          1.0432, 0.9653, 1.0085, 1.0996, 1.0880, 0.9945, 1.0361, 1.0033, 0.9768,\n",
       "          0.9913, 0.9507, 0.9968, 1.0155, 0.9684, 0.9296, 0.9488, 0.9205, 0.9975,\n",
       "          1.0781, 0.9992, 0.9412, 0.9867, 1.0127, 0.9665, 0.9785, 0.9715, 1.0382,\n",
       "          0.9572, 1.0097, 0.9739, 1.0471, 0.9980, 0.9109, 1.0569, 0.9799, 0.9866,\n",
       "          0.9985, 1.0070, 1.0184, 1.0052, 0.9940, 1.0096, 0.9939, 1.0620, 1.0223,\n",
       "          0.9483, 0.9669, 0.9607, 1.0174, 0.9808, 0.9916, 0.9570, 1.1180, 0.9961,\n",
       "          0.9702, 0.9760, 0.9939, 1.0275, 0.9584, 1.0149, 0.9924, 0.9605, 1.0006,\n",
       "          0.9714, 0.9733, 1.0164, 1.0017]),\n",
       "  tensor([0.9705, 0.9654, 0.9960, 1.0596, 0.9977, 1.0268, 0.9747, 0.9711, 0.9484,\n",
       "          1.0063, 1.0534, 0.9578, 0.9550, 0.9608, 0.9877, 0.9858, 1.0166, 0.9717,\n",
       "          1.0297, 1.0636, 1.0394, 0.9566, 0.9582, 0.9388, 0.9611, 0.9704, 1.1071,\n",
       "          0.9072, 1.0565, 0.9444, 1.0030, 0.9883, 0.9593, 1.0097, 1.0178, 0.9552,\n",
       "          0.9887, 0.9826, 1.0073, 1.0327, 0.9369, 0.9731, 1.0414, 1.0218, 1.0480,\n",
       "          1.0468, 1.0467, 1.0311, 1.0235, 1.0495, 1.0338, 1.0462, 0.9355, 0.9516,\n",
       "          0.9691, 1.0361, 1.0001, 0.9578, 0.9837, 1.0454, 1.0083, 1.0078, 0.9621,\n",
       "          0.9946, 1.0758, 0.9668, 1.0375, 1.0292, 1.0760, 0.9631, 1.0497, 0.9397,\n",
       "          0.9494, 1.0312, 0.9911, 0.9672, 1.0659, 1.0480, 0.9614, 1.0378, 0.9651,\n",
       "          1.0011, 0.9583, 1.0543, 1.0728, 0.9621, 0.9334, 1.0130, 0.9988, 1.0435,\n",
       "          0.9702, 0.9757, 0.9886, 0.9447, 1.0189, 1.0615, 0.9743, 0.9697, 0.9958,\n",
       "          1.0059, 1.0541, 0.9564, 0.9955, 1.0059, 0.9396, 1.0007, 1.0110, 1.0251,\n",
       "          0.9696, 1.0562, 0.9775, 0.9638, 0.9465, 1.0075, 1.0643, 1.0322, 1.0491,\n",
       "          0.9333, 0.9281, 1.0067, 0.9868, 0.8995, 1.0718, 1.0914, 0.9709, 1.0617,\n",
       "          0.9121, 1.0299]),\n",
       "  tensor([1.1379, 1.1327, 1.1292, 1.1226, 1.1153, 1.1314, 1.1308, 1.1180, 1.1306,\n",
       "          1.1381, 1.1290, 1.1326, 1.1332, 1.1412, 1.1530, 1.1468, 1.1321, 1.1260,\n",
       "          1.1384, 1.1260, 1.1318, 1.1420, 1.1443, 1.1323, 1.1113, 1.1422, 1.1177,\n",
       "          1.1369, 1.1161, 1.1307, 1.1208, 1.1474, 1.1344, 1.1249, 1.1269, 1.1224,\n",
       "          1.1234, 1.1304, 1.1158, 1.1280, 1.1374, 1.1313, 1.1144, 1.1302, 1.1322,\n",
       "          1.1195, 1.1308, 1.1116, 1.1200, 1.1399, 1.1190, 1.1422, 1.1391, 1.1147,\n",
       "          1.1221, 1.1220, 1.1247, 1.1365, 1.1361, 1.1282, 1.1415, 1.1361, 1.1200,\n",
       "          1.1271])],\n",
       " [tensor([1.0047, 1.0040, 0.9974, 0.9939, 0.9754, 1.0131, 0.9484, 1.0526, 1.0099,\n",
       "          1.0419, 0.9687, 1.0016, 1.0218, 1.0697, 1.0319, 0.9778, 0.9604, 0.9736,\n",
       "          0.9775, 0.9658, 0.9999, 1.0364, 0.9842, 0.9982, 1.0128, 0.9619, 0.9997,\n",
       "          0.9969, 1.0345, 1.0102, 1.0058, 1.0090, 1.0048, 1.0381, 1.0308, 0.9535,\n",
       "          1.0027, 1.0268, 0.9707, 0.9913, 0.9427, 1.0130, 0.9583, 1.0028, 1.0203,\n",
       "          1.0211, 1.0575, 1.0910, 1.0033, 0.9904, 1.0057, 0.9605, 1.0136, 0.9985,\n",
       "          1.0124, 0.9619, 1.0070, 1.0375, 1.0939, 1.0133, 1.0059, 1.0350, 0.9605,\n",
       "          0.9993, 1.0747, 1.0371, 0.9977, 0.9568, 1.0040, 0.9930, 0.9859, 1.0164,\n",
       "          1.0248, 0.9603, 0.9895, 0.9816, 0.9840, 0.9857, 0.9541, 1.0218, 0.9861,\n",
       "          0.9882, 1.0116, 0.9890, 0.9961, 0.9799, 1.0911, 0.9819, 1.0171, 1.0628,\n",
       "          0.9829, 0.9850, 0.9589, 0.9577, 0.9977, 0.9939, 1.0112, 0.9768, 1.0686,\n",
       "          1.0034, 1.0120, 1.0165, 0.9852, 0.9822, 0.9795, 1.0676, 0.9930, 0.9687,\n",
       "          0.8955, 0.9724, 1.0391, 1.0722, 0.9692, 0.9889, 1.0529, 1.0311, 1.0043,\n",
       "          0.9382, 0.9861, 0.9931, 1.0304, 1.0517, 1.0067, 0.9939, 0.9916, 0.9095,\n",
       "          0.9626, 0.9144, 1.0793, 0.9712, 1.0059, 0.9539, 1.0156, 0.9983, 0.9931,\n",
       "          0.9871, 1.0274, 1.0451, 0.9534, 1.0407, 0.9706, 0.9845, 0.9527, 0.9893,\n",
       "          0.9704, 0.9827, 1.0723, 1.0146, 0.9593, 0.9615, 1.0172, 1.0598, 0.9821,\n",
       "          0.9878, 0.9665, 1.0500, 0.9800, 1.0029, 0.9806, 0.9987, 1.0865, 1.0403,\n",
       "          0.9707, 1.0094, 1.0094, 0.9828, 1.0217, 0.9839, 1.0249, 1.0500, 1.0287,\n",
       "          1.0271, 0.9750, 1.0016, 0.9558, 1.0278, 1.0340, 1.0043, 1.0783, 0.9731,\n",
       "          0.9437, 0.9828, 1.0433, 0.9561, 1.0305, 1.0389, 1.0071, 0.9986, 1.0319,\n",
       "          1.0432, 0.9653, 1.0085, 1.0996, 1.0880, 0.9945, 1.0361, 1.0033, 0.9768,\n",
       "          0.9913, 0.9507, 0.9968, 1.0155, 0.9684, 0.9296, 0.9488, 0.9205, 0.9975,\n",
       "          1.0781, 0.9992, 0.9412, 0.9867, 1.0127, 0.9665, 0.9785, 0.9715, 1.0382,\n",
       "          0.9572, 1.0097, 0.9739, 1.0471, 0.9980, 0.9109, 1.0569, 0.9799, 0.9866,\n",
       "          0.9985, 1.0070, 1.0184, 1.0052, 0.9940, 1.0096, 0.9939, 1.0620, 1.0223,\n",
       "          0.9483, 0.9669, 0.9607, 1.0174, 0.9808, 0.9916, 0.9570, 1.1180, 0.9961,\n",
       "          0.9702, 0.9760, 0.9939, 1.0275, 0.9584, 1.0149, 0.9924, 0.9605, 1.0006,\n",
       "          0.9714, 0.9733, 1.0164, 1.0017]),\n",
       "  tensor([0.9705, 0.9654, 0.9960, 1.0596, 0.9977, 1.0268, 0.9747, 0.9711, 0.9484,\n",
       "          1.0063, 1.0534, 0.9578, 0.9550, 0.9608, 0.9877, 0.9858, 1.0166, 0.9717,\n",
       "          1.0297, 1.0636, 1.0394, 0.9566, 0.9582, 0.9388, 0.9611, 0.9704, 1.1071,\n",
       "          0.9072, 1.0565, 0.9444, 1.0030, 0.9883, 0.9593, 1.0097, 1.0178, 0.9552,\n",
       "          0.9887, 0.9826, 1.0073, 1.0327, 0.9369, 0.9731, 1.0414, 1.0218, 1.0480,\n",
       "          1.0468, 1.0467, 1.0311, 1.0235, 1.0495, 1.0338, 1.0462, 0.9355, 0.9516,\n",
       "          0.9691, 1.0361, 1.0001, 0.9578, 0.9837, 1.0454, 1.0083, 1.0078, 0.9621,\n",
       "          0.9946, 1.0758, 0.9668, 1.0375, 1.0292, 1.0760, 0.9631, 1.0497, 0.9397,\n",
       "          0.9494, 1.0312, 0.9911, 0.9672, 1.0659, 1.0480, 0.9614, 1.0378, 0.9651,\n",
       "          1.0011, 0.9583, 1.0543, 1.0728, 0.9621, 0.9334, 1.0130, 0.9988, 1.0435,\n",
       "          0.9702, 0.9757, 0.9886, 0.9447, 1.0189, 1.0615, 0.9743, 0.9697, 0.9958,\n",
       "          1.0059, 1.0541, 0.9564, 0.9955, 1.0059, 0.9396, 1.0007, 1.0110, 1.0251,\n",
       "          0.9696, 1.0562, 0.9775, 0.9638, 0.9465, 1.0075, 1.0643, 1.0322, 1.0491,\n",
       "          0.9333, 0.9281, 1.0067, 0.9868, 0.8995, 1.0718, 1.0914, 0.9709, 1.0617,\n",
       "          0.9121, 1.0299]),\n",
       "  tensor([1.1379, 1.1327, 1.1292, 1.1226, 1.1153, 1.1314, 1.1308, 1.1180, 1.1306,\n",
       "          1.1381, 1.1290, 1.1326, 1.1332, 1.1412, 1.1530, 1.1468, 1.1321, 1.1260,\n",
       "          1.1384, 1.1260, 1.1318, 1.1420, 1.1443, 1.1323, 1.1113, 1.1422, 1.1177,\n",
       "          1.1369, 1.1161, 1.1307, 1.1208, 1.1474, 1.1344, 1.1249, 1.1269, 1.1224,\n",
       "          1.1234, 1.1304, 1.1158, 1.1280, 1.1374, 1.1313, 1.1144, 1.1302, 1.1322,\n",
       "          1.1195, 1.1308, 1.1116, 1.1200, 1.1399, 1.1190, 1.1422, 1.1391, 1.1147,\n",
       "          1.1221, 1.1220, 1.1247, 1.1365, 1.1361, 1.1282, 1.1415, 1.1361, 1.1200,\n",
       "          1.1271])]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0047, 1.0040, 0.9974, 0.9939, 0.9754, 1.0131, 0.9484, 1.0526, 1.0099,\n",
      "        1.0419, 0.9687, 1.0016, 1.0218, 1.0697, 1.0320, 0.9778, 0.9604, 0.9736,\n",
      "        0.9775, 0.9658, 0.9999, 1.0364, 0.9842, 0.9982, 1.0128, 0.9619, 0.9997,\n",
      "        0.9969, 1.0345, 1.0102, 1.0058, 1.0090, 1.0048, 1.0382, 1.0308, 0.9535,\n",
      "        1.0027, 1.0268, 0.9707, 0.9913, 0.9427, 1.0130, 0.9583, 1.0028, 1.0203,\n",
      "        1.0211, 1.0575, 1.0910, 1.0033, 0.9904, 1.0057, 0.9605, 1.0137, 0.9985,\n",
      "        1.0124, 0.9619, 1.0070, 1.0375, 1.0939, 1.0134, 1.0059, 1.0350, 0.9605,\n",
      "        0.9993, 1.0747, 1.0371, 0.9977, 0.9568, 1.0040, 0.9930, 0.9859, 1.0164,\n",
      "        1.0248, 0.9603, 0.9895, 0.9816, 0.9840, 0.9857, 0.9541, 1.0218, 0.9861,\n",
      "        0.9882, 1.0117, 0.9890, 0.9961, 0.9799, 1.0911, 0.9819, 1.0171, 1.0628,\n",
      "        0.9829, 0.9850, 0.9589, 0.9577, 0.9977, 0.9939, 1.0113, 0.9768, 1.0686,\n",
      "        1.0034, 1.0120, 1.0165, 0.9852, 0.9822, 0.9796, 1.0676, 0.9930, 0.9687,\n",
      "        0.8955, 0.9724, 1.0391, 1.0722, 0.9692, 0.9889, 1.0529, 1.0311, 1.0043,\n",
      "        0.9382, 0.9861, 0.9931, 1.0304, 1.0517, 1.0068, 0.9939, 0.9916, 0.9095,\n",
      "        0.9626, 0.9144, 1.0794, 0.9712, 1.0059, 0.9540, 1.0156, 0.9983, 0.9931,\n",
      "        0.9871, 1.0274, 1.0451, 0.9534, 1.0407, 0.9706, 0.9845, 0.9527, 0.9893,\n",
      "        0.9704, 0.9827, 1.0723, 1.0146, 0.9593, 0.9615, 1.0172, 1.0598, 0.9822,\n",
      "        0.9878, 0.9665, 1.0501, 0.9800, 1.0029, 0.9806, 0.9987, 1.0866, 1.0403,\n",
      "        0.9707, 1.0094, 1.0094, 0.9828, 1.0217, 0.9839, 1.0249, 1.0500, 1.0287,\n",
      "        1.0271, 0.9750, 1.0016, 0.9558, 1.0278, 1.0340, 1.0043, 1.0783, 0.9731,\n",
      "        0.9437, 0.9828, 1.0433, 0.9561, 1.0305, 1.0389, 1.0072, 0.9986, 1.0319,\n",
      "        1.0432, 0.9653, 1.0085, 1.0996, 1.0880, 0.9945, 1.0361, 1.0033, 0.9768,\n",
      "        0.9913, 0.9507, 0.9968, 1.0155, 0.9684, 0.9296, 0.9488, 0.9205, 0.9975,\n",
      "        1.0781, 0.9992, 0.9412, 0.9867, 1.0128, 0.9666, 0.9785, 0.9715, 1.0382,\n",
      "        0.9572, 1.0097, 0.9739, 1.0471, 0.9980, 0.9109, 1.0570, 0.9799, 0.9866,\n",
      "        0.9985, 1.0071, 1.0184, 1.0052, 0.9940, 1.0096, 0.9939, 1.0620, 1.0223,\n",
      "        0.9483, 0.9669, 0.9607, 1.0174, 0.9808, 0.9917, 0.9570, 1.1180, 0.9961,\n",
      "        0.9702, 0.9761, 0.9939, 1.0275, 0.9584, 1.0149, 0.9924, 0.9606, 1.0006,\n",
      "        0.9714, 0.9733, 1.0164, 1.0017])\n",
      "tensor([0.9706, 0.9654, 0.9960, 1.0596, 0.9977, 1.0268, 0.9747, 0.9711, 0.9484,\n",
      "        1.0063, 1.0534, 0.9578, 0.9550, 0.9608, 0.9877, 0.9859, 1.0166, 0.9717,\n",
      "        1.0297, 1.0636, 1.0394, 0.9566, 0.9582, 0.9388, 0.9611, 0.9704, 1.1071,\n",
      "        0.9072, 1.0565, 0.9444, 1.0030, 0.9883, 0.9593, 1.0097, 1.0178, 0.9552,\n",
      "        0.9887, 0.9826, 1.0073, 1.0327, 0.9369, 0.9731, 1.0415, 1.0218, 1.0481,\n",
      "        1.0468, 1.0467, 1.0311, 1.0235, 1.0496, 1.0338, 1.0462, 0.9355, 0.9516,\n",
      "        0.9691, 1.0361, 1.0001, 0.9578, 0.9837, 1.0454, 1.0083, 1.0078, 0.9621,\n",
      "        0.9946, 1.0758, 0.9668, 1.0376, 1.0292, 1.0761, 0.9631, 1.0498, 0.9397,\n",
      "        0.9494, 1.0312, 0.9911, 0.9672, 1.0659, 1.0480, 0.9614, 1.0378, 0.9651,\n",
      "        1.0011, 0.9583, 1.0543, 1.0728, 0.9621, 0.9334, 1.0130, 0.9988, 1.0435,\n",
      "        0.9702, 0.9757, 0.9886, 0.9447, 1.0189, 1.0615, 0.9744, 0.9697, 0.9958,\n",
      "        1.0059, 1.0541, 0.9564, 0.9955, 1.0059, 0.9396, 1.0007, 1.0110, 1.0252,\n",
      "        0.9696, 1.0563, 0.9775, 0.9638, 0.9465, 1.0076, 1.0644, 1.0322, 1.0491,\n",
      "        0.9333, 0.9281, 1.0067, 0.9868, 0.8995, 1.0718, 1.0914, 0.9709, 1.0617,\n",
      "        0.9121, 1.0299])\n",
      "tensor([1.1379, 1.1327, 1.1292, 1.1227, 1.1153, 1.1314, 1.1308, 1.1180, 1.1306,\n",
      "        1.1381, 1.1290, 1.1326, 1.1332, 1.1412, 1.1530, 1.1468, 1.1322, 1.1260,\n",
      "        1.1384, 1.1260, 1.1318, 1.1420, 1.1443, 1.1323, 1.1113, 1.1422, 1.1177,\n",
      "        1.1369, 1.1161, 1.1308, 1.1208, 1.1474, 1.1345, 1.1249, 1.1269, 1.1224,\n",
      "        1.1234, 1.1304, 1.1158, 1.1280, 1.1374, 1.1313, 1.1144, 1.1302, 1.1322,\n",
      "        1.1195, 1.1308, 1.1116, 1.1200, 1.1399, 1.1190, 1.1422, 1.1391, 1.1147,\n",
      "        1.1221, 1.1221, 1.1247, 1.1365, 1.1361, 1.1282, 1.1415, 1.1361, 1.1200,\n",
      "        1.1271])\n"
     ]
    }
   ],
   "source": [
    "for bns in classes[0].keys[:-1]:\n",
    "    print(bns.weight.data)\n",
    "#     print(bns.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
