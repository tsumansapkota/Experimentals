{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "import random, os, pathlib, time\n",
    "from tqdm import tqdm\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nflib\n",
    "from nflib.flows import SequentialFlow, NormalizingFlow, ActNorm, AffineConstantFlow\n",
    "import nflib.coupling_flows as icf\n",
    "import nflib.inn_flow as inn\n",
    "import nflib.res_flow as irf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import distributions\n",
    "from torch.distributions import MultivariateNormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mylibrary.datasets as datasets\n",
    "import mylibrary.nnlib as tnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = datasets.MNIST()\n",
    "train_data, train_label_, test_data, test_label_ = mnist.load()\n",
    "\n",
    "train_data = train_data / 255.\n",
    "test_data = test_data / 255.\n",
    "\n",
    "# train_label = tnn.Logits.index_to_logit(train_label_)\n",
    "train_size = len(train_label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting data to pytorch format\n",
    "train_data = torch.Tensor(train_data)\n",
    "test_data = torch.Tensor(test_data)\n",
    "train_label = torch.LongTensor(train_label_)\n",
    "test_label = torch.LongTensor(test_label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Dataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        \n",
    "#         self.label = mask.type(torch.float32).reshape(-1,1)\n",
    "        self._shuffle_data_()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _shuffle_data_(self):\n",
    "        randidx = random.sample(range(len(self.data)), k=len(self.data))\n",
    "        self.data = self.data[randidx]\n",
    "        self.label = self.label[randidx]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, lbl = self.data[idx], self.label[idx]\n",
    "        return img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST_Dataset(train_data, train_label)\n",
    "test_dataset = MNIST_Dataset(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectedClassifier_Softmax(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim, num_sets, output_dim, inv_temp=1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_sets = num_sets\n",
    "        self.inv_temp = nn.Parameter(torch.ones(1)*inv_temp)\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, num_sets)\n",
    "        self.linear.bias.data *= 0\n",
    "        self.linear.weight.data *= 0.1\n",
    "        self.cls_weight = nn.Parameter(torch.ones(num_sets, output_dim)/output_dim)\n",
    "        self.cls_confidence = None\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hard=True):\n",
    "        x = self.linear(x)\n",
    "        if hard:\n",
    "            x = torch.softmax(-x*1e5, dim=1)\n",
    "        else:\n",
    "            x = torch.softmax(-x*self.inv_temp, dim=1)\n",
    "        self.cls_confidence = x\n",
    "        c = torch.softmax(self.cls_weight, dim=1)\n",
    "#         c = self.cls_weight\n",
    "        return x@c ## since both are normalized, it is also normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectedClassifier_SoftKMeans(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim, num_sets, output_dim, inv_temp=1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_sets = num_sets\n",
    "        self.inv_temp = nn.Parameter(torch.ones(1)*inv_temp)\n",
    "        \n",
    "        self.centers = nn.Parameter(torch.rand(num_sets, input_dim)*2-1)\n",
    "        \n",
    "#         self.cls_weight = nn.Parameter(torch.ones(num_sets, output_dim)/output_dim)\n",
    "\n",
    "        init_val = torch.randn(num_sets, output_dim)*0.01\n",
    "        for ns in range(num_sets):\n",
    "            init_val[ns, ns%output_dim] = 10.\n",
    "        self.cls_weight = nn.Parameter(init_val)\n",
    "\n",
    "        self.cls_confidence = None\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hard=False):\n",
    "        x = x[:, :self.input_dim]\n",
    "        dists = torch.cdist(x, self.centers)\n",
    "        dists = dists/np.sqrt(self.input_dim) ### correction to make diagonal of unit square 1 in nD space\n",
    "        \n",
    "        if hard:\n",
    "            x = torch.softmax(-dists*1e5, dim=1)\n",
    "        else:\n",
    "            x = torch.softmax(-dists*self.inv_temp, dim=1)\n",
    "        self.cls_confidence = x\n",
    "        c = torch.softmax(self.cls_weight, dim=1)\n",
    "#         c = self.cls_weight\n",
    "        return x@c ## since both are normalized, it is also normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "actf = irf.Swish\n",
    "flows = [\n",
    "    ActNorm(784),\n",
    "    irf.ResidualFlow(784, [784], activation=actf),\n",
    "    ActNorm(784),\n",
    "    irf.ResidualFlow(784, [784], activation=actf),\n",
    "    ActNorm(784),\n",
    "        ]\n",
    "\n",
    "model = SequentialFlow(flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(nn.Linear(784, 784, bias=False),\n",
    "#                       nn.BatchNorm1d(784),\n",
    "#                       nn.SELU(),\n",
    "#                       nn.Linear(784, 784, bias=False),\n",
    "#                       nn.BatchNorm1d(784),\n",
    "#                       nn.SELU(),\n",
    "#                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialFlow(\n",
       "  (flows): ModuleList(\n",
       "    (0): ActNorm()\n",
       "    (1): ResidualFlow(\n",
       "      (resblock): ModuleList(\n",
       "        (0): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (1): Swish()\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ActNorm()\n",
       "    (3): ResidualFlow(\n",
       "      (resblock): ModuleList(\n",
       "        (0): Linear(in_features=784, out_features=784, bias=True)\n",
       "        (1): Swish()\n",
       "        (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (4): ActNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ConnectedClassifier_SoftKMeans(784, 100, 10)\n",
    "# classifier = ConnectedClassifier_Softmax(784, 10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(dataset=train_dataset, num_workers=4, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, num_workers=4, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  2466466\n"
     ]
    }
   ],
   "source": [
    "# criterion = nn.NLLLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(list(model.parameters())+list(classifier.parameters()),\n",
    "                       lr=learning_rate, weight_decay=1e-15) # todo tune WD\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "print(\"number of params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(torch.isnan(p).type(torch.float32).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.6462e-01, -8.2422e-01,  8.1742e-01,  ...,  4.4810e-01,\n",
       "          1.2630e+00, -4.7599e-01],\n",
       "        [-5.1802e-01,  1.9383e-01, -1.3537e+00,  ...,  9.6932e-01,\n",
       "          4.4705e-01, -8.1837e-01],\n",
       "        [ 3.3639e-01,  5.7598e-01, -9.1273e-01,  ..., -1.2152e+00,\n",
       "         -7.9978e-01,  4.9138e-01],\n",
       "        ...,\n",
       "        [ 3.1210e-01, -9.5864e-01,  3.9177e-01,  ...,  3.7570e-01,\n",
       "          8.2501e-01,  6.3896e-01],\n",
       "        [-1.1926e+00,  1.1075e+00, -1.1066e+00,  ..., -4.7953e-01,\n",
       "          3.2403e-01,  1.3781e+00],\n",
       "        [ 1.4601e-03,  2.0014e+00,  2.3595e-01,  ...,  8.9468e-01,\n",
       "         -4.2698e-01, -3.1332e-01]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(10, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 784])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = iter(test_loader).next()[0]\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [01:17<00:00, 15.51it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0:0,  Loss:1.747867465019226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 67.87it/s]\n",
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:60.77%, Test Acc:92.35%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [01:16<00:00, 15.72it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1:0,  Loss:1.5182769298553467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 66.13it/s]\n",
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:94.60%, Test Acc:96.47%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [01:14<00:00, 16.19it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2:0,  Loss:1.5134742259979248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 68.87it/s]\n",
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:96.92%, Test Acc:97.38%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [01:16<00:00, 15.67it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3:0,  Loss:1.48512601852417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 62.93it/s]\n",
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:97.75%, Test Acc:97.39%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [01:19<00:00, 15.11it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4:0,  Loss:1.5048245191574097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 61.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:98.14%, Test Acc:97.78%\n",
      "\n",
      "\t-> Train Acc 98.14333333333335 ; Test Acc 97.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "EPOCHS = 5\n",
    "\n",
    "index = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_acc = 0\n",
    "    train_count = 0\n",
    "    for xx, yy in tqdm(train_loader):\n",
    "#     for xx, yy in tqdm(test_loader):\n",
    "\n",
    "        yout = model(xx)\n",
    "#         print(yout)\n",
    "        yout = classifier(yout)    \n",
    "#         print(yout)\n",
    "        loss = criterion(yout, yy)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(float(loss))\n",
    "\n",
    "        outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "        correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "        train_acc += correct\n",
    "        train_count += len(outputs)\n",
    "#         break\n",
    "\n",
    "    train_accs.append(float(train_acc)/train_count*100)\n",
    "    train_acc = 0\n",
    "    train_count = 0\n",
    "\n",
    "    print(f'Epoch: {epoch}:{index},  Loss:{float(loss)}')\n",
    "    test_count = 0\n",
    "    test_acc = 0\n",
    "    for xx, yy in tqdm(test_loader):\n",
    "        with torch.no_grad():\n",
    "            yout = classifier(model(xx))    \n",
    "        outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "        correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "        test_acc += correct\n",
    "        test_count += len(xx)\n",
    "    test_accs.append(float(test_acc)/test_count*100)\n",
    "    print(f'Train Acc:{train_accs[-1]:.2f}%, Test Acc:{test_accs[-1]:.2f}%')\n",
    "    print()\n",
    "\n",
    "### after each class index is finished training\n",
    "print(f'\\t-> Train Acc {max(train_accs)} ; Test Acc {max(test_accs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.cls_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 61.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Test Acc:97.77%\n",
      "[0, 0, 0, 0, 961, 0, 0, 8, 0, 0, 0, 0, 10, 1, 0, 0, 0, 2, 2, 2, 0, 7, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1012, 0, 1, 0, 0, 0, 0, 1, 8, 0, 0, 1132, 1010, 0, 6, 0, 5, 0, 4, 0, 0, 0, 0, 3, 0, 2, 0, 3, 942, 5, 0, 0, 0, 1, 0, 893, 930, 1037, 0, 0, 0, 0, 0, 995, 0, 0, 0, 0, 0, 1006, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_count = 0\n",
    "test_acc = 0\n",
    "set_count = torch.zeros(classifier.num_sets)\n",
    "for xx, yy in tqdm(test_loader):\n",
    "    with torch.no_grad():\n",
    "        yout = classifier(model(xx), hard=True)\n",
    "        set_indx, count = torch.unique(torch.argmax(classifier.cls_confidence, dim=1), return_counts=True) \n",
    "        set_count[set_indx] += count\n",
    "    outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "    correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "    test_acc += correct\n",
    "    test_count += len(xx)\n",
    "\n",
    "print(f'Hard Test Acc:{float(test_acc)/test_count*100:.2f}%')\n",
    "print(set_count.type(torch.long).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:18<00:00, 65.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Train Acc:98.52%\n",
      "[0, 0, 0, 0, 5787, 0, 0, 16, 0, 3, 0, 0, 30, 0, 0, 0, 0, 2, 13, 17, 0, 18, 0, 13, 0, 0, 0, 0, 86, 0, 0, 0, 0, 30, 0, 0, 0, 0, 2, 2, 5985, 0, 18, 1, 0, 0, 0, 3, 36, 1, 20, 6710, 5904, 1, 16, 0, 13, 1, 6, 0, 0, 0, 0, 14, 0, 13, 0, 12, 5676, 13, 0, 0, 0, 0, 0, 5408, 5858, 6328, 0, 0, 0, 0, 0, 5972, 0, 0, 0, 0, 0, 5969, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_count = 0\n",
    "test_acc = 0\n",
    "set_count = torch.zeros(classifier.num_sets)\n",
    "for xx, yy in tqdm(train_loader):\n",
    "    with torch.no_grad():\n",
    "        yout = classifier(model(xx), hard=True)\n",
    "        set_indx, count = torch.unique(torch.argmax(classifier.cls_confidence, dim=1), return_counts=True) \n",
    "        set_count[set_indx] += count\n",
    "    outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "    correct = (outputs == yy.data.cpu().numpy()).astype(float).sum()\n",
    "    test_acc += correct\n",
    "    test_count += len(xx)\n",
    "\n",
    "print(f'Hard Train Acc:{float(test_acc)/test_count*100:.2f}%')\n",
    "print(set_count.type(torch.long).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(39)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.count_nonzero(set_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3,\n",
       "        4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7,\n",
       "        8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,\n",
       "        2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5,\n",
       "        6, 7, 8, 9])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(classifier.cls_weight, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 3.2481, -1.9120, -1.8775, -1.8452, -2.0118, -1.8551, -1.8489, -1.9447,\n",
       "         -1.9533, -2.0220],\n",
       "        [-2.2413,  3.4531, -2.1003, -2.0808, -2.2142, -2.2052, -2.1769, -2.0775,\n",
       "         -2.1191, -2.2117],\n",
       "        [-2.2893, -1.9798,  3.5763, -2.0842, -2.3458, -2.3421, -2.1180, -2.2457,\n",
       "         -2.1323, -2.3975],\n",
       "        [-2.4627, -2.3062, -2.3605,  3.8263, -2.5696, -2.2584, -2.5298, -2.5112,\n",
       "         -2.3591, -2.5930],\n",
       "        [-3.9166, -3.7967, -3.8445, -3.8763,  5.1556, -3.8806, -3.7623, -3.6258,\n",
       "         -3.8696, -3.1604],\n",
       "        [-1.9905, -2.1742, -2.2445, -1.7629, -2.2293,  3.5023, -2.1339, -2.2450,\n",
       "         -1.9059, -2.2329],\n",
       "        [-2.0167, -2.0158, -1.9016, -2.0930, -2.0277, -2.0935,  3.4135, -2.1528,\n",
       "         -2.0537, -2.1572],\n",
       "        [-3.0335, -2.8421, -2.9603, -2.9363, -2.8616, -3.0288, -3.0203,  4.2774,\n",
       "         -2.9650, -2.5665],\n",
       "        [-2.1527, -1.9891, -2.0262, -1.9953, -2.2812, -2.0870, -2.1779, -2.2314,\n",
       "          3.5292, -2.2495],\n",
       "        [-2.8236, -2.6532, -2.8081, -2.7690, -1.9864, -2.8052, -2.7747, -2.2089,\n",
       "         -2.7383,  4.0892],\n",
       "        [ 3.2397, -1.8929, -1.8933, -1.8254, -2.0198, -1.8527, -1.8233, -1.9350,\n",
       "         -1.9585, -2.0101],\n",
       "        [-2.2053,  3.4234, -2.0610, -2.0298, -2.1760, -2.1729, -2.1397, -2.0628,\n",
       "         -2.0579, -2.1709],\n",
       "        [-2.9869, -2.8297,  4.3526, -2.9157, -3.1239, -3.1217, -2.8473, -3.0826,\n",
       "         -2.9643, -3.1674],\n",
       "        [-2.4432, -2.3275, -2.3372,  3.7998, -2.5347, -2.2355, -2.5044, -2.4534,\n",
       "         -2.3311, -2.5432],\n",
       "        [-2.3200, -2.1549, -2.2193, -2.2957,  3.5636, -2.2735, -2.1771, -2.0452,\n",
       "         -2.2568, -1.5444],\n",
       "        [-1.3733, -1.5006, -1.6295, -1.0410, -1.6968,  2.9234, -1.6279, -1.6244,\n",
       "         -1.2656, -1.6384],\n",
       "        [-2.2010, -2.2157, -2.0930, -2.2722, -2.2364, -2.3247,  3.6077, -2.3441,\n",
       "         -2.2827, -2.3683],\n",
       "        [-2.7892, -2.5827, -2.7335, -2.6991, -2.6178, -2.8058, -2.8046,  4.0441,\n",
       "         -2.7549, -2.3534],\n",
       "        [-2.7429, -2.4561, -2.4975, -2.4663, -2.7623, -2.6009, -2.6823, -2.7012,\n",
       "          4.0038, -2.7426],\n",
       "        [-2.9748, -2.7401, -2.9313, -2.8921, -2.1416, -2.9294, -2.9258, -2.2906,\n",
       "         -2.8851,  4.2044],\n",
       "        [ 3.2484, -1.9099, -1.8775, -1.8771, -1.9941, -1.8827, -1.8399, -1.9261,\n",
       "         -1.9563, -2.0214],\n",
       "        [-2.7341,  3.9522, -2.5866, -2.5938, -2.7012, -2.6976, -2.6562, -2.5868,\n",
       "         -2.5812, -2.6849],\n",
       "        [-2.3146, -2.0219,  3.5855, -2.1171, -2.3497, -2.3511, -2.0200, -2.3042,\n",
       "         -2.1725, -2.3803],\n",
       "        [-2.6640, -2.5297, -2.6073,  4.0429, -2.8233, -2.4573, -2.7631, -2.7227,\n",
       "         -2.6145, -2.8155],\n",
       "        [-2.3880, -2.2371, -2.3326, -2.3336,  3.6136, -2.3491, -2.2218, -2.0551,\n",
       "         -2.3117, -1.5402],\n",
       "        [-1.7497, -1.9068, -1.9770, -1.4524, -1.9867,  3.2520, -1.9108, -1.9908,\n",
       "         -1.6294, -1.9803],\n",
       "        [-2.0304, -2.0402, -1.9293, -2.1199, -2.0803, -2.1417,  3.4428, -2.1533,\n",
       "         -2.0967, -2.1767],\n",
       "        [-2.5794, -2.3577, -2.5213, -2.4866, -2.3923, -2.5822, -2.5672,  3.8305,\n",
       "         -2.5425, -2.1091],\n",
       "        [-2.9253, -2.6770, -2.7445, -2.7296, -2.9659, -2.7987, -2.8821, -2.9828,\n",
       "          4.2190, -2.9898],\n",
       "        [-2.6695, -2.5324, -2.6510, -2.6311, -1.8095, -2.6605, -2.6215, -2.0961,\n",
       "         -2.6039,  3.9556],\n",
       "        [ 3.4891, -2.1531, -2.1238, -2.1032, -2.2698, -2.1356, -2.1125, -2.1856,\n",
       "         -2.1888, -2.2805],\n",
       "        [-2.2610,  3.4690, -2.0981, -2.0770, -2.2233, -2.2145, -2.1869, -2.1132,\n",
       "         -2.0835, -2.2168],\n",
       "        [-2.2823, -2.1319,  3.6444, -2.1637, -2.4073, -2.3897, -2.1426, -2.3620,\n",
       "         -2.2017, -2.4630],\n",
       "        [-2.7619, -2.6393, -2.6221,  4.1261, -2.9035, -2.6030, -2.8241, -2.8327,\n",
       "         -2.6753, -2.9056],\n",
       "        [-2.5909, -2.4204, -2.4901, -2.5324,  3.8150, -2.5211, -2.4274, -2.2459,\n",
       "         -2.5083, -1.8140],\n",
       "        [-1.8108, -2.0848, -2.1132, -1.6413, -2.1864,  3.4053, -2.0964, -2.1910,\n",
       "         -1.7301, -2.1706],\n",
       "        [-1.9405, -1.9499, -1.8334, -2.0616, -1.9544, -2.0604,  3.3503, -2.0906,\n",
       "         -2.0188, -2.0919],\n",
       "        [-2.4634, -2.2902, -2.4398, -2.3890, -2.3211, -2.4825, -2.4971,  3.7514,\n",
       "         -2.4506, -2.0351],\n",
       "        [-2.5650, -2.3949, -2.4357, -2.3664, -2.6091, -2.4107, -2.5221, -2.5484,\n",
       "          3.8730, -2.5851],\n",
       "        [-2.8729, -2.6723, -2.8127, -2.7954, -2.0694, -2.8081, -2.7813, -2.2208,\n",
       "         -2.7175,  4.1131],\n",
       "        [ 5.1287, -3.8111, -3.7734, -3.7417, -3.8976, -3.7485, -3.7533, -3.8131,\n",
       "         -3.8265, -3.9031],\n",
       "        [-2.2566,  3.4845, -2.1151, -2.0961, -2.2311, -2.2256, -2.2216, -2.1242,\n",
       "         -2.1265, -2.2503],\n",
       "        [-2.9016, -2.7032,  4.2086, -2.7262, -2.9799, -2.9700, -2.7347, -2.9248,\n",
       "         -2.8007, -3.0305],\n",
       "        [-2.6195, -2.4189, -2.4793,  3.9530, -2.7169, -2.4104, -2.6544, -2.6411,\n",
       "         -2.4907, -2.7329],\n",
       "        [-2.4122, -2.3109, -2.3569, -2.3931,  3.6792, -2.3944, -2.2944, -2.1055,\n",
       "         -2.3883, -1.6527],\n",
       "        [-1.7699, -1.8491, -1.9855, -1.4772, -2.0333,  3.2727, -1.9800, -2.0221,\n",
       "         -1.6357, -2.0243],\n",
       "        [-1.9246, -1.8810, -1.7673, -1.9997, -1.9726, -2.0246,  3.3213, -2.0438,\n",
       "         -1.9867, -2.0814],\n",
       "        [-2.7419, -2.5229, -2.7032, -2.6569, -2.6026, -2.7627, -2.7510,  4.0125,\n",
       "         -2.6922, -2.3355],\n",
       "        [-2.8714, -2.6856, -2.7264, -2.6317, -2.9402, -2.7108, -2.8655, -2.9210,\n",
       "          4.1734, -2.9223],\n",
       "        [-2.8410, -2.6612, -2.8217, -2.7670, -2.0950, -2.7836, -2.8044, -2.2158,\n",
       "         -2.7303,  4.1109],\n",
       "        [ 3.6965, -2.3824, -2.3478, -2.3221, -2.4526, -2.3173, -2.3049, -2.4030,\n",
       "         -2.3904, -2.4581],\n",
       "        [-3.9405,  5.1845, -3.8153, -3.8098, -3.9459, -3.9318, -3.9017, -3.8260,\n",
       "         -3.8225, -3.9206],\n",
       "        [-3.7834, -3.5289,  5.0925, -3.6025, -3.8564, -3.8711, -3.6184, -3.8062,\n",
       "         -3.6500, -3.8870],\n",
       "        [-2.5287, -2.2880, -2.3762,  3.8428, -2.6048, -2.3066, -2.5473, -2.5027,\n",
       "         -2.3579, -2.6036],\n",
       "        [-2.8419, -2.6872, -2.7809, -2.7720,  4.0693, -2.7863, -2.7231, -2.5276,\n",
       "         -2.7673, -2.0448],\n",
       "        [-1.8181, -2.0832, -2.1479, -1.6584, -2.1924,  3.4177, -2.0986, -2.1598,\n",
       "         -1.8031, -2.1828],\n",
       "        [-2.3919, -2.4236, -2.3269, -2.5236, -2.4860, -2.5153,  3.8246, -2.5831,\n",
       "         -2.4721, -2.5966],\n",
       "        [-2.7489, -2.5312, -2.7159, -2.6799, -2.5930, -2.7539, -2.7620,  4.0133,\n",
       "         -2.6942, -2.3590],\n",
       "        [-2.7155, -2.4570, -2.5041, -2.5008, -2.7689, -2.5905, -2.6717, -2.7193,\n",
       "          4.0088, -2.7554],\n",
       "        [-2.4717, -2.3427, -2.4482, -2.4456, -1.7589, -2.4745, -2.4310, -1.8527,\n",
       "         -2.4262,  3.7749],\n",
       "        [ 3.3047, -1.9915, -1.9540, -1.9001, -2.0820, -1.8850, -1.9281, -2.0158,\n",
       "         -1.9965, -2.0747],\n",
       "        [-2.2904,  3.5315, -2.1753, -2.1443, -2.2742, -2.2736, -2.2423, -2.1851,\n",
       "         -2.1799, -2.2948],\n",
       "        [-2.2031, -2.0030,  3.5313, -1.9835, -2.3121, -2.2946, -2.0763, -2.2289,\n",
       "         -2.1189, -2.3392],\n",
       "        [-2.7949, -2.5019, -2.6448,  4.0971, -2.8642, -2.5706, -2.8206, -2.7653,\n",
       "         -2.6563, -2.8579],\n",
       "        [-2.4334, -2.3335, -2.3621, -2.3804,  3.6854, -2.4174, -2.3079, -2.1366,\n",
       "         -2.3721, -1.6635],\n",
       "        [-2.1852, -2.3444, -2.4239, -1.9401, -2.4910,  3.7139, -2.3564, -2.4782,\n",
       "         -2.0867, -2.4921],\n",
       "        [-2.1294, -2.1437, -2.0172, -2.2530, -2.1783, -2.2516,  3.5476, -2.2838,\n",
       "         -2.2129, -2.3016],\n",
       "        [-2.9486, -2.7118, -2.8719, -2.8146, -2.7572, -2.9515, -2.9307,  4.1923,\n",
       "         -2.8592, -2.4967],\n",
       "        [-3.7880, -3.5374, -3.6548, -3.5741, -3.8063, -3.6649, -3.7511, -3.7530,\n",
       "          5.0644, -3.7851],\n",
       "        [-2.9788, -2.7205, -2.9192, -2.8730, -2.1431, -2.9059, -2.9159, -2.2873,\n",
       "         -2.8512,  4.2041],\n",
       "        [ 3.2959, -1.9737, -1.9387, -1.9338, -2.0296, -1.9150, -1.8923, -1.9788,\n",
       "         -1.9996, -2.0399],\n",
       "        [-2.1065,  3.3190, -1.9439, -1.9554, -2.0462, -2.0663, -2.0311, -1.9119,\n",
       "         -1.9480, -2.0595],\n",
       "        [-2.4039, -2.1723,  3.7032, -2.2545, -2.4566, -2.4724, -2.2110, -2.3992,\n",
       "         -2.2936, -2.4956],\n",
       "        [-2.3983, -2.1813, -2.3331,  3.7304, -2.4853, -2.1596, -2.4607, -2.3874,\n",
       "         -2.2719, -2.4764],\n",
       "        [-2.4736, -2.3293, -2.3809, -2.4323,  3.7176, -2.4286, -2.3045, -2.1827,\n",
       "         -2.4019, -1.7261],\n",
       "        [-3.5592, -3.7060, -3.8208, -3.2775, -3.8493,  5.0684, -3.7741, -3.8668,\n",
       "         -3.4614, -3.8415],\n",
       "        [-3.7183, -3.7309, -3.6369, -3.8453, -3.7538, -3.8742,  5.1339, -3.8584,\n",
       "         -3.8210, -3.8880],\n",
       "        [-3.9028, -3.6405, -3.8458, -3.8153, -3.7083, -3.8905, -3.8614,  5.1412,\n",
       "         -3.8352, -3.4429],\n",
       "        [-2.3638, -2.1353, -2.2276, -2.1127, -2.4288, -2.2288, -2.3845, -2.3953,\n",
       "          3.6891, -2.4239],\n",
       "        [-2.4443, -2.2734, -2.3962, -2.3702, -1.6761, -2.3753, -2.3685, -1.8528,\n",
       "         -2.3247,  3.7185],\n",
       "        [ 3.4075, -2.0722, -2.0579, -2.0277, -2.1707, -2.0255, -2.0268, -2.0987,\n",
       "         -2.1105, -2.1974],\n",
       "        [-2.4176,  3.6366, -2.2677, -2.2698, -2.3638, -2.3793, -2.3370, -2.2794,\n",
       "         -2.3017, -2.4028],\n",
       "        [-2.1610, -2.0664,  3.5279, -2.0460, -2.2635, -2.2813, -1.9927, -2.2345,\n",
       "         -2.1042, -2.3215],\n",
       "        [-3.7989, -3.6258, -3.7336,  5.1541, -3.9206, -3.5939, -3.8527, -3.8332,\n",
       "         -3.6858, -3.8986],\n",
       "        [-2.3144, -2.1722, -2.2261, -2.2739,  3.5550, -2.2750, -2.1536, -1.9930,\n",
       "         -2.2500, -1.5168],\n",
       "        [-1.5565, -1.6934, -1.7774, -1.2103, -1.8642,  3.0790, -1.7316, -1.8150,\n",
       "         -1.4731, -1.8450],\n",
       "        [-2.2807, -2.2433, -2.1380, -2.3335, -2.2783, -2.3657,  3.6505, -2.3916,\n",
       "         -2.3188, -2.4169],\n",
       "        [-2.6929, -2.4792, -2.6334, -2.6191, -2.5302, -2.7034, -2.7113,  3.9632,\n",
       "         -2.6811, -2.2760],\n",
       "        [-2.5939, -2.3958, -2.4245, -2.3466, -2.6273, -2.4433, -2.5629, -2.5942,\n",
       "          3.8937, -2.6249],\n",
       "        [-3.8922, -3.7368, -3.8616, -3.8739, -3.1451, -3.8726, -3.8223, -3.3191,\n",
       "         -3.8252,  5.1636],\n",
       "        [ 3.3969, -2.0811, -2.0395, -2.0031, -2.1635, -2.0005, -2.0244, -2.0926,\n",
       "         -2.0833, -2.1756],\n",
       "        [-2.2327,  3.4571, -2.0957, -2.0775, -2.2122, -2.1825, -2.1780, -2.1314,\n",
       "         -2.0684, -2.2060],\n",
       "        [-2.8364, -2.5997,  4.1396, -2.6367, -2.9220, -2.9025, -2.6400, -2.8742,\n",
       "         -2.6776, -2.9550],\n",
       "        [-2.4975, -2.3191, -2.3818,  3.8396, -2.5854, -2.2510, -2.5248, -2.5072,\n",
       "         -2.3675, -2.5615],\n",
       "        [-2.7053, -2.4967, -2.6103, -2.6558,  3.9164, -2.6636, -2.5127, -2.4090,\n",
       "         -2.6164, -1.9294],\n",
       "        [-1.8693, -2.1294, -2.2519, -1.7619, -2.2864,  3.5077, -2.2398, -2.2517,\n",
       "         -1.9292, -2.2455],\n",
       "        [-1.9995, -1.9223, -1.8595, -2.0504, -1.9996, -2.0707,  3.3704, -2.1069,\n",
       "         -2.0337, -2.1343],\n",
       "        [-2.4931, -2.3039, -2.4968, -2.4438, -2.3833, -2.5301, -2.5359,  3.8007,\n",
       "         -2.4945, -2.1194],\n",
       "        [-2.7059, -2.4585, -2.4551, -2.4407, -2.7787, -2.5651, -2.6688, -2.7118,\n",
       "          3.9887, -2.7545],\n",
       "        [-2.7721, -2.6034, -2.7496, -2.6881, -1.9952, -2.7023, -2.7478, -2.1629,\n",
       "         -2.6505,  4.0389]], requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.cls_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.unique(torch.argmax(classifier.cls_confidence, dim=1), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([4.9740], requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.inv_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3953e-04, 1.7997e-04, 9.9867e-01, 1.6720e-04, 1.2971e-04, 1.2782e-04,\n",
       "        1.6456e-04, 1.3638e-04, 1.5945e-04, 1.2579e-04])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yout[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdfsdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-50d96eb4626a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0masdfsdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'asdfsdf' is not defined"
     ]
    }
   ],
   "source": [
    "asdfsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### analyze per classifier accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:18<00:00, 64.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Train Acc:98.52%\n",
      "[0, 0, 0, 0, 5787, 0, 0, 16, 0, 3, 0, 0, 30, 0, 0, 0, 0, 2, 13, 17, 0, 18, 0, 13, 0, 0, 0, 0, 86, 0, 0, 0, 0, 30, 0, 0, 0, 0, 2, 2, 5985, 0, 18, 1, 0, 0, 0, 3, 36, 1, 20, 6710, 5904, 1, 16, 0, 13, 1, 6, 0, 0, 0, 0, 14, 0, 13, 0, 12, 5676, 13, 0, 0, 0, 0, 0, 5408, 5858, 6328, 0, 0, 0, 0, 0, 5972, 0, 0, 0, 0, 0, 5969, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_count = 0\n",
    "test_acc = 0\n",
    "set_count = torch.zeros(classifier.num_sets)\n",
    "set_acc = torch.zeros(classifier.num_sets)\n",
    "for xx, yy in tqdm(train_loader):\n",
    "    with torch.no_grad():\n",
    "        yout = classifier(model(xx), hard=True)\n",
    "        \n",
    "    cls_indx = torch.argmax(classifier.cls_confidence, dim=1)\n",
    "    set_indx, count = torch.unique(cls_indx, return_counts=True) \n",
    "    set_count[set_indx] += count\n",
    "    \n",
    "    outputs = torch.argmax(yout, dim=1).data.cpu().numpy()\n",
    "    correct = (outputs == yy.data.cpu().numpy()).astype(float)\n",
    "    \n",
    "    ### class_index has 100 possible values\n",
    "    for i, c in enumerate(correct):\n",
    "        set_acc[cls_indx[i]] += c\n",
    "    \n",
    "#     print(set_acc.sum(), set_count.sum())\n",
    "#     break\n",
    "    test_acc += correct.sum()\n",
    "    test_count += len(xx)\n",
    "\n",
    "print(f'Hard Train Acc:{float(test_acc)/test_count*100:.2f}%')\n",
    "print(set_count.type(torch.long).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   nan,    nan,    nan,    nan, 0.9893,    nan,    nan, 0.3750,    nan,\n",
       "        0.0000,    nan,    nan, 0.7000,    nan,    nan,    nan,    nan, 0.0000,\n",
       "        0.8462, 0.3529,    nan, 0.3333,    nan, 0.6154,    nan,    nan,    nan,\n",
       "           nan, 0.8721,    nan,    nan,    nan,    nan, 0.6667,    nan,    nan,\n",
       "           nan,    nan, 0.0000, 0.5000, 0.9828,    nan, 0.6111, 0.0000,    nan,\n",
       "           nan,    nan, 0.0000, 0.8333, 0.0000, 0.5500, 0.9931, 0.9861, 0.0000,\n",
       "        0.5000,    nan, 0.8462, 1.0000, 0.3333,    nan,    nan,    nan,    nan,\n",
       "        0.9286,    nan, 0.3077,    nan, 0.5000, 0.9919, 0.4615,    nan,    nan,\n",
       "           nan,    nan,    nan, 0.9871, 0.9947, 0.9776,    nan,    nan,    nan,\n",
       "           nan,    nan, 0.9950,    nan,    nan,    nan,    nan,    nan, 0.9776,\n",
       "           nan,    nan, 1.0000,    nan,    nan,    nan,    nan,    nan, 0.5000,\n",
       "           nan])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_acc/set_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,\t 5787,\t 98.93%\n",
      "7,\t 16,\t 37.50%\n",
      "9,\t 3,\t 0.00%\n",
      "12,\t 30,\t 70.00%\n",
      "17,\t 2,\t 0.00%\n",
      "18,\t 13,\t 84.62%\n",
      "19,\t 17,\t 35.29%\n",
      "21,\t 18,\t 33.33%\n",
      "23,\t 13,\t 61.54%\n",
      "28,\t 86,\t 87.21%\n",
      "33,\t 30,\t 66.67%\n",
      "38,\t 2,\t 0.00%\n",
      "39,\t 2,\t 50.00%\n",
      "40,\t 5985,\t 98.28%\n",
      "42,\t 18,\t 61.11%\n",
      "43,\t 1,\t 0.00%\n",
      "47,\t 3,\t 0.00%\n",
      "48,\t 36,\t 83.33%\n",
      "49,\t 1,\t 0.00%\n",
      "50,\t 20,\t 55.00%\n",
      "51,\t 6710,\t 99.31%\n",
      "52,\t 5904,\t 98.61%\n",
      "53,\t 1,\t 0.00%\n",
      "54,\t 16,\t 50.00%\n",
      "56,\t 13,\t 84.62%\n",
      "57,\t 1,\t 100.00%\n",
      "58,\t 6,\t 33.33%\n",
      "63,\t 14,\t 92.86%\n",
      "65,\t 13,\t 30.77%\n",
      "67,\t 12,\t 50.00%\n",
      "68,\t 5676,\t 99.19%\n",
      "69,\t 13,\t 46.15%\n",
      "75,\t 5408,\t 98.71%\n",
      "76,\t 5858,\t 99.47%\n",
      "77,\t 6328,\t 97.76%\n",
      "83,\t 5972,\t 99.50%\n",
      "89,\t 5969,\t 97.76%\n",
      "92,\t 1,\t 100.00%\n",
      "98,\t 2,\t 50.00%\n"
     ]
    }
   ],
   "source": [
    "for i, (cnt, acc) in enumerate(zip(set_count.type(torch.long).tolist(), (set_acc/set_count).tolist())):\n",
    "    if cnt == 0: continue\n",
    "    print(f\"{i},\\t {cnt},\\t {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
