{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE parameters\n",
    "num_inputs = 10\n",
    "neurons_hid1 = 5\n",
    "neurons_hid2 = 3\n",
    "neurons_hid3 = 2\n",
    "\n",
    "learning_rate = 0.00003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PINV(PSEUDO-INVERSE) function\n",
    "def pinv(a, rcond=1e-15):\n",
    "    s, u, v = tf.svd(a)\n",
    "    # Ignore singular values close to zero to prevent numerical overflow\n",
    "    limit = rcond * tf.reduce_max(s)\n",
    "    non_zero = tf.greater(s, limit)\n",
    "\n",
    "    reciprocal = tf.where(non_zero, tf.reciprocal(s), tf.zeros(s.shape))\n",
    "    lhs = tf.matmul(v, tf.diag(reciprocal))\n",
    "    return tf.matmul(lhs, u, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSPOSE OR PINV\n",
    "tie_weight = tf.transpose #pinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLACEHOLDER DEFINATION\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WEIGHTS DEFINATION\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "w1 = tf.Variable(np.load('./save/weights/lw1.npy'))\n",
    "w1_ = tie_weight(w1)\n",
    "\n",
    "w2 = tf.Variable(np.load('./save/weights/lw2.npy'))\n",
    "w2_ = tie_weight(w2)\n",
    "\n",
    "w3 = tf.Variable(initializer([neurons_hid2, neurons_hid3]), dtype=tf.float32)\n",
    "w3_ = tie_weight(w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTIVATION FUNCTION  [ lambda X:X  <OR>  tf.nn.relu  ]\n",
    "act_func = lambda X:X #tf.nn.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYER MODELING OF :NN\n",
    "hid_layer1 = act_func(tf.matmul(X, w1))\n",
    "hid_layer2 = act_func(tf.matmul(hid_layer1, w2))\n",
    "\n",
    "hid_layer3 = act_func(tf.matmul(hid_layer2, w3))\n",
    "\n",
    "hid_layer2_= act_func(tf.matmul(hid_layer3, w3_))\n",
    "hid_layer1_= act_func(tf.matmul(hid_layer2_, w2_))\n",
    "output_layer = tf.nn.softmax(tf.matmul(hid_layer1_, w1_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTIONS\n",
    "# loss = tf.reduce_mean(tf.square(output_layer - X))\n",
    "# loss = tf.reduce_mean(tf.abs(output_layer - X))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=X,logits=output_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss, var_list=[w3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARATION\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Complete. Training Loss: 1.9471555948257446\n",
      "Epoch 1 Complete. Training Loss: 1.9665710926055908\n",
      "Epoch 2 Complete. Training Loss: 1.864491581916809\n",
      "Epoch 3 Complete. Training Loss: 1.8764228820800781\n",
      "Epoch 4 Complete. Training Loss: 1.9083211421966553\n",
      "Epoch 5 Complete. Training Loss: 1.9254591464996338\n",
      "Epoch 6 Complete. Training Loss: 1.9468547105789185\n",
      "Epoch 7 Complete. Training Loss: 1.8455122709274292\n",
      "Epoch 8 Complete. Training Loss: 1.8190712928771973\n",
      "Epoch 9 Complete. Training Loss: 1.899095058441162\n",
      "Epoch 10 Complete. Training Loss: 1.8678102493286133\n",
      "Epoch 11 Complete. Training Loss: 1.8378230333328247\n",
      "Epoch 12 Complete. Training Loss: 1.8705426454544067\n",
      "Epoch 13 Complete. Training Loss: 1.828100323677063\n",
      "Epoch 14 Complete. Training Loss: 1.8310855627059937\n",
      "Epoch 15 Complete. Training Loss: 1.8672418594360352\n",
      "Epoch 16 Complete. Training Loss: 1.870818018913269\n",
      "Epoch 17 Complete. Training Loss: 1.8299062252044678\n",
      "Epoch 18 Complete. Training Loss: 1.837077021598816\n",
      "Epoch 19 Complete. Training Loss: 1.7825109958648682\n",
      "Epoch 20 Complete. Training Loss: 1.7476857900619507\n",
      "Epoch 21 Complete. Training Loss: 1.695341944694519\n",
      "Epoch 22 Complete. Training Loss: 1.6619008779525757\n",
      "Epoch 23 Complete. Training Loss: 1.6204839944839478\n",
      "Epoch 24 Complete. Training Loss: 1.6154279708862305\n",
      "Epoch 25 Complete. Training Loss: 1.5873854160308838\n",
      "Epoch 26 Complete. Training Loss: 1.5748947858810425\n",
      "Epoch 27 Complete. Training Loss: 1.5625972747802734\n",
      "Epoch 28 Complete. Training Loss: 1.5603135824203491\n",
      "Epoch 29 Complete. Training Loss: 1.5525364875793457\n",
      "Epoch 30 Complete. Training Loss: 1.5442006587982178\n",
      "Epoch 31 Complete. Training Loss: 1.5381667613983154\n",
      "Epoch 32 Complete. Training Loss: 1.532902717590332\n",
      "Epoch 33 Complete. Training Loss: 1.5257046222686768\n",
      "Epoch 34 Complete. Training Loss: 1.5211373567581177\n",
      "Epoch 35 Complete. Training Loss: 1.5158278942108154\n",
      "Epoch 36 Complete. Training Loss: 1.5116888284683228\n",
      "Epoch 37 Complete. Training Loss: 1.506354570388794\n",
      "Epoch 38 Complete. Training Loss: 1.5020792484283447\n",
      "Epoch 39 Complete. Training Loss: 1.4976924657821655\n",
      "Epoch 40 Complete. Training Loss: 1.4949015378952026\n",
      "Epoch 41 Complete. Training Loss: 1.4917010068893433\n",
      "Epoch 42 Complete. Training Loss: 1.4887691736221313\n",
      "Epoch 43 Complete. Training Loss: 1.4860752820968628\n",
      "Epoch 44 Complete. Training Loss: 1.4838249683380127\n",
      "Epoch 45 Complete. Training Loss: 1.481130838394165\n",
      "Epoch 46 Complete. Training Loss: 1.4794633388519287\n",
      "Epoch 47 Complete. Training Loss: 1.4775185585021973\n",
      "Epoch 48 Complete. Training Loss: 1.4759303331375122\n",
      "Epoch 49 Complete. Training Loss: 1.4741581678390503\n",
      "Epoch 50 Complete. Training Loss: 1.4730740785598755\n",
      "Epoch 51 Complete. Training Loss: 1.4719066619873047\n",
      "Epoch 52 Complete. Training Loss: 1.4708102941513062\n",
      "Epoch 53 Complete. Training Loss: 1.46977961063385\n",
      "Epoch 54 Complete. Training Loss: 1.468837022781372\n",
      "Epoch 55 Complete. Training Loss: 1.468091607093811\n",
      "Epoch 56 Complete. Training Loss: 1.467294454574585\n",
      "Epoch 57 Complete. Training Loss: 1.4666447639465332\n",
      "Epoch 58 Complete. Training Loss: 1.4659992456436157\n",
      "Epoch 59 Complete. Training Loss: 1.465539813041687\n",
      "Epoch 60 Complete. Training Loss: 1.465083360671997\n",
      "Epoch 61 Complete. Training Loss: 1.4646490812301636\n",
      "Epoch 62 Complete. Training Loss: 1.4642962217330933\n",
      "Epoch 63 Complete. Training Loss: 1.4639512300491333\n",
      "Epoch 64 Complete. Training Loss: 1.4636226892471313\n",
      "Epoch 65 Complete. Training Loss: 1.463343858718872\n",
      "Epoch 66 Complete. Training Loss: 1.4631073474884033\n",
      "Epoch 67 Complete. Training Loss: 1.4629074335098267\n",
      "Epoch 68 Complete. Training Loss: 1.4627138376235962\n",
      "Epoch 69 Complete. Training Loss: 1.4625338315963745\n",
      "Epoch 70 Complete. Training Loss: 1.4623740911483765\n",
      "Epoch 71 Complete. Training Loss: 1.4622341394424438\n",
      "Epoch 72 Complete. Training Loss: 1.4621187448501587\n",
      "Epoch 73 Complete. Training Loss: 1.462019681930542\n",
      "Epoch 74 Complete. Training Loss: 1.4618974924087524\n",
      "Epoch 75 Complete. Training Loss: 1.4618327617645264\n",
      "Epoch 76 Complete. Training Loss: 1.461742877960205\n",
      "Epoch 77 Complete. Training Loss: 1.4616771936416626\n",
      "Epoch 78 Complete. Training Loss: 1.4616252183914185\n",
      "Epoch 79 Complete. Training Loss: 1.4615650177001953\n",
      "Epoch 80 Complete. Training Loss: 1.4615179300308228\n",
      "Epoch 81 Complete. Training Loss: 1.461478590965271\n",
      "Epoch 82 Complete. Training Loss: 1.4614421129226685\n",
      "Epoch 83 Complete. Training Loss: 1.461401104927063\n",
      "Epoch 84 Complete. Training Loss: 1.4613733291625977\n",
      "Epoch 85 Complete. Training Loss: 1.461346983909607\n",
      "Epoch 86 Complete. Training Loss: 1.461323857307434\n",
      "Epoch 87 Complete. Training Loss: 1.461303472518921\n",
      "Epoch 88 Complete. Training Loss: 1.461287260055542\n",
      "Epoch 89 Complete. Training Loss: 1.461270809173584\n",
      "Epoch 90 Complete. Training Loss: 1.4612568616867065\n",
      "Epoch 91 Complete. Training Loss: 1.461242914199829\n",
      "Epoch 92 Complete. Training Loss: 1.4612326622009277\n",
      "Epoch 93 Complete. Training Loss: 1.4612221717834473\n",
      "Epoch 94 Complete. Training Loss: 1.4612149000167847\n",
      "Epoch 95 Complete. Training Loss: 1.4612065553665161\n",
      "Epoch 96 Complete. Training Loss: 1.461199402809143\n",
      "Epoch 97 Complete. Training Loss: 1.4611936807632446\n",
      "Epoch 98 Complete. Training Loss: 1.461188554763794\n",
      "Epoch 99 Complete. Training Loss: 1.4611841440200806\n",
      "Finished Training the Model\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 150\n",
    "\n",
    "try:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        # Epoch == Entire Training Set\n",
    "        for epoch in range(num_epochs):\n",
    "            num_batches = mnist.train.num_examples // batch_size\n",
    "\n",
    "            # 150 batch size\n",
    "            for iteration in range(num_batches):\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "                sess.run(train, feed_dict={X: y_batch})\n",
    "\n",
    "            training_loss = loss.eval(feed_dict={X: y_batch})   \n",
    "            print(\"Epoch {} Complete. Training Loss: {}\".format(epoch,training_loss))\n",
    "            saver.save(sess, \"./save/label_ae_3.ckpt\")\n",
    "            \n",
    "            \n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print('Keyboard Interrupted')\n",
    "finally:\n",
    "    print('Finished Training the Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save/label_ae_3.ckpt\n",
      "TRAIN ACCURACY: \n",
      "1.0\n",
      "TEST ACCURACY: \n",
      "1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_test_labels = 10\n",
    "start_point = 15\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver.restore(sess,\"./save/label_ae_3.ckpt\")\n",
    "    \n",
    "    results,compressed = sess.run([output_layer, hid_layer3],\n",
    "                                  feed_dict={X:mnist.test.labels[start_point:num_test_labels+start_point]})\n",
    "    weight3 = w3.eval()\n",
    "    \n",
    "    matches = tf.equal(tf.argmax(output_layer, 1), tf.argmax(X,1))\n",
    "    acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "    print('TRAIN ACCURACY: ')\n",
    "    print (sess.run(acc, feed_dict={X:mnist.train.labels}))\n",
    "\n",
    "    print('TEST ACCURACY: ')\n",
    "    print (sess.run(acc, feed_dict={X:mnist.test.labels}))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL VS RECONSTRUCTED\n",
    "f, a = plt.subplots(3, num_test_labels, figsize=(20, 6))\n",
    "for i in range(start_point,num_test_labels+start_point):\n",
    "    j = i-start_point\n",
    "    a[0][j].imshow(np.reshape(mnist.test.labels[i], (10, 1)))\n",
    "    a[1][j].imshow(np.reshape(results[j], (10, 1)))\n",
    "    a[2][j].imshow(np.reshape(compressed[j], (2, 1)))\n",
    "    print(mnist.test.labels[i])\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "    print(results[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0463, -0.9424],\n",
       "       [ 0.95  ,  0.9727],\n",
       "       [ 1.1914, -0.7222]], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "print(weight3.shape)\n",
    "np.save('./save/weights/lw3.npy',weight3)\n",
    "#wt1 = np.load('./save/weights/w1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
