{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0ae49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "434afa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random, time, os, sys, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba3e099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sparse_nonlinear_lib as snl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea7de099",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72997d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time.sleep(60*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6015bc91",
   "metadata": {},
   "source": [
    "## For CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9efd59af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_train = transforms.Compose([\n",
    "    transforms.RandomCrop(size=32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "        std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "    ),\n",
    "])\n",
    "\n",
    "cifar_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "        std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"../../../../../_Datasets/cifar10/\", train=True, download=True, transform=cifar_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"../../../../../_Datasets/cifar10/\", train=False, download=True, transform=cifar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e30401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09e01ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 32, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## demo of train loader\n",
    "# xx, yy = iter(train_loader).next()\n",
    "for xx, yy in train_loader:\n",
    "    break\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b7c334",
   "metadata": {},
   "source": [
    "# Model Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0430d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpBLock(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_layers_ratio=[2], actf=nn.GELU):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        #### convert hidden layers ratio to list if integer is inputted\n",
    "        if isinstance(hidden_layers_ratio, int):\n",
    "            hidden_layers_ratio = [hidden_layers_ratio]\n",
    "            \n",
    "        self.hlr = [1]+hidden_layers_ratio+[1]\n",
    "        \n",
    "        self.mlp = []\n",
    "        ### for 1 hidden layer, we iterate 2 times\n",
    "        for h in range(len(self.hlr)-1):\n",
    "            i, o = int(self.hlr[h]*self.input_dim),\\\n",
    "                    int(self.hlr[h+1]*self.input_dim)\n",
    "            self.mlp.append(nn.Linear(i, o))\n",
    "            self.mlp.append(actf())\n",
    "        self.mlp = self.mlp[:-1]\n",
    "        \n",
    "        self.mlp = nn.Sequential(*self.mlp)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "907b8ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CIFAR10_ImageMonarchMLP(nn.Module):\n",
    "    \n",
    "#     def __init__(self, img_size=(3, 32, 32), hidden_layers_ratio=[2], actf=nn.GELU):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.block0 = MlpBLock(img_size[0]*img_size[1], hidden_layers_ratio, actf=actf)\n",
    "#         self.block1 = MlpBLock(img_size[0]*img_size[2], hidden_layers_ratio, actf=actf)\n",
    "        \n",
    "# #         self.norm = nn.BatchNorm1d(select)\n",
    "#         self.norm = nn.LayerNorm(np.prod(img_size))\n",
    "    \n",
    "#         ### Can also use normalization per block for effeciency\n",
    "# #         self.norm1 = nn.LayerNorm(self.block1.input_dim)\n",
    "\n",
    "#         self.actf = actf()\n",
    "#         self.fc = nn.Linear(np.prod(img_size), 10)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         bs, C, H, W = x.shape\n",
    "        \n",
    "#         ### use B, W, C*H\n",
    "#         x = x.permute(0, 3, 1, 2).contiguous().view(bs, W, -1)\n",
    "#         x = self.block0(x).view(bs, W, C, H)\n",
    "#         ### use B, H, C*W\n",
    "#         x = x.permute(0, 3, 2, 1).contiguous().view(bs, H, -1)\n",
    "#         x = self.block1(x).view(bs, -1)\n",
    "        \n",
    "#         x = self.norm(x)\n",
    "#         x = self.actf(x)\n",
    "#         x = self.fc(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e5215b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR10_ImageMonarchMLP()(torch.randn(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb85e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowColMixer(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_size=(3, 32, 32), hidden_layers_ratio=[2], actf=nn.GELU):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block0 = MlpBLock(img_size[0]*img_size[1], hidden_layers_ratio, actf=actf)\n",
    "        self.norm0 = nn.LayerNorm(self.block0.input_dim)\n",
    "        self.block1 = MlpBLock(img_size[0]*img_size[2], hidden_layers_ratio, actf=actf)\n",
    "        self.norm1 = nn.LayerNorm(self.block1.input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, C, H, W = x.shape\n",
    "        res = x\n",
    "        \n",
    "        ### use B, W, C*H\n",
    "        x = x.permute(0, 3, 1, 2).contiguous().view(bs, W, -1)\n",
    "        x = self.block0(self.norm0(x))\n",
    "        ### use B, H, C*W\n",
    "        x = x.view(bs, W, C, H).permute(0, 3, 2, 1).contiguous().view(bs, H, -1)\n",
    "        x = self.block1(self.norm1(x))\n",
    "\n",
    "        x = x.view(bs, H, C, W).permute(0, 2, 1, 3).contiguous()\n",
    "        return x + res\n",
    "    \n",
    "class CIFAR10_RowColMixer(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_size=(3, 32, 32), hidden_layers_ratio=[2], layers=1, channel_expand=3, actf=nn.GELU):\n",
    "        super().__init__()\n",
    "        assert img_size[0] <= channel_expand, \"Can't reduce channels than original size\"\n",
    "        self.select_channels = torch.randperm(channel_expand - img_size[0])%3\n",
    "        \n",
    "        img_size = (channel_expand, img_size[1], img_size[2])\n",
    "        \n",
    "        self.blocks = []\n",
    "        for i in range(layers):\n",
    "            self.blocks.append(RowColMixer(img_size, hidden_layers_ratio, actf=actf))\n",
    "        self.blocks = nn.Sequential(*self.blocks)\n",
    "        \n",
    "        self.norm = nn.BatchNorm1d(np.prod(img_size))\n",
    "#         self.norm = nn.LayerNorm(np.prod(img_size))\n",
    "#         self.actf = actf()\n",
    "        self.fc = nn.Linear(np.prod(img_size), 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, C, H, W = x.shape\n",
    "        x = torch.cat((x, x[:, self.select_channels, :, :]), dim=1)\n",
    "        \n",
    "        ### use B, W, C*H\n",
    "        x = self.blocks(x).view(bs, -1)\n",
    "        x = self.norm(x)\n",
    "#         x = self.actf(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3488430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.3503e-02,  4.4751e-01, -5.6777e-05,  9.5314e-01,  1.1769e+00,\n",
       "          1.2710e+00,  7.1167e-01,  1.0610e-01,  9.0168e-01,  3.8667e-01],\n",
       "        [-3.0119e-02, -8.1653e-01,  3.0493e-01,  1.7339e-01, -1.5425e+00,\n",
       "         -3.9107e-01,  7.6206e-01,  1.0401e+00, -3.7562e-01,  1.3260e-02],\n",
       "        [ 4.6819e-01, -2.5582e-03, -6.9367e-02,  6.7653e-01,  1.1961e-01,\n",
       "          5.5355e-01, -1.2394e+00, -1.7039e+00, -8.1242e-01,  2.0579e-01],\n",
       "        [-2.9483e-01, -9.7842e-01, -4.6154e-01, -1.1676e+00,  9.3388e-01,\n",
       "         -1.7197e-01, -5.5773e-01,  1.8984e-01, -1.7195e-01, -4.7012e-02],\n",
       "        [-1.7174e-01,  7.6339e-02,  2.6950e-02, -5.7526e-01, -5.2477e-01,\n",
       "          3.4190e-01, -3.1683e-01,  1.5339e+00,  8.9587e-01, -7.1919e-01],\n",
       "        [ 6.3098e-01,  2.1883e-01,  3.2724e-01,  3.8056e-01,  3.2671e-03,\n",
       "         -5.9096e-01,  5.2233e-01, -1.4360e+00, -6.5502e-01,  7.3660e-01],\n",
       "        [-1.8325e-01,  2.3869e-02,  7.4325e-01,  4.5781e-02,  3.0987e-01,\n",
       "          4.6250e-01, -9.3400e-01, -2.0175e-02,  1.6047e-01, -3.3286e-01],\n",
       "        [ 1.3672e-01, -1.3062e-01,  2.6679e-01,  7.6814e-01, -2.2373e-01,\n",
       "         -6.8143e-01,  8.9176e-01, -2.2613e-01,  8.1281e-01,  4.3895e-01],\n",
       "        [-8.0985e-01,  1.2393e+00, -3.6088e-01, -5.4388e-01, -4.3753e-01,\n",
       "         -4.9305e-01,  1.4011e-02, -3.4317e-01, -8.0725e-01, -2.4048e-01],\n",
       "        [ 3.2705e-01, -1.1964e-01, -7.2721e-01, -7.2241e-01,  1.6810e-01,\n",
       "         -2.4295e-01,  1.9893e-01,  9.0082e-01,  8.8111e-02, -4.2102e-01]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CIFAR10_RowColMixer(channel_expand=20)(torch.randn(10, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998962aa",
   "metadata": {},
   "source": [
    "## Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01bda469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CIFAR10_ImageMonarchMLP()\n",
    "model = CIFAR10_RowColMixer(layers=2, channel_expand=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da798939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CIFAR10_RowColMixer(\n",
       "  (blocks): Sequential(\n",
       "    (0): RowColMixer(\n",
       "      (block0): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=640, out_features=320, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm0): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (block1): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=640, out_features=320, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): RowColMixer(\n",
       "      (block0): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=640, out_features=320, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm0): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (block1): MlpBLock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=320, out_features=640, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=640, out_features=320, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (norm): BatchNorm1d(10240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc): Linear(in_features=10240, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da2406aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(2, 3, 32, 32).to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6484ba76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  1767690\n"
     ]
    }
   ],
   "source": [
    "print(\"number of params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed83146",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "437aa6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## debugging to find the good classifier/output distribution.\n",
    "# model_name = 'RowColumn_Mixer_CIFAR10_v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b754a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 50\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# # optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "# # optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "# # optimizer = torch.optim.Adam(model.parameters(), lr=0.00003)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "198c5b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Following is copied from \n",
    "### https://github.com/kuangliu/pytorch-cifar/blob/master/main.py\n",
    "\n",
    "# Training\n",
    "def train(epoch, model, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "#     for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    loss = train_loss/(batch_idx+1)\n",
    "    acc = 100.*correct/total\n",
    "#     print(f\"[Train] {epoch} Loss: {loss:.3f} | Acc: {acc:.3f} {correct}/{total}\")\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd7c4ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_acc = -1\n",
    "def test(epoch, model, optimizer, best_acc, model_name):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    latency = []\n",
    "    with torch.no_grad():\n",
    "#         for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader)):\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            start = time.time()\n",
    "            outputs = model(inputs)\n",
    "            ttaken = time.time()-start\n",
    "                \n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            latency.append(ttaken)\n",
    "    \n",
    "    loss = test_loss/(batch_idx+1)\n",
    "    acc = 100.*correct/total\n",
    "#     print(f\"[Test] {epoch} Loss: {test_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "    \n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "#         print(f'Saving.. Acc: {100.*correct/total:.3f}')\n",
    "        state = {\n",
    "            'model': model.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('models'):\n",
    "            os.mkdir('models')\n",
    "        torch.save(state, f'./models/{model_name}.pth')\n",
    "        best_acc = acc\n",
    "        \n",
    "    return loss, acc, best_acc, latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfc6f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "# resume = False\n",
    "\n",
    "# if resume:\n",
    "#     # Load checkpoint.\n",
    "#     print('==> Resuming from checkpoint..')\n",
    "#     assert os.path.isdir('./models'), 'Error: no checkpoint directory found!'\n",
    "#     checkpoint = torch.load(f'./models/{model_name}.pth')\n",
    "#     model.load_state_dict(checkpoint['model'])\n",
    "#     best_acc = checkpoint['acc']\n",
    "#     start_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be18cb60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # ### Train the whole damn thing\n",
    "\n",
    "# best_acc = -1\n",
    "# for epoch in range(start_epoch, start_epoch+EPOCHS): ## for 200 epochs\n",
    "#     trloss, tracc = train(epoch, model, optimizer)\n",
    "#     teloss, teacc, best_acc, latency = test(epoch, model, optimizer, best_acc, model_name)\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ad02c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_acc ## 90.42 for ordinary, 89.59 for sparse, 89.82 fro 32bMLP, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73e262e",
   "metadata": {},
   "source": [
    "### Do all experiments in repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7c5fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, lr, model_name, epochs=200, seed=0):\n",
    "    global criterion, train_loader, test_loader\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "    \n",
    "    best_acc = -1\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    stats = {'num_param':n_params, 'latency': [], \n",
    "             'train_acc':[], 'train_loss':[], \n",
    "             'test_acc':[], 'test_loss':[] \n",
    "            }\n",
    "    \n",
    "    print(f\"Begin Training for {model_name}\")\n",
    "    print(f\"Num Parameters: {n_params}\")\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        trloss, tracc = train(epoch, model, optimizer)\n",
    "        teloss, teacc, best_acc, laten = test(epoch, model, optimizer, best_acc, model_name)\n",
    "        scheduler.step()\n",
    "        \n",
    "        stats['latency'] += laten\n",
    "        stats['train_acc'].append(tracc)\n",
    "        stats['test_acc'].append(teacc)\n",
    "        stats['train_loss'].append(trloss)\n",
    "        stats['test_loss'].append(teloss)\n",
    "        \n",
    "    print()\n",
    "    \n",
    "    latency = np.array(stats['latency'])\n",
    "    mu, std = np.mean(latency), np.std(latency)\n",
    "    stats['latency'] = {'mean':mu, 'std':std}\n",
    "    ### Save stats of the model\n",
    "    with open(f'./models/stats/{model_name}_stats.json', 'w') as f:\n",
    "        json.dump(stats, f)\n",
    "    \n",
    "    return stats, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd00e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FOR MULTI-LAYERED MONARCH/ROW-COL VISION MIXER\n",
    "hidden_scale = 2\n",
    "channel_expand = [5, 10, 20]\n",
    "EPOCHS = 200\n",
    "LR = 0.001\n",
    "seed = 147\n",
    "\n",
    "def benchmark_cifar10():\n",
    "    for layers in [2, 4, 3]:\n",
    "        ### First test MLP with allowed dimension mixing\n",
    "        \n",
    "        h = hidden_scale\n",
    "        for c in channel_expand:\n",
    "            torch.manual_seed(seed)\n",
    "            \n",
    "            model = CIFAR10_RowColMixer(layers=layers, hidden_layers_ratio=[h], channel_expand=c)\n",
    "            n_params = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"\\t\\t{n_params}\\tRowCol-MLP-Mixer ChannelExpand\")\n",
    "            model_name = f\"cifar10_layered_RowColMixer_l{layers}_c{c}_h{h}_s{seed}\"\n",
    "            \n",
    "            train_model(model, LR, model_name, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b246d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t474250\tRowCol-MLP-Mixer ChannelExpand\n",
      "Begin Training for cifar10_layered_RowColMixer_l2_c5_h2_s147\n",
      "Num Parameters: 474250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 200/200 [23:12<00:00,  6.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t1767690\tRowCol-MLP-Mixer ChannelExpand\n",
      "Begin Training for cifar10_layered_RowColMixer_l2_c10_h2_s147\n",
      "Num Parameters: 1767690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 200/200 [28:55<00:00,  8.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t6812170\tRowCol-MLP-Mixer ChannelExpand\n",
      "Begin Training for cifar10_layered_RowColMixer_l2_c20_h2_s147\n",
      "Num Parameters: 6812170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|█████████████████████████████████                | 135/200 [1:03:21<36:27, 33.65s/it]"
     ]
    }
   ],
   "source": [
    "benchmark_cifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af605891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
