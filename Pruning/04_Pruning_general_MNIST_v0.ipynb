{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mylibrary.nnlib as tnn\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import mylibrary.datasets as datasets\n",
    "import prunelib\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = datasets.MNIST()\n",
    "train_data, train_label_, test_data, test_label_ = mnist.load()\n",
    "\n",
    "train_data = train_data / 255.\n",
    "test_data = test_data / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = tnn.Logits.index_to_logit(train_label_)\n",
    "train_size = len(train_label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.Tensor(train_data)\n",
    "test_data = torch.Tensor(test_data)\n",
    "yy = torch.LongTensor(train_label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(784, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10),\n",
    "#     nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.003)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(100):\n",
    "#     yout = net(xx)\n",
    "\n",
    "#     loss = criterion(yout, yy)\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     error = float(loss)\n",
    "#     print(epoch, 'Error = ', error)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         yout = net(xx)\n",
    "#         out = torch.argmax(yout, axis=1)\n",
    "#         acc = (out.data.numpy() == np.array(train_label_)).astype(np.float).mean()\n",
    "#         print(\"Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', Linear(in_features=784, out_features=256, bias=True)),\n",
       " ('1', ReLU()),\n",
       " ('2', Linear(in_features=256, out_features=128, bias=True)),\n",
       " ('3', ReLU()),\n",
       " ('4', Linear(in_features=128, out_features=64, bias=True)),\n",
       " ('5', ReLU()),\n",
       " ('6', Linear(in_features=64, out_features=10, bias=True))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(net._modules.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\"model\":net.state_dict(), \"optimizer\":optimizer.state_dict()},\n",
    "#           \"./mnist_100_mlp.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"./mnist_100_mlp.pth\")\n",
    "net.load_state_dict(checkpoint[\"model\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle Pruning Modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pruning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pruner():\n",
    "    \n",
    "    def __init__(self, net, prune_mask=None):\n",
    "        self.net = net\n",
    "        self.keys = []\n",
    "        self.prune_mask = {}\n",
    "        self.forward_hook = {}\n",
    "        \n",
    "        self.activations = []\n",
    "        \n",
    "        for name, module in list(self.net._modules.items()):\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                self.keys.append(module)\n",
    "\n",
    "        if prune_mask is not None:\n",
    "            self.add_prune_mask(prune_mask)\n",
    "        self.remove_hook()\n",
    "        \n",
    "    def add_prune_mask(self, prune_mask):\n",
    "        for module, pm in zip(self.keys[:-1], prune_mask):\n",
    "            self.prune_mask[module] = pm.type(torch.float)\n",
    "        self.prune_mask[self.keys[-1]] = torch.ones(self.keys[-1].out_features, dtype=torch.float)\n",
    "            \n",
    "        \n",
    "    def prune_neurons(self, module, inp, out):\n",
    "        mask = self.prune_mask[module]\n",
    "        output = out*mask\n",
    "        \n",
    "        self.activations.append(output)\n",
    "        return output\n",
    "        \n",
    "    def forward(self, x, prune_mask=None):\n",
    "        if prune_mask:\n",
    "            self.add_prune_mask(prune_mask)\n",
    "            if len(self.forward_hook) == 0:\n",
    "                self.add_hook()\n",
    "        \n",
    "        y = self.net(x)\n",
    "        self.remove_hook()\n",
    "        return y\n",
    "        \n",
    "        \n",
    "    def add_hook(self):\n",
    "        if len(self.forward_hook) > 0:\n",
    "            self.remove_hook()\n",
    "            \n",
    "        self.forward_hook = {}\n",
    "        for name, module in list(self.net._modules.items()):\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                hook = module.register_forward_hook(self.prune_neurons)\n",
    "                self.forward_hook[module] = hook\n",
    "        return\n",
    "        \n",
    "    def remove_hook(self):       \n",
    "        for module in self.forward_hook.keys():\n",
    "            hook = self.forward_hook[module]\n",
    "            hook.remove()\n",
    "        self.forward_hook = {}\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparing for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is  0.1028500348329544\n",
      "taylorfo\n",
      "new error =  2.135969400405884\n",
      "deviation =  25.84116554260254\n",
      "\n",
      "taylorfo_abs\n",
      "new error =  2.2881228923797607\n",
      "deviation =  29.152713775634766\n",
      "\n",
      "taylorfo_sq\n",
      "new error =  2.2435989379882812\n",
      "deviation =  29.51301383972168\n",
      "\n",
      "taylorfo_norm\n",
      "new error =  2.634556293487549\n",
      "deviation =  31.42356300354004\n",
      "\n",
      "taylorfo_abs_norm\n",
      "new error =  2.2881228923797607\n",
      "deviation =  29.152713775634766\n",
      "\n",
      "taylorfo_sq_norm\n",
      "new error =  2.244985342025757\n",
      "deviation =  29.50473976135254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num = 300\n",
    "pnet = Pruner(net)\n",
    "\n",
    "yout_normal = net.forward(xx).data.cpu()\n",
    "print(\"loss is \", float(criterion(yout_normal, yy)))\n",
    "\n",
    "taylorfo_mod = prunelib.Importance_TaylorFO_Modified(net, criterion)\n",
    "for i in range(6):\n",
    "# for i in range(len(prunelib.taylorfo_mode_list)):\n",
    "    print(prunelib.taylorfo_mode_list[i])\n",
    "    importance = taylorfo_mod.compute_significance(xx, yy,\n",
    "                                                 config=prunelib.taylorfo_mode_config[prunelib.taylorfo_mode_list[i]],\n",
    "                                                 normalize=True, layerwise_norm=False,\n",
    "                                                 use_unit_grad=np.random.randint(1024)\n",
    "                                                  )\n",
    "    pmask = prunelib.get_pruning_mask(importance, num_prune=num)\n",
    "\n",
    "    yout_prune = pnet.forward(xx, prune_mask=pmask).data.cpu()\n",
    "    new_err = criterion(yout_prune, yy)\n",
    "    print(\"new error = \", float(new_err))\n",
    "    \n",
    "    deviation = ((yout_prune-yout_normal)**2).mean()\n",
    "    print(\"deviation = \", float(deviation))\n",
    "    print()\n",
    "    \n",
    "# print(\"Molchanov parameter\")\n",
    "# mol_parm = prunelib.Importance_Molchanov_2019(net, criterion)\n",
    "# importance = mol_parm.compute_significance(xx, yy)\n",
    "# pmask = prunelib.get_pruning_mask(importance, num_prune=num)\n",
    "\n",
    "# yout_prune = pnet.forward(xx, prune_mask=pmask).data.cpu()\n",
    "# new_err = criterion(yout_prune, yy)\n",
    "# print(\"new error = \", float(new_err))\n",
    "\n",
    "# deviation = ((yout_prune-yout_normal)**2).mean()\n",
    "# print(\"deviation = \", float(deviation))\n",
    "# print()\n",
    "\n",
    "# print(\"APnZ\")\n",
    "# apnz = prunelib.Importance_APoZ(net, criterion)\n",
    "# importance = apnz.compute_significance(xx, yy)\n",
    "# pmask = prunelib.get_pruning_mask(importance, num_prune=num)\n",
    "\n",
    "# yout_prune = pnet.forward(xx, prune_mask=pmask).data.cpu()\n",
    "# new_err = criterion(yout_prune, yy)\n",
    "# print(\"new error = \", float(new_err))\n",
    "\n",
    "# deviation = ((yout_prune-yout_normal)**2).mean()\n",
    "# print(\"deviation = \", float(deviation))\n",
    "# print()\n",
    "\n",
    "# print(\"Magnitude\")\n",
    "# mag = prunelib.Importance_Magnitude(net, criterion)\n",
    "# importance = mag.compute_significance(xx, yy)\n",
    "# pmask = prunelib.get_pruning_mask(importance, num_prune=num)\n",
    "\n",
    "# yout_prune = pnet.forward(xx, prune_mask=pmask).data.cpu()\n",
    "# new_err = criterion(yout_prune, yy)\n",
    "# print(\"new error = \", float(new_err))\n",
    "\n",
    "# deviation = ((yout_prune-yout_normal)**2).mean()\n",
    "# print(\"deviation = \", float(deviation))\n",
    "# print()\n",
    "\n",
    "# num = 50\n",
    "# loss is  0.1028500348329544\n",
    "# taylorfo\n",
    "# new error =  0.12499944865703583\n",
    "# deviation =  1.1074118614196777\n",
    "\n",
    "# taylorfo_abs\n",
    "# new error =  0.11183208972215652\n",
    "# deviation =  0.2707483768463135\n",
    "\n",
    "# taylorfo_sq\n",
    "# new error =  0.10807643085718155\n",
    "# deviation =  0.2193640172481537\n",
    "\n",
    "# taylorfo_norm\n",
    "# new error =  0.2512335777282715\n",
    "# deviation =  3.4444315433502197\n",
    "\n",
    "# taylorfo_abs_norm\n",
    "# new error =  0.1116778701543808\n",
    "# deviation =  0.35614505410194397\n",
    "\n",
    "# taylorfo_sq_norm\n",
    "# new error =  0.10743308067321777\n",
    "# deviation =  0.2153528928756714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss is  0.1028500348329544\n",
    "# taylorfo\n",
    "# new error =  2.080207586288452\n",
    "# deviation =  29.253026962280273\n",
    "\n",
    "# taylorfo_abs\n",
    "# new error =  2.3692681789398193\n",
    "# deviation =  29.090253829956055\n",
    "\n",
    "# taylorfo_sq\n",
    "# new error =  2.1357648372650146\n",
    "# deviation =  29.005739212036133\n",
    "\n",
    "# taylorfo_norm\n",
    "# new error =  2.237595558166504\n",
    "# deviation =  30.961593627929688\n",
    "\n",
    "# taylorfo_abs_norm\n",
    "# new error =  2.386812925338745\n",
    "# deviation =  29.190523147583008\n",
    "\n",
    "# taylorfo_sq_norm\n",
    "# new error =  2.1146740913391113\n",
    "# deviation =  29.0556640625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taylorfo\n",
      "new error =  2.4779293537139893\n",
      "deviation =  28.82784652709961\n",
      "\n",
      "taylorfo_abs\n",
      "new error =  0.9035003185272217\n",
      "deviation =  15.854421615600586\n",
      "\n",
      "taylorfo_sq\n",
      "new error =  2.304408311843872\n",
      "deviation =  31.76415252685547\n",
      "\n",
      "taylorfo_norm\n",
      "new error =  2.4779293537139893\n",
      "deviation =  28.82784652709961\n",
      "\n",
      "taylorfo_abs_norm\n",
      "new error =  0.9035003185272217\n",
      "deviation =  15.854421615600586\n",
      "\n",
      "taylorfo_sq_norm\n",
      "new error =  0.9820285439491272\n",
      "deviation =  19.799896240234375\n",
      "\n",
      "taylorfo_nolin\n",
      "new error =  2.4779293537139893\n",
      "deviation =  28.82784652709961\n",
      "\n",
      "taylorfo_abs_nolin\n",
      "new error =  0.9035003185272217\n",
      "deviation =  15.854421615600586\n",
      "\n",
      "taylorfo_sq_nolin\n",
      "new error =  2.304408311843872\n",
      "deviation =  31.76415252685547\n",
      "\n",
      "taylorfo_norm_nolin\n",
      "new error =  2.4779293537139893\n",
      "deviation =  28.82784652709961\n",
      "\n",
      "taylorfo_abs_norm_nolin\n",
      "new error =  0.9035003185272217\n",
      "deviation =  15.854421615600586\n",
      "\n",
      "taylorfo_sq_norm_nolin\n",
      "new error =  0.9820285439491272\n",
      "deviation =  19.799896240234375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# taylorfo_norm = prunelib.Importance_TaylorFO_Normalized(net, criterion)\n",
    "# for i in range(len(prunelib.taylorfo_mode_list)):\n",
    "#     print(prunelib.taylorfo_mode_list[i])\n",
    "#     importance = taylorfo_norm.compute_significance(xx, yy,\n",
    "#                                                  config=prunelib.taylorfo_mode_config[prunelib.taylorfo_mode_list[i]],\n",
    "#                                                  normalize=True, layerwise_norm=False)\n",
    "#     pmask = prunelib.get_pruning_mask(importance, num_prune=num)\n",
    "\n",
    "#     yout_prune = pnet.forward(xx, prune_mask=pmask).data.cpu()\n",
    "#     new_err = criterion(yout_prune, yy)\n",
    "#     print(\"new error = \", float(new_err))\n",
    "    \n",
    "#     deviation = ((yout_prune-yout_normal)**2).mean()\n",
    "#     print(\"deviation = \", float(deviation))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grad = torch.randn(100, 10)\n",
    "# torch.norm(grad, dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Importance_Molchanov2(prunelib.Importance):\n",
    "\n",
    "#     def __init__(self, net, criterion):\n",
    "#         self.net = net\n",
    "#         self.criterion = criterion\n",
    "        \n",
    "#         self.inputs = {}\n",
    "#         self.gradients = {}\n",
    "#         self.forward_hook = {}\n",
    "#         self.backward_hook = {}\n",
    "#         self.keys = []\n",
    "#         pass\n",
    "\n",
    "#     def add_hook(self):\n",
    "#         self.inputs = {}\n",
    "#         self.gradients = {}\n",
    "#         self.forward_hook = {}\n",
    "#         self.backward_hook = {}\n",
    "#         self.keys = []\n",
    "        \n",
    "#         for name, module in list(self.net._modules.items()):\n",
    "#             if isinstance(module, torch.nn.Linear):\n",
    "#                 hook = module.register_backward_hook(self.capture_gradients)\n",
    "#                 self.backward_hook[module] = hook\n",
    "#                 hook = module.register_forward_hook(self.capture_inputs)\n",
    "#                 self.forward_hook[module] = hook\n",
    "                \n",
    "#                 self.inputs[module] = None\n",
    "#                 self.gradients[module] = None\n",
    "#                 self.keys.append(module)\n",
    "        \n",
    "#     def remove_hook(self):\n",
    "#         for module in self.keys:\n",
    "#             hook = self.forward_hook[module]\n",
    "#             hook.remove()\n",
    "#             hook = self.backward_hook[module]\n",
    "#             hook.remove()\n",
    "    \n",
    "#     def capture_inputs(self, module, inp, out):\n",
    "# #         print(\"inp >>\")\n",
    "# #         for i in inp:\n",
    "# #             if i is not None:\n",
    "# #                 print(i.shape)\n",
    "# #         print(\"<< inp\")\n",
    "#         self.inputs[module] = inp[0].data\n",
    "        \n",
    "#     def capture_gradients(self, module, gradi, grado):\n",
    "# #         print(grado[0].shape)\n",
    "# #         print(module.weight.shape)\n",
    "# #         print(module.bias.shape)\n",
    "#         self.gradients[module] = grado[0]\n",
    "        \n",
    "#     def gather_inputs_gradients(self, x, t):\n",
    "#         self.add_hook()\n",
    "\n",
    "#         self.net.zero_grad()\n",
    "#         y = self.net(x)\n",
    "        \n",
    "#         error = self.criterion(y,t)\n",
    "#         error.backward()\n",
    "        \n",
    "#         self.remove_hook()\n",
    "#         return\n",
    "    \n",
    "#     def compute_significance(self, x, t, normalize=True):\n",
    "#         self.gather_inputs_gradients(x, t)\n",
    "\n",
    "# #         importance = [0]*len(self.keys)\n",
    "#         importance = []\n",
    "#         for module in self.keys:\n",
    "#             ## compute weight and bias gradients\n",
    "#             inp = self.inputs[module]\n",
    "#             inp = inp.reshape(inp.shape[0], 1, -1)\n",
    "#             grd = self.gradients[module]\n",
    "#             grd = inp.reshape(grd.shape[0], -1, 1)\n",
    "            \n",
    "#             wgrad = torch.matmul(grd, inp)#.pow(2).sum(dim=0) + \n",
    "#             w_ = module.weight.data.reshape(1, module.weight.data.shape[0], module.weight.data.shape[1])\n",
    "#             wz = w_*wgrad\n",
    "#             print(wz.shape)\n",
    "#             bz = module.bias.data.reshape(1, -1)@grd.reshape(grd.shape[0], -1)\n",
    "#             print(bz.shape)\n",
    "#             z = wz.pow(2).sum(dim=[0,2]) + bz.pow(2).sum(dim=0)\n",
    "#             print(z.shape)\n",
    "#             print()\n",
    "#             importance.append(z)\n",
    "# #             for i in tqdm(range(len(x))):\n",
    "# #                 dwi = grd[i:i+1].t()@inp[i:i+1]\n",
    "# #                 dbi = grd[i]\n",
    "# #                 z = (module.weight.data*dwi).pow(2).sum(dim=1) + \\\n",
    "# #                     (module.bias*dbi).pow(2)\n",
    "# #                 impo += z\n",
    "# #             impo = impo/len(x)\n",
    "# #             importance.append(impo)\n",
    "\n",
    "#         importance = importance[:-1]\n",
    "#         if normalize:\n",
    "#             sums = 0\n",
    "#             count = 0\n",
    "#             for imp in importance:\n",
    "#                 sums += imp.sum()\n",
    "#                 count += len(imp)\n",
    "#             divider = sums/count ## total importance is number of neurons\n",
    "#             for i in range(len(importance)):\n",
    "#                 importance[i] = importance[i]/divider\n",
    "            \n",
    "#         return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = torch.randn(60000, 784)\n",
    "# grd = torch.randn(60000, 256)\n",
    "\n",
    "# m1 = inp.reshape(inp.shape[0], 1, -1)\n",
    "# m2 = grd.reshape(grd.shape[0], -1, 1)\n",
    "# torch.matmul(m2, m1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(inp)):\n",
    "#     print((grd[i:i+1].t()@inp[i:i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Molchanov parameter 2\") ## overflows the memory\n",
    "\n",
    "# mol_parm = Importance_Molchanov2(net, criterion)\n",
    "# importance = mol_parm.compute_significance(xx, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Importance_Molchanov2_1(prunelib.Importance):\n",
    "\n",
    "    def __init__(self, net, criterion):\n",
    "        self.net = net\n",
    "        self.criterion = criterion\n",
    "        self.keys = []\n",
    "        for name, module in list(self.net._modules.items()):\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                self.keys.append(module)\n",
    "        \n",
    "    def compute_significance(self, x, t, normalize=True, batch_size=32):\n",
    "\n",
    "        importance = [0]*len(self.keys)\n",
    "        bstrt = list(range(0, len(x), batch_size))\n",
    "        bstop = bstrt[1:]+[len(x)]\n",
    "        for i in tqdm(range(len(bstrt))):\n",
    "            self.net.zero_grad()\n",
    "            y = self.net(x[bstrt[i]:bstop[i]])\n",
    "            error = self.criterion(y,t[bstrt[i]:bstop[i]])\n",
    "            error.backward()\n",
    "        \n",
    "            ## compute importance for each input\n",
    "            for j, module in enumerate(self.keys):\n",
    "                z = (module.weight.data*module.weight.grad).pow(2).sum(dim=1) + \\\n",
    "                    (module.bias*module.bias.grad).pow(2)\n",
    "                importance[j] += z\n",
    "                \n",
    "        ## compute mean\n",
    "        for i, module in enumerate(self.keys):\n",
    "            importance[i] = importance[i]/len(bstrt) \n",
    "\n",
    "\n",
    "        importance = importance[:-1]\n",
    "        if normalize:\n",
    "            sums = 0\n",
    "            count = 0\n",
    "            for imp in importance:\n",
    "                sums += imp.sum()\n",
    "                count += len(imp)\n",
    "            divider = sums/count ## total importance is number of neurons\n",
    "            for i in range(len(importance)):\n",
    "                importance[i] = importance[i]/divider\n",
    "            \n",
    "        return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Molchanov parameter 2.1\")\n",
    "\n",
    "mol_parm = Importance_Molchanov2_1(net, criterion)\n",
    "importance = mol_parm.compute_significance(xx, yy, batch_size=32)\n",
    "pmask = prunelib.get_pruning_mask(importance, num_prune=num)\n",
    "\n",
    "yout_prune = pnet.forward(xx, prune_mask=pmask).data.cpu()\n",
    "new_err = criterion(yout_prune, yy)\n",
    "print(\"new error = \", float(new_err))\n",
    "\n",
    "deviation = ((yout_prune-yout_normal)**2).mean()\n",
    "print(\"deviation = \", float(deviation))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for num=200\n",
    "# 1024\n",
    "# new error =  1.1853246688842773\n",
    "# deviation =  21.11980628967285\n",
    "\n",
    "# 512\n",
    "# new error =  1.3359383344650269\n",
    "# deviation =  22.091835021972656\n",
    "\n",
    "# 256\n",
    "# new error =  1.3257719278335571\n",
    "# deviation =  22.431970596313477\n",
    "\n",
    "# 128\n",
    "# new error =  1.3867021799087524\n",
    "# deviation =  22.838653564453125\n",
    "\n",
    "# 64\n",
    "# new error =  1.3867021799087524\n",
    "# deviation =  22.838653564453125\n",
    "\n",
    "# 32\n",
    "# new error =  1.3867021799087524\n",
    "# deviation =  22.838653564453125\n",
    "\n",
    "# 16\n",
    "# new error =  1.3867021799087524\n",
    "# deviation =  22.838653564453125\n",
    "\n",
    "# 8\n",
    "# new error =  1.421226978302002\n",
    "# deviation =  23.039220809936523\n",
    "\n",
    "# 4\n",
    "# new error =  1.421226978302002\n",
    "# deviation =  23.039220809936523"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitude based and APoZ(Relu only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Importance_APoZ(prunelib.Importance):\n",
    "\n",
    "    def __init__(self, net, criterion):\n",
    "        self.net = net\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        self.activations = {}\n",
    "        self.forward_hook = {}\n",
    "        self.keys = []\n",
    "        pass\n",
    "\n",
    "    def add_hook(self):\n",
    "        self.activations = {}\n",
    "        self.forward_hook = {}\n",
    "        self.keys = []\n",
    "        \n",
    "        for name, module in list(self.net._modules.items()):\n",
    "            if isinstance(module, torch.nn.ReLU):\n",
    "                hook = module.register_forward_hook(self.capture_inputs)\n",
    "                self.forward_hook[module] = hook\n",
    "                \n",
    "                self.activations[module] = None\n",
    "                self.keys.append(module)\n",
    "        \n",
    "    def remove_hook(self):\n",
    "        for module in self.keys:\n",
    "            hook = self.forward_hook[module]\n",
    "            hook.remove()\n",
    "    \n",
    "    def capture_inputs(self, module, inp, out):\n",
    "        self.activations[module] = out.data\n",
    "        \n",
    "    def gather_activations(self, x, t):\n",
    "        self.add_hook()\n",
    "\n",
    "        self.net.zero_grad()\n",
    "        y = self.net(x)\n",
    "        \n",
    "        self.remove_hook()\n",
    "        return\n",
    "    \n",
    "    def compute_significance(self, x, t, normalize=True):\n",
    "        self.gather_activations(x, t)\n",
    "\n",
    "        importance = []\n",
    "        for module in self.keys:\n",
    "            apnz = torch.sum(self.activations[module] > 0., dim=0, dtype=torch.float)\n",
    "            importance.append(apnz)\n",
    "\n",
    "        if normalize:\n",
    "            sums = 0\n",
    "            count = 0\n",
    "            for imp in importance:\n",
    "                sums += imp.sum()\n",
    "                count += len(imp)\n",
    "            divider = sums/count ## total importance is number of neurons\n",
    "            for i in range(len(importance)):\n",
    "                importance[i] = importance[i]/divider\n",
    "            \n",
    "        return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"APnZ\")\n",
    "\n",
    "apnz = Importance_APoZ(net, criterion)\n",
    "importance = apnz.compute_significance(xx, yy)\n",
    "pmask = prunelib.get_pruning_mask(importance, num_prune=num)\n",
    "\n",
    "yout_prune = pnet.forward(xx, prune_mask=pmask).data.cpu()\n",
    "new_err = criterion(yout_prune, yy)\n",
    "print(\"new error = \", float(new_err))\n",
    "\n",
    "deviation = ((yout_prune-yout_normal)**2).mean()\n",
    "print(\"deviation = \", float(deviation))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apnz.remove_hook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Importance_Magnitude(prunelib.Importance):\n",
    "\n",
    "    def __init__(self, net, criterion=None):\n",
    "        self.net = net\n",
    "        self.keys = []\n",
    "        for name, module in list(self.net._modules.items()):\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                self.keys.append(module)\n",
    "        \n",
    "    def compute_significance(self, x=None, t=None, normalize=True):\n",
    "\n",
    "        importance = []\n",
    "        for module in self.keys:\n",
    "            z = torch.norm(module.weight.data, p=2, dim=1)\n",
    "            importance.append(z)\n",
    "\n",
    "        importance = importance[:-1]\n",
    "        if normalize:\n",
    "            sums = 0\n",
    "            count = 0\n",
    "            for imp in importance:\n",
    "                sums += imp.sum()\n",
    "                count += len(imp)\n",
    "            divider = sums/count ## total importance is number of neurons\n",
    "            for i in range(len(importance)):\n",
    "                importance[i] = importance[i]/divider\n",
    "            \n",
    "        return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Magnitude\")\n",
    "\n",
    "mag = Importance_Magnitude(net, criterion)\n",
    "importance = mag.compute_significance(xx, yy)\n",
    "pmask = prunelib.get_pruning_mask(importance, num_prune=num)\n",
    "\n",
    "yout_prune = pnet.forward(xx, prune_mask=pmask).data.cpu()\n",
    "new_err = criterion(yout_prune, yy)\n",
    "print(\"new error = \", float(new_err))\n",
    "\n",
    "deviation = ((yout_prune-yout_normal)**2).mean()\n",
    "print(\"deviation = \", float(deviation))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss is  0.1028500348329544\n",
    "# oracle\n",
    "# new error =  1.8523311614990234\n",
    "# deviation =  24.276596069335938\n",
    "\n",
    "# oracle_abs\n",
    "# new error =  1.2125073671340942\n",
    "# deviation =  14.391016960144043\n",
    "\n",
    "# oracle_sq\n",
    "# new error =  0.8678733706474304\n",
    "# deviation =  17.924470901489258\n",
    "\n",
    "# oracle_norm\n",
    "# new error =  2.3006534576416016\n",
    "# deviation =  27.330440521240234\n",
    "\n",
    "# oracle_abs_norm\n",
    "# new error =  1.2079159021377563\n",
    "# deviation =  14.306313514709473\n",
    "\n",
    "# oracle_sq_norm\n",
    "# new error =  0.8907394409179688\n",
    "# deviation =  18.013774871826172\n",
    "\n",
    "# oracle_nolin\n",
    "# new error =  1.917508840560913\n",
    "# deviation =  24.35315704345703\n",
    "\n",
    "# oracle_abs_nolin\n",
    "# new error =  1.8538734912872314\n",
    "# deviation =  16.1265926361084\n",
    "\n",
    "# oracle_sq_nolin\n",
    "# new error =  0.8453637957572937\n",
    "# deviation =  14.958662033081055\n",
    "\n",
    "# oracle_norm_nolin\n",
    "# new error =  2.6378610134124756\n",
    "# deviation =  29.217300415039062\n",
    "\n",
    "# oracle_abs_norm_nolin\n",
    "# new error =  1.8637737035751343\n",
    "# deviation =  16.142126083374023\n",
    "\n",
    "# oracle_sq_norm_nolin\n",
    "# new error =  0.8662921786308289\n",
    "# deviation =  15.479593276977539"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
