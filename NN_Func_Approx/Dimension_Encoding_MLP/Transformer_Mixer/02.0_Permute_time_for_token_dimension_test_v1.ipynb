{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6287145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsuman/All-Files/Program_Files/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os, sys, pathlib, random, time, pickle, copy, json\n",
    "# from tqdm.autonotebook import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0173cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcaa3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_lib import TransformerBlock, \\\n",
    "        Mixer_TransformerBlock_Encoder, \\\n",
    "        PositionalEncoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2e11b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b45f6816",
   "metadata": {},
   "outputs": [],
   "source": [
    "### add randomize patches for clear benefit\n",
    "class Mixer_ViT_Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_dim:tuple, patch_size:tuple, hidden_channel:int, num_blocks:int, num_classes:int, block_seq_size:int, block_mlp_size:int, forward_expansion:float=2.0, pos_emb=True, dropout:float=0.0, randomize_patch:bool=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_dim = image_dim ### must contain (C, H, W) or (H, W)\n",
    "        \n",
    "        ### find patch dim\n",
    "        d0 = int(image_dim[-2]/patch_size[0])\n",
    "        d1 = int(image_dim[-1]/patch_size[1])\n",
    "        assert d0*patch_size[0]==image_dim[-2], \"Image must be divisible into patch size\"\n",
    "        assert d1*patch_size[1]==image_dim[-1], \"Image must be divisible into patch size\"\n",
    "        \n",
    "#         self.d0, self.d1 = d0, d1 ### number of patches in each axis\n",
    "        __patch_size = patch_size[0]*patch_size[1]*image_dim[0] ## number of channels in each patch\n",
    "    \n",
    "        ### find channel dim\n",
    "        channel_size = d0*d1 ## number of patches\n",
    "        \n",
    "        ### after the number of channels are changed\n",
    "        init_dim = __patch_size\n",
    "        final_dim = hidden_channel\n",
    "        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        #### rescale the patches (patch wise image non preserving transform, unlike bilinear interpolation)\n",
    "        self.channel_change = nn.Linear(init_dim, final_dim)\n",
    "        print(f\"ViT Mixer : Channels per patch -> Initial:{init_dim} Final:{final_dim}\")\n",
    "        \n",
    "        \n",
    "        self.channel_dim = final_dim\n",
    "        self.patch_dim = channel_size\n",
    "        \n",
    "        self.transformer_blocks = []\n",
    "        \n",
    "        f = self.get_factors(self.channel_dim)\n",
    "        print(f)\n",
    "        fi = np.abs(np.array(f) - np.sqrt(self.channel_dim)).argmin()\n",
    "        \n",
    "        _n_heads = f[fi]\n",
    "        \n",
    "        ## number of dims per channel -> channel_dim\n",
    "#         print('Num patches:', self.patch_dim)\n",
    "        print(f'Sequence len: {self.patch_dim} ; Block size: {block_seq_size}')\n",
    "        print('Channel dim:', self.channel_dim, 'num heads:',_n_heads)\n",
    "            \n",
    "        \n",
    "        if block_seq_size is None or block_seq_size<2:\n",
    "            ### Find the block size for sequence:\n",
    "            block_seq_size = int(2**np.ceil(np.log2(np.sqrt(self.patch_dim))))\n",
    "            \n",
    "        print(f'MLP dim: {self.channel_dim} ; Block size: {block_mlp_size}')\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            L = Mixer_TransformerBlock_Encoder(self.patch_dim, block_seq_size, self.channel_dim, _n_heads, dropout, forward_expansion, nn.GELU, block_mlp_size)\n",
    "            self.transformer_blocks.append(L)\n",
    "        self.transformer_blocks = nn.Sequential(*self.transformer_blocks)\n",
    "        \n",
    "        self.linear = nn.Linear(self.patch_dim*self.channel_dim, num_classes)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(self.channel_dim, dropout=0)\n",
    "        if not pos_emb:\n",
    "            self.positional_encoding = nn.Identity()\n",
    "            \n",
    "        self.randomize = None\n",
    "        if randomize_patch is not None:\n",
    "            self.randomize = torch.randperm(self.patch_dim)\n",
    "        \n",
    "        \n",
    "    def get_factors(self, n):\n",
    "        facts = []\n",
    "        for i in range(2, n+1):\n",
    "            if n%i == 0:\n",
    "                facts.append(i)\n",
    "        return facts\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.unfold(x).swapaxes(-1, -2)\n",
    "        x = self.channel_change(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        ## swap position of patches here\n",
    "        if self.randomize is not None:\n",
    "            x = x[..., self.randomize, :]\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.linear(x.view(bs, -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7094b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b0ac9f5-f1e2-4dba-a8e3-06bb4d5acac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a356db1-2fad-40ba-8186-7ca3c40ddd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3efe0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67e27199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Mixer_ViT_Classifier([3, 32, 32], [2, 2], 64, num_blocks=2, num_classes=10, \n",
    "#                             block_seq_size=16, block_mlp_size=None, pos_emb=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f741268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1017ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Mixer_ViT_Classifier([3, 32, 32], [1, 1], 64, num_blocks=2, num_classes=10, \n",
    "#                             block_seq_size=32, block_mlp_size=None, pos_emb=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae575940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(\"number of params: \", sum(p.numel() for p in model.parameters()))\n",
    "# for name, m in model.named_children():\n",
    "# #     print(name)\n",
    "#     print(f\"{name}: {sum(p.numel() for p in m.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d642126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b258ea4-8037-457d-aea6-35e9e02d9be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"number of params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca20622e",
   "metadata": {},
   "source": [
    "## Benchmark Memory and Time CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1faebb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nvidia_smi\n",
    "# ### pip install nvidia-ml-py3\n",
    "\n",
    "\n",
    "# MB = 1024*1024\n",
    "# def get_memory_used(cuda_idx):\n",
    "#     nvidia_smi.nvmlInit()\n",
    "#     handle = nvidia_smi.nvmlDeviceGetHandleByIndex(cuda_idx)\n",
    "#     info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "#     val = info.used/MB\n",
    "#     nvidia_smi.nvmlShutdown()\n",
    "#     return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1812db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_memory_used(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb7fa91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "def get_memory_used(cuda_idx):\n",
    "    if cuda_idx == 0:\n",
    "        cuda_idx = 1\n",
    "    else:\n",
    "        cuda_idx = 0\n",
    "    command = \"nvidia-smi --query-gpu=memory.used --format=csv\"\n",
    "    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    return memory_free_values[cuda_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30adc9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_used(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c9ae24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4 MiB', '13093 MiB']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = \"nvidia-smi --query-gpu=memory.used --format=csv\"\n",
    "memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "memory_free_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47d9cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asdfsadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7429fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "expansion_dict = {16:1024, 8:256, 4:128, 2:64, 1:64}\n",
    "def benchmark_memory(dataset:str, patch_size:int, num_layers:int, SEED:int, sparse_att:bool=False, sparse_mlp:bool=False, pos_emb:bool=False, cuda:int=0):\n",
    "    global expansion_dict\n",
    "    device = torch.device(f\"cuda:{cuda}\")\n",
    "    \n",
    "    if sparse_att:\n",
    "        assert num_layers%2 == 0, 'number of blocks on sparse transformer is (x2)/2 hence it must be even'\n",
    "        num_layers_ = num_layers//2\n",
    "    else:\n",
    "        num_layers_ = num_layers\n",
    "    \n",
    "    BS = 32\n",
    "    NC = -1\n",
    "    EPOCHS = 1\n",
    "    imsize = (3, 32, 32)\n",
    "    expansion = expansion_dict[patch_size]\n",
    "\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    ##### Data Transforms\n",
    "        \n",
    "    if dataset == 'cifar10':\n",
    "        NC = 10\n",
    "    elif dataset == 'cifar100':\n",
    "        NC = 100\n",
    "\n",
    "    ### Now create models\n",
    "    \n",
    "    seq_len = (imsize[-1]*imsize[-2])//(patch_size*patch_size)\n",
    "    mlp_dim = expansion\n",
    "    print(seq_len, mlp_dim)\n",
    "    \n",
    "    if sparse_att:\n",
    "        seq_len = int(2**np.ceil(np.log2(np.sqrt(seq_len))))\n",
    "    if sparse_mlp:\n",
    "        mlp_dim = int(2**np.ceil(np.log2(np.sqrt(expansion))))\n",
    "    \n",
    "    mem_begin = get_memory_used(cuda)\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    model = Mixer_ViT_Classifier(imsize, \n",
    "                                 patch_size=[patch_size]*2, \n",
    "                                 hidden_channel=expansion, \n",
    "                                 num_blocks=num_layers_, \n",
    "                                 num_classes=NC, \n",
    "                                 block_seq_size=seq_len, \n",
    "                                 block_mlp_size=mlp_dim,\n",
    "                                 pos_emb=pos_emb).to(device)\n",
    "    \n",
    "    _x = torch.randn(BS, *imsize)#.to(device)\n",
    "    _y = torch.randint(10, (BS,))\n",
    "#     print(\"Output: \",vit_mixer(_x).shape)\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"number of params: \", num_params)\n",
    "    \n",
    "    _a, _b, _c = 'att', 'mlp', 'nPE'\n",
    "    if sparse_att: _a = 'sAtt'\n",
    "    if sparse_mlp: _b = 'sMlp'\n",
    "    if pos_emb: _c = 'PE'\n",
    "        \n",
    "        \n",
    "    ### Training\n",
    "    model_name = f'01.3_ViT_train_{dataset}_patch{patch_size}_l{num_layers}_exp{expansion}_{_a}_{_b}_s{SEED}'\n",
    "    ### Inference\n",
    "    model_name = f'01.3_ViT_eval_{dataset}_patch{patch_size}_l{num_layers}_exp{expansion}_{_a}_{_b}_s{SEED}'\n",
    "    \n",
    "    print(f\"Model Name: {model_name}\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    ### Training\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    ### Inference\n",
    "#     model.eval()\n",
    "\n",
    "    inputs, targets = _x.to(device), _y.to(device)\n",
    "    ### test time taken for multiple iterations\n",
    "    time_taken = []\n",
    "    for i in tqdm(range(50)):\n",
    "\n",
    "        ### Inference\n",
    "#         with torch.no_grad():\n",
    "#             start = time.time()\n",
    "#             outputs = model(inputs)\n",
    "#             start = time.time()-start\n",
    "            \n",
    "        ### Training\n",
    "        start = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        start = time.time()-start\n",
    "        \n",
    "        time_taken.append(start)\n",
    "    train_time = np.mean(time_taken)\n",
    "        \n",
    "    mem_end = get_memory_used(cuda)\n",
    "    print(f\"mem begin: {mem_begin}  end: {mem_end}\")\n",
    "\n",
    "    model.eval()\n",
    "    time_taken = []\n",
    "    for i in range(50):\n",
    "        with torch.no_grad():\n",
    "            start = time.time()\n",
    "            outputs = model(inputs)\n",
    "            start = time.time()-start\n",
    "            time_taken.append(start)\n",
    "            \n",
    "    test_time = np.mean(time_taken)\n",
    "    \n",
    "    filename = f\"./output/bench_mem_retest_data2.json\"\n",
    "    if not os.path.exists(filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump({}, f, indent=0)\n",
    "            \n",
    "    with open(filename, 'r+') as f:\n",
    "#         with open(filename,'r+') as f:\n",
    "        file_data = json.load(f)\n",
    "        file_data[f\"{model_name}\"] = {'memory':mem_end-mem_begin, \n",
    "                                      'time_train':train_time, \n",
    "                                      'time_test':test_time,\n",
    "                                      'param':num_params}\n",
    "        f.seek(0)\n",
    "        json.dump(file_data, f, indent = 0)\n",
    "\n",
    "    del model, optimizer\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c0e54bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark_memory(dataset='cifar100', \n",
    "#                   patch_size=2, \n",
    "#                   num_layers=4, \n",
    "#                   SEED=147, \n",
    "#                   sparse_att=True, \n",
    "#                   sparse_mlp=False, \n",
    "#                   pos_emb=False,\n",
    "#                   cuda=0\n",
    "#                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "675693c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Experimenting on cifar10 Dataset \n",
      "                patch:4,\n",
      "                sparse_att: False,\n",
      "                sparse_mlp: False,\n",
      "                num_layers: 2,\n",
      "                pos_embed: False,\n",
      "                seed: 147\n",
      "            \n",
      "64 64\n",
      "ViT Mixer : Channels per patch -> Initial:48 Final:64\n",
      "[2, 4, 8, 16, 32, 64]\n",
      "Sequence len: 64 ; Block size: 64\n",
      "Channel dim: 64 num heads: 8\n",
      "MLP dim: 64 ; Block size: 64\n",
      "number of params:  111050\n",
      "Model Name: 01.3_ViT_train_cifar10_patch4_l2_exp64_att_mlp_s147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:01<00:00, 42.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem begin: 4  end: 382\n",
      "\n",
      "                Experimenting on cifar10 Dataset \n",
      "                patch:4,\n",
      "                sparse_att: True,\n",
      "                sparse_mlp: False,\n",
      "                num_layers: 2,\n",
      "                pos_embed: False,\n",
      "                seed: 147\n",
      "            \n",
      "64 64\n",
      "ViT Mixer : Channels per patch -> Initial:48 Final:64\n",
      "[2, 4, 8, 16, 32, 64]\n",
      "Sequence len: 64 ; Block size: 8\n",
      "Channel dim: 64 num heads: 8\n",
      "MLP dim: 64 ; Block size: 64\n",
      "number of params:  111050\n",
      "Model Name: 01.3_ViT_train_cifar10_patch4_l2_exp64_sAtt_mlp_s147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:00<00:00, 64.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem begin: 344  end: 364\n",
      "\n",
      "                Experimenting on cifar10 Dataset \n",
      "                patch:4,\n",
      "                sparse_att: False,\n",
      "                sparse_mlp: False,\n",
      "                num_layers: 2,\n",
      "                pos_embed: False,\n",
      "                seed: 147\n",
      "            \n",
      "64 256\n",
      "ViT Mixer : Channels per patch -> Initial:48 Final:256\n",
      "[2, 4, 8, 16, 32, 64, 128, 256]\n",
      "Sequence len: 64 ; Block size: 64\n",
      "Channel dim: 256 num heads: 16\n",
      "MLP dim: 256 ; Block size: 256\n",
      "number of params:  1230602\n",
      "Model Name: 01.3_ViT_train_cifar10_patch4_l2_exp256_att_mlp_s147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:00<00:00, 66.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem begin: 344  end: 446\n",
      "\n",
      "                Experimenting on cifar10 Dataset \n",
      "                patch:4,\n",
      "                sparse_att: True,\n",
      "                sparse_mlp: False,\n",
      "                num_layers: 2,\n",
      "                pos_embed: False,\n",
      "                seed: 147\n",
      "            \n",
      "64 256\n",
      "ViT Mixer : Channels per patch -> Initial:48 Final:256\n",
      "[2, 4, 8, 16, 32, 64, 128, 256]\n",
      "Sequence len: 64 ; Block size: 8\n",
      "Channel dim: 256 num heads: 16\n",
      "MLP dim: 256 ; Block size: 256\n",
      "number of params:  1230602\n",
      "Model Name: 01.3_ViT_train_cifar10_patch4_l2_exp256_sAtt_mlp_s147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:00<00:00, 63.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem begin: 344  end: 432\n",
      "\n",
      "                Experimenting on cifar10 Dataset \n",
      "                patch:4,\n",
      "                sparse_att: False,\n",
      "                sparse_mlp: False,\n",
      "                num_layers: 2,\n",
      "                pos_embed: False,\n",
      "                seed: 147\n",
      "            \n",
      "64 1024\n",
      "ViT Mixer : Channels per patch -> Initial:48 Final:1024\n",
      "[2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
      "Sequence len: 64 ; Block size: 64\n",
      "Channel dim: 1024 num heads: 32\n",
      "MLP dim: 1024 ; Block size: 1024\n",
      "number of params:  17505290\n",
      "Model Name: 01.3_ViT_train_cifar10_patch4_l2_exp1024_att_mlp_s147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:01<00:00, 25.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem begin: 344  end: 952\n",
      "\n",
      "                Experimenting on cifar10 Dataset \n",
      "                patch:4,\n",
      "                sparse_att: True,\n",
      "                sparse_mlp: False,\n",
      "                num_layers: 2,\n",
      "                pos_embed: False,\n",
      "                seed: 147\n",
      "            \n",
      "64 1024\n",
      "ViT Mixer : Channels per patch -> Initial:48 Final:1024\n",
      "[2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
      "Sequence len: 64 ; Block size: 8\n",
      "Channel dim: 1024 num heads: 32\n",
      "MLP dim: 1024 ; Block size: 1024\n",
      "number of params:  17505290\n",
      "Model Name: 01.3_ViT_train_cifar10_patch4_l2_exp1024_sAtt_mlp_s147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:02<00:00, 23.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem begin: 344  end: 932\n",
      "\n",
      "                Experimenting on cifar10 Dataset \n",
      "                patch:2,\n",
      "                sparse_att: False,\n",
      "                sparse_mlp: False,\n",
      "                num_layers: 2,\n",
      "                pos_embed: False,\n",
      "                seed: 147\n",
      "            \n",
      "256 64\n",
      "ViT Mixer : Channels per patch -> Initial:12 Final:64\n",
      "[2, 4, 8, 16, 32, 64]\n",
      "Sequence len: 256 ; Block size: 256\n",
      "Channel dim: 64 num heads: 8\n",
      "MLP dim: 64 ; Block size: 64\n",
      "number of params:  231626\n",
      "Model Name: 01.3_ViT_train_cifar10_patch2_l2_exp64_att_mlp_s147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:01<00:00, 49.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem begin: 344  end: 734\n",
      "\n",
      "                Experimenting on cifar10 Dataset \n",
      "                patch:2,\n",
      "                sparse_att: True,\n",
      "                sparse_mlp: False,\n",
      "                num_layers: 2,\n",
      "                pos_embed: False,\n",
      "                seed: 147\n",
      "            \n",
      "256 64\n",
      "ViT Mixer : Channels per patch -> Initial:12 Final:64\n",
      "[2, 4, 8, 16, 32, 64]\n",
      "Sequence len: 256 ; Block size: 16\n",
      "Channel dim: 64 num heads: 8\n",
      "MLP dim: 64 ; Block size: 64\n",
      "number of params:  231626\n",
      "Model Name: 01.3_ViT_train_cifar10_patch2_l2_exp64_sAtt_mlp_s147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:00<00:00, 71.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem begin: 344  end: 430\n",
      "\n",
      "                Experimenting on cifar10 Dataset \n",
      "                patch:2,\n",
      "                sparse_att: False,\n",
      "                sparse_mlp: False,\n",
      "                num_layers: 2,\n",
      "                pos_embed: False,\n",
      "                seed: 147\n",
      "            \n",
      "256 256\n",
      "ViT Mixer : Channels per patch -> Initial:12 Final:256\n",
      "[2, 4, 8, 16, 32, 64, 128, 256]\n",
      "Sequence len: 256 ; Block size: 256\n",
      "Channel dim: 256 num heads: 16\n",
      "MLP dim: 256 ; Block size: 256\n",
      "number of params:  1712906\n",
      "Model Name: 01.3_ViT_train_cifar10_patch2_l2_exp256_att_mlp_s147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:01<00:00, 27.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem begin: 344  end: 1192\n",
      "\n",
      "                Experimenting on cifar10 Dataset \n",
      "                patch:2,\n",
      "                sparse_att: True,\n",
      "                sparse_mlp: False,\n",
      "                num_layers: 2,\n",
      "                pos_embed: False,\n",
      "                seed: 147\n",
      "            \n",
      "256 256\n",
      "ViT Mixer : Channels per patch -> Initial:12 Final:256\n",
      "[2, 4, 8, 16, 32, 64, 128, 256]\n",
      "Sequence len: 256 ; Block size: 16\n",
      "Channel dim: 256 num heads: 16\n",
      "MLP dim: 256 ; Block size: 256\n",
      "number of params:  1712906\n",
      "Model Name: 01.3_ViT_train_cifar10_patch2_l2_exp256_sAtt_mlp_s147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:01<00:00, 40.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem begin: 344  end: 710\n",
      "\n",
      "                Experimenting on cifar10 Dataset \n",
      "                patch:2,\n",
      "                sparse_att: False,\n",
      "                sparse_mlp: False,\n",
      "                num_layers: 2,\n",
      "                pos_embed: False,\n",
      "                seed: 147\n",
      "            \n",
      "256 1024\n",
      "ViT Mixer : Channels per patch -> Initial:12 Final:1024\n",
      "[2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
      "Sequence len: 256 ; Block size: 256\n",
      "Channel dim: 1024 num heads: 32\n",
      "MLP dim: 1024 ; Block size: 1024\n",
      "number of params:  19434506\n",
      "Model Name: 01.3_ViT_train_cifar10_patch2_l2_exp1024_att_mlp_s147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:07<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem begin: 344  end: 2658\n",
      "\n",
      "                Experimenting on cifar10 Dataset \n",
      "                patch:2,\n",
      "                sparse_att: True,\n",
      "                sparse_mlp: False,\n",
      "                num_layers: 2,\n",
      "                pos_embed: False,\n",
      "                seed: 147\n",
      "            \n",
      "256 1024\n",
      "ViT Mixer : Channels per patch -> Initial:12 Final:1024\n",
      "[2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
      "Sequence len: 256 ; Block size: 16\n",
      "Channel dim: 1024 num heads: 32\n",
      "MLP dim: 1024 ; Block size: 1024\n",
      "number of params:  19434506\n",
      "Model Name: 01.3_ViT_train_cifar10_patch2_l2_exp1024_sAtt_mlp_s147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:06<00:00,  7.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem begin: 344  end: 1650\n",
      "\n",
      "                Experimenting on cifar10 Dataset \n",
      "                patch:1,\n",
      "                sparse_att: False,\n",
      "                sparse_mlp: False,\n",
      "                num_layers: 2,\n",
      "                pos_embed: False,\n",
      "                seed: 147\n",
      "            \n",
      "1024 64\n",
      "ViT Mixer : Channels per patch -> Initial:3 Final:64\n",
      "[2, 4, 8, 16, 32, 64]\n",
      "Sequence len: 1024 ; Block size: 1024\n",
      "Channel dim: 64 num heads: 8\n",
      "MLP dim: 64 ; Block size: 64\n",
      "number of params:  722570\n",
      "Model Name: 01.3_ViT_train_cifar10_patch1_l2_exp64_att_mlp_s147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:06<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem begin: 344  end: 6552\n",
      "\n",
      "                Experimenting on cifar10 Dataset \n",
      "                patch:1,\n",
      "                sparse_att: True,\n",
      "                sparse_mlp: False,\n",
      "                num_layers: 2,\n",
      "                pos_embed: False,\n",
      "                seed: 147\n",
      "            \n",
      "1024 64\n",
      "ViT Mixer : Channels per patch -> Initial:3 Final:64\n",
      "[2, 4, 8, 16, 32, 64]\n",
      "Sequence len: 1024 ; Block size: 32\n",
      "Channel dim: 64 num heads: 8\n",
      "MLP dim: 64 ; Block size: 64\n",
      "number of params:  722570\n",
      "Model Name: 01.3_ViT_train_cifar10_patch1_l2_exp64_sAtt_mlp_s147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:01<00:00, 33.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem begin: 344  end: 830\n",
      "\n",
      "                Experimenting on cifar10 Dataset \n",
      "                patch:1,\n",
      "                sparse_att: False,\n",
      "                sparse_mlp: False,\n",
      "                num_layers: 2,\n",
      "                pos_embed: False,\n",
      "                seed: 147\n",
      "            \n",
      "1024 256\n",
      "ViT Mixer : Channels per patch -> Initial:3 Final:256\n",
      "[2, 4, 8, 16, 32, 64, 128, 256]\n",
      "Sequence len: 1024 ; Block size: 1024\n",
      "Channel dim: 256 num heads: 16\n",
      "MLP dim: 256 ; Block size: 256\n",
      "number of params:  3676682\n",
      "Model Name: 01.3_ViT_train_cifar10_patch1_l2_exp256_att_mlp_s147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                              | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda out of memory \n",
      " !!!!!!!!!!!!!!!!!!! \n",
      "\n",
      "\n",
      "                Experimenting on cifar10 Dataset \n",
      "                patch:1,\n",
      "                sparse_att: True,\n",
      "                sparse_mlp: False,\n",
      "                num_layers: 2,\n",
      "                pos_embed: False,\n",
      "                seed: 147\n",
      "            \n",
      "1024 256\n",
      "ViT Mixer : Channels per patch -> Initial:3 Final:256\n",
      "[2, 4, 8, 16, 32, 64, 128, 256]\n",
      "Sequence len: 1024 ; Block size: 32\n",
      "Channel dim: 256 num heads: 16\n",
      "MLP dim: 256 ; Block size: 256\n",
      "number of params:  3676682\n",
      "Model Name: 01.3_ViT_train_cifar10_patch1_l2_exp256_sAtt_mlp_s147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:03<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem begin: 344  end: 1590\n",
      "\n",
      "                Experimenting on cifar10 Dataset \n",
      "                patch:1,\n",
      "                sparse_att: False,\n",
      "                sparse_mlp: False,\n",
      "                num_layers: 2,\n",
      "                pos_embed: False,\n",
      "                seed: 147\n",
      "            \n",
      "1024 1024\n",
      "ViT Mixer : Channels per patch -> Initial:3 Final:1024\n",
      "[2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
      "Sequence len: 1024 ; Block size: 1024\n",
      "Channel dim: 1024 num heads: 32\n",
      "MLP dim: 1024 ; Block size: 1024\n",
      "number of params:  27289610\n",
      "Model Name: 01.3_ViT_train_cifar10_patch1_l2_exp1024_att_mlp_s147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                              | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda out of memory \n",
      " !!!!!!!!!!!!!!!!!!! \n",
      "\n",
      "\n",
      "                Experimenting on cifar10 Dataset \n",
      "                patch:1,\n",
      "                sparse_att: True,\n",
      "                sparse_mlp: False,\n",
      "                num_layers: 2,\n",
      "                pos_embed: False,\n",
      "                seed: 147\n",
      "            \n",
      "1024 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT Mixer : Channels per patch -> Initial:3 Final:1024\n",
      "[2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
      "Sequence len: 1024 ; Block size: 32\n",
      "Channel dim: 1024 num heads: 32\n",
      "MLP dim: 1024 ; Block size: 1024\n",
      "number of params:  27289610\n",
      "Model Name: 01.3_ViT_train_cifar10_patch1_l2_exp1024_sAtt_mlp_s147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 50/50 [00:23<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem begin: 344  end: 5096\n"
     ]
    }
   ],
   "source": [
    "### Automate the benchmark\n",
    "###### for c10\n",
    "\n",
    "cuda_idx = 1\n",
    "seed = 147\n",
    "PE = False\n",
    "nlayers=2\n",
    "dataset='cifar10'\n",
    "nlayers = 2\n",
    "sparse_mlp = False\n",
    "for patch_size in [4, 2, 1]:\n",
    "    for _expansion in [64, 256, 1024]:\n",
    "        for sparse_attention in [False, True]:\n",
    "            expansion_dict[patch_size] = _expansion\n",
    "            print(f'''\n",
    "                Experimenting on {dataset} Dataset \n",
    "                patch:{patch_size},\n",
    "                sparse_att: {sparse_attention},\n",
    "                sparse_mlp: {sparse_mlp},\n",
    "                num_layers: {nlayers},\n",
    "                pos_embed: {PE},\n",
    "                seed: {seed}\n",
    "            ''')\n",
    "\n",
    "            try:\n",
    "                benchmark_memory(dataset=dataset, \n",
    "                          patch_size=patch_size, \n",
    "                          num_layers=nlayers, \n",
    "                          SEED=seed, \n",
    "                          sparse_att=sparse_attention, sparse_mlp=sparse_mlp, \n",
    "                          pos_emb=PE,\n",
    "                          cuda=cuda_idx\n",
    "                         )\n",
    "            except Exception as e:\n",
    "                print(\"Cuda out of memory \\n !!!!!!!!!!!!!!!!!!! \\n\")\n",
    "\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed029069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
