{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-b22782e32495>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\heythere\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../../Tensorflow-Bootcamp-master/03-Convolutional-Neural-Networks/MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE parameters\n",
    "num_inputs = 784 # 28*28\n",
    "neurons_hid1 = 392\n",
    "neurons_hid2 = 196\n",
    "neurons_hid3 = 98\n",
    "neurons_hid4 = 49\n",
    "neurons_hid5 = 25\n",
    "neurons_hid6 = 12\n",
    "neurons_hid7 = 6\n",
    "neurons_hid8 = 3\n",
    "neurons_hid9 = 2\n",
    "neurons_hid10 = 1\n",
    "\n",
    "learning_rate = 0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PINV(PSEUDO-INVERSE) function\n",
    "def pinv(a, rcond=1e-15):\n",
    "    s, u, v = tf.svd(a)\n",
    "    # Ignore singular values close to zero to prevent numerical overflow\n",
    "    limit = rcond * tf.reduce_max(s)\n",
    "    non_zero = tf.greater(s, limit)\n",
    "\n",
    "    reciprocal = tf.where(non_zero, tf.reciprocal(s), tf.zeros(s.shape))\n",
    "    lhs = tf.matmul(v, tf.diag(reciprocal))\n",
    "    return tf.matmul(lhs, u, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSPOSE OR PINV\n",
    "tie_weight = tf.transpose #pinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLACEHOLDER DEFINATION\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WEIGHTS DEFINATION\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "w1 = tf.Variable(np.load('./save/weights/w1.npy'))\n",
    "w2 = tf.Variable(np.load('./save/weights/w2.npy'))\n",
    "w3 = tf.Variable(np.load('./save/weights/w3.npy'))\n",
    "w4 = tf.Variable(np.load('./save/weights/w4.npy'))\n",
    "w5 = tf.Variable(np.load('./save/weights/w5.npy'))\n",
    "w6 = tf.Variable(np.load('./save/weights/w6.npy'))\n",
    "w7 = tf.Variable(np.load('./save/weights/w7.npy'))\n",
    "w8 = tf.Variable(np.load('./save/weights/w8.npy'))\n",
    "w9 = tf.Variable(np.load('./save/weights/w9.npy'))\n",
    "w10 = tf.Variable(np.load('./save/weights/w10.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTIVATION FUNCTION  [ lambda X:X  <OR>  tf.nn.relu  ]\n",
    "act_func = lambda X:X #tf.nn.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYER MODELING OF :NN\n",
    "hid_layer1 = act_func(tf.matmul(X, w1))\n",
    "hid_layer2 = act_func(tf.matmul(hid_layer1, w2))\n",
    "hid_layer3 = act_func(tf.matmul(hid_layer2, w3))\n",
    "hid_layer4 = act_func(tf.matmul(hid_layer3, w4))\n",
    "hid_layer5 = act_func(tf.matmul(hid_layer4, w5))\n",
    "hid_layer6 = act_func(tf.matmul(hid_layer5, w6))\n",
    "hid_layer7 = act_func(tf.matmul(hid_layer6, w7))\n",
    "hid_layer8 = act_func(tf.matmul(hid_layer7, w8))\n",
    "hid_layer9 = act_func(tf.matmul(hid_layer8, w9))\n",
    "\n",
    "hid_layer10 = act_func(tf.matmul(hid_layer9, w10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = hid_layer8\n",
    "YY = hid_layer9\n",
    "\n",
    "wlearn = tf.Variable(initializer([neurons_hid8, neurons_hid9]), dtype=tf.float32)\n",
    "yy = tf.matmul(XX,wlearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTIONS\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "loss = tf.reduce_mean(tf.square(yy - YY))\n",
    "# loss = tf.reduce_mean(tf.abs(output_layer - X))\n",
    "# cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y,logits=yout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "train = optimizer.minimize(loss, var_list=[wlearn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARATION\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Complete. Training Loss: 7.194417476654053\n",
      "Epoch 1 Complete. Training Loss: 5.4940361976623535\n",
      "Epoch 2 Complete. Training Loss: 3.181454658508301\n",
      "Epoch 3 Complete. Training Loss: 2.7376527786254883\n",
      "Epoch 4 Complete. Training Loss: 2.001504421234131\n",
      "Epoch 5 Complete. Training Loss: 1.6806573867797852\n",
      "Epoch 6 Complete. Training Loss: 1.421770453453064\n",
      "Epoch 7 Complete. Training Loss: 0.9333379864692688\n",
      "Epoch 8 Complete. Training Loss: 0.725246012210846\n",
      "Epoch 9 Complete. Training Loss: 0.4878579080104828\n",
      "Epoch 10 Complete. Training Loss: 0.2851279377937317\n",
      "Epoch 11 Complete. Training Loss: 0.23172828555107117\n",
      "Epoch 12 Complete. Training Loss: 0.17320914566516876\n",
      "Epoch 13 Complete. Training Loss: 0.09385443478822708\n",
      "Epoch 14 Complete. Training Loss: 0.049512967467308044\n",
      "Epoch 15 Complete. Training Loss: 0.022198494523763657\n",
      "Epoch 16 Complete. Training Loss: 0.010723239742219448\n",
      "Epoch 17 Complete. Training Loss: 0.004109770059585571\n",
      "Epoch 18 Complete. Training Loss: 0.0012678380589932203\n",
      "Epoch 19 Complete. Training Loss: 0.0004202628042548895\n",
      "Epoch 20 Complete. Training Loss: 8.714471914572641e-05\n",
      "Epoch 21 Complete. Training Loss: 1.1552498108358122e-05\n",
      "Epoch 22 Complete. Training Loss: 1.1282180594207603e-06\n",
      "Epoch 23 Complete. Training Loss: 7.84910696438601e-08\n",
      "Epoch 24 Complete. Training Loss: 2.549438216092881e-09\n",
      "Epoch 25 Complete. Training Loss: 5.5289745004571955e-11\n",
      "Epoch 26 Complete. Training Loss: 2.5592694630205415e-11\n",
      "Keyboard Interrupted\n",
      "Finished Training the Model\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 150\n",
    "\n",
    "try:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        # Epoch == Entire Training Set\n",
    "        for epoch in range(num_epochs):\n",
    "            num_batches = mnist.train.num_examples // batch_size\n",
    "\n",
    "            # 150 batch size\n",
    "            for iteration in range(num_batches):\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_size)                \n",
    "                sess.run(train, feed_dict={X: X_batch})\n",
    "\n",
    "            training_loss = loss.eval(feed_dict={X: X_batch})   \n",
    "            print(\"Epoch {} Complete. Training Loss: {}\".format(epoch,training_loss))\n",
    "            saver.save(sess, \"./save/aeRev_10_2.ckpt\")      \n",
    "except KeyboardInterrupt:\n",
    "    print('Keyboard Interrupted')\n",
    "finally:\n",
    "    print('Finished Training the Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save/aeRev_10_2.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_test_images = 10\n",
    "start_point = 15\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver.restore(sess,\"./save/aeRev_10_2.ckpt\")\n",
    "    weight = w9.eval()\n",
    "    weigh_ = wlearn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual\n",
      "[[-0.7163862  -0.4557416 ]\n",
      " [-0.36904937  0.7646215 ]\n",
      " [-0.31930184 -0.32398126]]\n",
      "\n",
      "Computed\n",
      "[[-0.71638566 -0.45573932]\n",
      " [-0.36904812  0.7646241 ]\n",
      " [-0.31930172 -0.32398242]]\n",
      "\n",
      "Difference\n",
      "[[-5.3644180e-07 -2.2947788e-06]\n",
      " [-1.2516975e-06 -2.6226044e-06]\n",
      " [-1.1920929e-07  1.1622906e-06]]\n"
     ]
    }
   ],
   "source": [
    "print('Actual')\n",
    "print(weight)\n",
    "print('\\nComputed')\n",
    "print(weigh_)\n",
    "print('\\nDifference')\n",
    "print(weight-weigh_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
