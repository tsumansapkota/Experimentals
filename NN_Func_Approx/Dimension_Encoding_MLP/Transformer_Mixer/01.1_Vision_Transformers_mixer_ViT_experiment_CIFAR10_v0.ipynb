{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6287145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os, sys, pathlib, random, time, pickle, copy, json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49924ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:1\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f4b8961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED = 147\n",
    "# SEED = 258\n",
    "SEED = 369\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0173cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf9f67dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# cifar_train = transforms.Compose([\n",
    "#     transforms.RandomCrop(size=32, padding=4),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(\n",
    "#         mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "#         std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "#     ),\n",
    "# ])\n",
    "\n",
    "cifar_train = transforms.Compose([\n",
    "    transforms.RandAugment(magnitude=9),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "        std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "    ),\n",
    "])\n",
    "\n",
    "cifar_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "        std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"../../../../../_Datasets/cifar10/\", train=True, download=True, transform=cifar_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"../../../../../_Datasets/cifar10/\", train=False, download=True, transform=cifar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbdc0585",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45072db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 32, 32]), torch.Size([32]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## demo of train loader\n",
    "xx, yy = iter(train_loader).next()\n",
    "xx.shape, yy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2e11b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dcaa3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_lib import TransformerBlock, \\\n",
    "        Mixer_TransformerBlock_Encoder, \\\n",
    "        PositionalEncoding, \\\n",
    "        ViT_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b45f6816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixer_ViT_Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_dim:tuple, patch_size:tuple, hidden_expansion:float, num_blocks:int, num_classes:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_dim = image_dim ### must contain (C, H, W) or (H, W)\n",
    "        \n",
    "        ### find patch dim\n",
    "        d0 = int(image_dim[-2]/patch_size[0])\n",
    "        d1 = int(image_dim[-1]/patch_size[1])\n",
    "        assert d0*patch_size[0]==image_dim[-2], \"Image must be divisible into patch size\"\n",
    "        assert d1*patch_size[1]==image_dim[-1], \"Image must be divisible into patch size\"\n",
    "#         self.d0, self.d1 = d0, d1 ### number of patches in each axis\n",
    "        __patch_size = patch_size[0]*patch_size[1]*image_dim[0] ## number of channels in each patch\n",
    "    \n",
    "        ### find channel dim\n",
    "        channel_size = d0*d1 ## number of patches\n",
    "        \n",
    "        ### after the number of channels are changed\n",
    "        init_dim = __patch_size\n",
    "        final_dim = int(__patch_size*hidden_expansion/2)*2\n",
    "        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        #### rescale the patches (patch wise image non preserving transform, unlike bilinear interpolation)\n",
    "        self.channel_change = nn.Linear(init_dim, final_dim)\n",
    "        print(f\"ViT Mixer : Channes per patch -> Initial:{init_dim} Final:{final_dim}\")\n",
    "        \n",
    "        \n",
    "        self.channel_dim = final_dim\n",
    "        self.patch_dim = channel_size\n",
    "        \n",
    "        self.transformer_blocks = []\n",
    "        \n",
    "        f = self.get_factors(self.channel_dim)\n",
    "        print(f)\n",
    "        fi = np.abs(np.array(f) - np.sqrt(self.channel_dim)).argmin()\n",
    "        \n",
    "        _n_heads = f[fi]\n",
    "        \n",
    "        ## number of dims per channel -> channel_dim\n",
    "        print('Num patches',self.patch_dim)\n",
    "        print(self.channel_dim, _n_heads)\n",
    "        \n",
    "        ### Find the block size for sequence:\n",
    "#         block_seq_size = int(2**np.ceil(np.log2(np.sqrt(16))))\n",
    "        block_seq_size = 16\n",
    "\n",
    "#         block_seq_size = int(2**np.ceil(np.log2(np.sqrt(self.patch_dim))))\n",
    "        print(f'Mixing with block: {block_seq_size}')\n",
    "        \n",
    "#         block = int(np.ceil(np.sqrt(self.patch_dim)))\n",
    "        for i in range(num_blocks):\n",
    "            L = Mixer_TransformerBlock_Encoder(self.patch_dim, block_seq_size, self.channel_dim, _n_heads, 0, 2)\n",
    "            self.transformer_blocks.append(L)\n",
    "        self.transformer_blocks = nn.Sequential(*self.transformer_blocks)\n",
    "        \n",
    "        self.linear = nn.Linear(self.patch_dim*self.channel_dim, num_classes)\n",
    "        self.positional_encoding = PositionalEncoding(self.channel_dim, dropout=0)\n",
    "        \n",
    "        \n",
    "    def get_factors(self, n):\n",
    "        facts = []\n",
    "        for i in range(2, n+1):\n",
    "            if n%i == 0:\n",
    "                facts.append(i)\n",
    "        return facts\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.unfold(x).swapaxes(-1, -2)\n",
    "        x = self.channel_change(x)\n",
    "#         x = self.positional_encoding(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.linear(x.view(bs, -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "297726a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT Mixer : Channes per patch -> Initial:48 Final:114\n",
      "[2, 3, 6, 19, 38, 57, 114]\n",
      "Num patches 64\n",
      "114 6\n",
      "Mixing with block: 16\n"
     ]
    }
   ],
   "source": [
    "vit_mixer = Mixer_ViT_Classifier((3, 32, 32), patch_size=[4, 4], hidden_expansion=2.4, num_blocks=1, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c009dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mixer_ViT_Classifier(\n",
       "  (unfold): Unfold(kernel_size=[4, 4], dilation=1, padding=0, stride=[4, 4])\n",
       "  (channel_change): Linear(in_features=48, out_features=114, bias=True)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): Mixer_TransformerBlock_Encoder(\n",
       "      (sparse_transformers): ModuleList(\n",
       "        (0): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention_Sparse(\n",
       "            (values): Linear(in_features=114, out_features=114, bias=True)\n",
       "            (keys): Linear(in_features=114, out_features=114, bias=True)\n",
       "            (queries): Linear(in_features=114, out_features=114, bias=True)\n",
       "            (fc_out): Linear(in_features=114, out_features=114, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): ResMlpBlock(\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=114, out_features=228, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=228, out_features=114, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (1): Sparse_TransformerBlock(\n",
       "          (attention): SelfAttention_Sparse(\n",
       "            (values): Linear(in_features=114, out_features=114, bias=True)\n",
       "            (keys): Linear(in_features=114, out_features=114, bias=True)\n",
       "            (queries): Linear(in_features=114, out_features=114, bias=True)\n",
       "            (fc_out): Linear(in_features=114, out_features=114, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): ResMlpBlock(\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=114, out_features=228, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=228, out_features=114, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm2): LayerNorm((114,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=7296, out_features=10, bias=True)\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70b3d019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_mixer(torch.randn(1, 3, 32, 32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8987e68c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdasd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43masdasd\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'asdasd' is not defined"
     ]
    }
   ],
   "source": [
    "asdasd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbefc39",
   "metadata": {},
   "source": [
    "#### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31be60c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT Mixer : Channes per patch -> Initial:3 Final:6\n",
      "[2, 3, 6]\n",
      "6 2\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "# model = ViT_Classifier((3, 32, 32), patch_size=[4, 4], hidden_expansion=2.4, num_blocks=3, num_classes=10, pos_emb=False)\n",
    "# model = Mixer_ViT_Classifier((3, 32, 32), patch_size=[4, 4], hidden_expansion=2.4, num_blocks=3, num_classes=10)\n",
    "\n",
    "model = ViT_Classifier((3, 32, 32), patch_size=[1, 1], hidden_expansion=2.4, num_blocks=9, num_classes=10, pos_emb=False)\n",
    "# model = Mixer_ViT_Classifier((3, 32, 32), patch_size=[1, 1], hidden_expansion=2.4, num_blocks=3, num_classes=10)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c173fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e411715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196fe4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of params: \", sum(p.numel() for p in model.parameters())) \n",
    "## Patch ||  1137220\n",
    "## Mixer ||  1141703\n",
    "## ViT   ||  1130776 / 1341220 (1) / 1025554 (2) / 709888 (3) / 394222 (4)\n",
    "## SMViT ||  1341220 / 1025554 (1) / 709888 (2) / 394222 (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b49dd",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = f'vit_mixer_c10_s{SEED}'\n",
    "# model_name = f'vit_pe_mixer_c10_s{SEED}'\n",
    "\n",
    "# model_name = f'vit_sparse_mixer_c10_s{SEED}' ## sparse but with 12 layers total\n",
    "# model_name = f'vit1_mixer_c10_s{SEED}' ## with 12 layers for comparision\n",
    "# model_name = f'vit1_sparse_mixer_c10_s{SEED}' ## sparse with 9 layers for reference\n",
    "# model_name = f'vit2_mixer_c10_s{SEED}' ## with 9 layers for reference\n",
    "# model_name = f'vit2_sparse_mixer_c10_s{SEED}' ## sparse with 6 layers for reference\n",
    "# model_name = f'vit3_mixer_c10_s{SEED}' ## with 6 layers for reference\n",
    "# model_name = f'vit3_sparse_mixer_c10_s{SEED}' ## sparse with 3 layers for reference\n",
    "# model_name = f'vit4_mixer_c10_s{SEED}' ## with 3 layers for reference\n",
    "# model_name = f'vit4_sparse_mixer_c10_s{SEED}' ## sparse with 3 layers for reference and 1x1 patch\n",
    "\n",
    "model_name = f'temp_s{SEED}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66447d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f63463",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAT ={'train_stat':[], 'test_stat':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d9af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Following is copied from \n",
    "### https://github.com/kuangliu/pytorch-cifar/blob/master/main.py\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "    STAT['train_stat'].append((epoch, train_loss/(batch_idx+1), 100.*correct/total)) ### (Epochs, Loss, Acc)\n",
    "    print(f\"[Train] {epoch} Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32895df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = -1\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "    STAT['test_stat'].append((epoch, test_loss/(batch_idx+1), 100.*correct/total)) ### (Epochs, Loss, Acc)\n",
    "    print(f\"[Test] {epoch} Loss: {test_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} {correct}/{total}\")\n",
    "    \n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'model': model.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        if not os.path.isdir('models'):\n",
    "            os.mkdir('models')\n",
    "        torch.save(state, f'./models/{model_name}.pth')\n",
    "        best_acc = acc\n",
    "        \n",
    "    with open(f\"./output/{model_name}_data.json\", 'w') as f:\n",
    "        json.dump(STAT, f, indent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d57ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "resume = False\n",
    "\n",
    "if resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('./models'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load(f'./models/{model_name}.pth')\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the whole damn thing\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+EPOCHS): ## for 200 epochs\n",
    "# for epoch in range(2): ## for 200 epochs\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7803b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f'./models/{model_name}.pth')\n",
    "best_acc = checkpoint['acc']\n",
    "start_epoch = checkpoint['epoch']\n",
    "\n",
    "best_acc, start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847f18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### the expansion is 2.4\n",
    "### 83.69 for 12 layers sparse vit\n",
    "### 82.46 for 12 layers vit\n",
    "### 82.57 for 10 layers vit\n",
    "### 84.84 for 9 = (3*3) layers sparse vit\n",
    "### 82.47 for 9 layers vit\n",
    "### 83.88 for 6 = (2*3) layers sparse vit\n",
    "### 81.36 for 6 layers vit\n",
    "### 81.68 for 3 = (1*3) layers sparse vit\n",
    "### 81.50 for 3 layers vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c6a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55759f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf1676",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./output/{model_name}_data.json\", 'r') as f:\n",
    "    STAT = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a339d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stat = np.array(STAT['train_stat'])\n",
    "test_stat = np.array(STAT['test_stat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_stat[:,1], label='train')\n",
    "plt.plot(test_stat[:,1], label='test')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"./output/plots/{model_name}_loss.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bee5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_stat[:,2], label='train')\n",
    "plt.plot(test_stat[:,2], label='test')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"./output/plots/{model_name}_accs.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a77b859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d174ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
